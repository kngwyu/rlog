{
  
    
        "post0": {
            "title": "Jaxãƒ»Braxãƒ»Haikuã§GPUå¼•ãã“ã‚‚ã‚Šå­¦ç¿’",
            "content": "0. &#26178;&#21218;&#12398;&#12354;&#12356;&#12373;&#12388;&#12392;&#12363; . . Note: ã“ã®ãƒ–ãƒ­ã‚°ã¯å¼·åŒ–å­¦ç¿’è‹¥æ‰‹ã®ä¼š Advent Calendar 2021 18æ—¥ç›®ã®è¨˜äº‹ã¨ã—ã¦æ›¸ã‹ã‚Œã¾ã—ãŸ ã“ã‚“ã«ã¡ã¯ã€‚ ã‚³ãƒ­ãƒŠç¦ã‚‚çµ‚ã‚ã‚ŠãŒè¦‹ãˆã¤ã¤ã‚ã‚‹ï¼ˆã¨æ€ã£ãŸã‚‰å¤‰ç•°æ ªãŒ...ï¼‰2021å¹´ã‚‚ã‚ã¨ã‚ãšã‹ã€‚å¯’ã•ã‚‚å³ã—ããªã£ã¦ãã¾ã—ãŸãŒã€çš†ã•ã‚“å¦‚ä½•ãŠéã”ã—ã§ã—ã‚‡ã†ã‹ã€‚ åƒ•ã¯æ²–ç¸„ã«ã„ã‚‹ã®ã§ã‚ã¾ã‚Šå¯’ãã¯ãªã„ã—è™«æ­¯ã®æ²»ç™‚ä¸­ã§å¥¥æ­¯ã«ç©´ãŒç©ºã„ã¦ã„ã‚‹ä»¥å¤–ã¯ãŠãŠã‚€ã­å…ƒæ°—ã§ã™ã€‚ . è‹¥æ‰‹ã®ä¼šã®ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã¨ã„ã†ã“ã¨ã§ã€å›½å†…ã§ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®æ´»å‹•ã«ã¤ã„ã¦ã€æœ€åˆã«ç·æ‹¬ã—ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚ . è‹¥æ‰‹ã®ä¼šã§ã¯æ¨¡å€£å­¦ç¿’ã®å‹‰å¼·ä¼šã‚’ã—ã¾ã—ãŸãŒã€çµå±€2å›ã—ã‹ç¶šãã¾ã›ã‚“ã§ã—ãŸã€‚ | è‹¦æ‰‹ã®ä¼šã®ã‚‚ãã‚‚ãä¼šã¯ãƒ•ãƒªã‚¹ãƒ“ãƒ¼ã‚„ãƒãƒ©ã‚½ãƒ³ã®ç·´ç¿’ã¨ãƒãƒƒãƒ†ã‚£ãƒ³ã‚°ã—ã¦ã‚„ã‚‰ãªããªã£ã¦ã—ã¾ã„ã¾ã—ãŸãŒã€æœ€è¿‘æ—¥ç¨‹ã‚’å¤‰ãˆã¦ã€ç«æ›œæ—¥ã®å¤œã«å§‹ã‚ã¾ã—ãŸã€‚æš‡ãªæ–¹ä¸€ç·’ã«ã‚‚ãã‚‚ãã—ã¾ã—ã‚‡ã†ã€‚ | å¼·åŒ–å­¦ç¿’ã®è¬›ç¾©è³‡æ–™ã®ç¿»è¨³ã‚’ã—ã¦ã„ã¾ã™ã€‚é›£ã—ã„ã§ã™ãŒã€ã‘ã£ã“ã†å‹‰å¼·ã«ãªã‚Šã¾ã™ã€‚æœ‰é™ã‚µãƒ³ãƒ—ãƒ«ã§ã®ãƒã‚¦ãƒ³ãƒ‰ã‚’ã€åˆã‚ã¦å‹‰å¼·ã—ã¾ã—ãŸã€‚èˆˆå‘³ãŒã‚ã‚‹æ–¹ã¯ã€ãœã²ä¸€ç·’ã«ã‚„ã‚Šã¾ã—ã‚‡ã†ã€‚ | æ˜¨å¹´ã®ãƒ–ãƒ­ã‚°ã‚’æ›¸ã„ã¦ã‹ã‚‰ã¯ã‚„ä¸€å¹´ã€ã›ã£ã‹ãå°‚ç”¨ã®ãƒ–ãƒ­ã‚°ã‚’ä½œã£ãŸã®ã§ä»Šå¹´ã‚‚ã„ã„æ„Ÿã˜ã«matplotlibèŠ¸å¼·åŒ–å­¦ç¿’ã®è¨˜äº‹ã‚’æ›¸ã„ã¦ã„ããŸã„ã¨æ€ã£ã¦ã„ã¾ã—ãŸãŒã€çµå±€ä½•ã‚‚æ›¸ãã¾ã›ã‚“ã§ã—ãŸã€‚ | . æœ€è¿‘ã¯äººå·¥é€²åŒ–ã‚„äººå·¥ç”Ÿå‘½ã®ç ”ç©¶ã‚‚å§‹ã‚ãŸã®ã§ã€ã‚‚ã¯ã‚„ã€Œå¼·åŒ–å­¦ç¿’ã®äººã€ã¨åä¹—ã£ã¦ã„ã„ã®ã‹ã‚ˆãã‚ã‹ã‚Šã¾ã›ã‚“ãŒã€ä»Šå¾Œã‚‚å›½å†…ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ä½•ã‹è²¢çŒ®ã§ãã‚Œã°ã¨æ€ã„ã¾ã™ã€‚ . ä»Šå¹´ã¯å¼·åŒ–å­¦ç¿’ã«å¯¾ã™ã‚‹æ¥½è¦³è«–ã‚‚æ‚²è¦³è«–ã‚‚å¤šãç›®ã«ã—ãŸä¸€å¹´ã§ã—ãŸã€‚ David Silverã‚„Suttonã¯Reward is Enoughã¨ã„ã†å¼·æ°—ãªè«–æ–‡ã‚’å‡ºã—ã€çŸ¥çš„ãªã‚·ã‚¹ãƒ†ãƒ ã¯ãŠã‚ˆãå…¨ã¦å ±é…¬æœ€å¤§åŒ–ã§ä½œã‚Œã‚‹ã¨ä¸»å¼µã—ã¾ã—ãŸã€‚ ã•ã™ãŒã«å¼·æ°—ã™ãã‚‹ã¨æ€ã„ã¾ã™ãŒã€ãã®å¾ŒReward is enough for convex MDPsã‚„On the Expressivity of Markov Rewardã¨ã„ã£ãŸãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãªè«–æ–‡ãŒå‡ºã¦ããŸã®ã¯é¢ç™½ã„ã§ã™ã€‚ ã¾ãŸã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ãƒ»æ•™å¸«ãªã—å¼·åŒ–å­¦ç¿’ã®è«–æ–‡ãŒå¢—ãˆã¦ããŸã¨æ€ã„ã¾ã™ã€‚ ã–ã£ãã‚Šã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ = å¼·åŒ–å­¦ç¿’ - æ¢ç´¢ã€æ•™å¸«ãªã—å¼·åŒ–å­¦ç¿’ = å¼·åŒ–å­¦ç¿’ - å ±é…¬ã¨æ€ã£ã¦ã‚‚ã‚‰ã£ã¦å•é¡Œãªã„ã§ã—ã‚‡ã†ã€‚ ä½•ã‚’éš ãã†åƒ•ã®ä¿®å£«è«–æ–‡ã‚‚å˜ãªã‚‹ã€Œéšå±¤å‹å¼·åŒ–å­¦ç¿’ã€ã ã£ãŸã®ã§ã™ãŒã€ãƒªã‚¸ã‚§ã‚¯ãƒˆè«¸èˆ¬ã®äº‹æƒ…ã«ã‚ˆã‚Šæ•™å¸«ãªã—ã«é­”æ”¹é€ ã—ã¦å†æŠ•ç¨¿ã—ã¾ã—ãŸã€‚ Sergey Levineã«ã„ãŸã£ã¦ã¯Understanding the World Through Actionã¨ã„ã†ã‚¿ã‚¤ãƒˆãƒ«ãŒå¼·ã„è«–æ–‡ã®ä¸­ã§ã€ã€Œå¤§é‡ã«ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã¦ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ•™å¸«ãªã—å¼·åŒ–å­¦ç¿’ã‚’ã™ã‚Œã°ä¸–ç•Œã‚’ç†è§£ã§ãã‚‹ï¼ˆâ‰’ä¸–ç•Œã‚’ç†è§£ã—ã¦ã„ã‚‹ã®ã¨åŒç­‰ã®ã‚·ã‚¹ãƒ†ãƒ ãŒä½œã‚Œã‚‹ï¼Ÿï¼‰ã€ã¨è¨€ã£ã¦ã„ã¾ã™ã€‚é¢ç™½ã„æ–¹å‘æ€§ã ã¨æ€ã„ã¾ã™ã€‚ ä¸€æ–¹ã§ã€ã¿ã‚“ãªå¤§å¥½ããƒ«ãƒ¼ãƒ“ãƒƒã‚¯ã‚­ãƒ¥ãƒ¼ãƒ–è«–æ–‡ã‚’å‡ºã—ãŸOpen AIã®ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ãƒãƒ¼ãƒ ã¯ã€ã€Œã¨ã‚Šã‚ãˆãšä»Šãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹é ˜åŸŸã«æ³¨åŠ›ã™ã‚‹ã€ã¨ã®ã“ã¨ã§è§£æ•£ã—ã¦ã—ã¾ã„ã¾ã—ãŸã€‚ ã“ã®ãƒ–ãƒ­ã‚°ã‚’æ›¸ã„ã¦ã„ã‚‹æœ€ä¸­ã«WebGPTã®è«–æ–‡ã‚’ç›®ã«ã—ã¾ã—ãŸãŒã€ä»Šå¾Œã¯è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‹å¼·åŒ–å­¦ç¿’ã§è‰²ã€…ã‚„ã£ã¦ã„ãã®ã§ã—ã‚‡ã†ã‹ã€‚å“å·ã•ã‚“ã¯å–œã³ãã†ã§ã™ãŒã€åƒ•ãªã‚“ã‹ã¯ã“ã†ã„ã†åˆ°åº•è‡ªåˆ†ã§ã§ããªã„ã‚‚ã®ã¯ã€Œãƒ†ãƒ¬ãƒ“ã®ä¸­ã®ç ”ç©¶ã€ã¨ã„ã†æ„Ÿã˜ãŒã—ã¦ä¸€æ­©å¼•ã„ã¦ã—ã¾ã„ã¾ã™ï¼ˆæœ€è¿‘ã¯ã€ãƒ†ãƒ¬ãƒ“ã¨ã‹ãŸã¨ãˆã«ä½¿ã†ã¨å¤ã„ã®ã‹ãª...ï¼‰ã€‚ Open AIã®ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã¯ã€Sim2Realã«ã“ã ã‚ã‚Šã™ããŸã®ã§ã¯ï¼Ÿã¨ã„ã†æ„è¦‹ã‚’æŸæ‰€ã§ãŠèãã—ã¾ã—ãŸã€‚å®Ÿéš›ãã†ãªã®ã‹ã¯çŸ¥ã‚Šã¾ã›ã‚“ãŒã€å¤§è¦æ¨¡ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦Sim2Realã‚’é ‘å¼µã‚‹ã®ã‹ã€å®Ÿæ©Ÿã®ãƒ‡ãƒ¼ã‚¿ã§é ‘å¼µã‚‹ã®ã‹ã¨ã„ã†ã®ã¯ã€é¢ç™½ã„è¦–ç‚¹ã§ã™ã‚ˆã­ã€‚ . Open AIãŒä»Šã¾ã§ã»ã©å¼·åŒ–å­¦ç¿’ã«æ³¨åŠ›ã—ãªããªã£ãŸã“ã¨ã§ã€Open AI gymã‚’ã¯ã˜ã‚å¼·åŒ–å­¦ç¿’ç ”ç©¶ã§ä½¿ã‚ã‚Œã¦ããŸã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ç¾¤ã«ã‚‚ã€è‰²ã€…ã¨æƒ…å‹¢ã®å¤‰åŒ–ãŒã‚ã‚Šãã†ã§ã™ã€‚ . OpenAI Gymã®ãƒ¡ãƒ³ãƒ†ãƒŠãŒå¤‰ã‚ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã‹ã‚‰ã¯Open AIã§ã¯ãªããƒ¡ãƒªãƒ¼ãƒ©ãƒ³ãƒ‰å¤§å­¦ã®å­¦ç”Ÿã•ã‚“ãŒãƒ¡ãƒ³ãƒ†ãƒŠã«ãªã‚‹ã‚ˆã†ã§ã™ã€‚mujoco-pyãªã©é–¢é€£ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã¤ã„ã¦ã¯ç›¸å¤‰ã‚ã‚‰ãšæ”¾ç½®ã•ã‚Œã¦ã„ã¾ã™ã€‚ | DeepmindãŒMuJoCoã‚’è²·ã„å–ã£ã¦ç„¡æ–™ã«ã—ã¾ã—ãŸã€‚ä»Šå¾Œã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚‚å…¬é–‹ã•ã‚Œã‚‹ã‚ˆã†ã§ã™ã€‚ | Googleã‹ã‚‰æ–°ã—ãbraxã¨ã„ã†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãŒå…¬é–‹ã•ã‚Œã¾ã—ãŸã€‚ | ãã‚“ãªã‚ã‘ã§ã€åƒ•ã¯ã“ã‚Œã¾ã§mujoco-py + gymã§ä½œæˆã—ãŸã‚«ã‚¹ã‚¿ãƒ ç’°å¢ƒã§ãŸãã•ã‚“å®Ÿé¨“ã‚’ã‚„ã£ã¦ãã¾ã—ãŸãŒã€MuJoCoã‚’ä½¿ã†ã«ã—ã¦ã‚‚dm_controlã‚’ä½¿ã†ã¨ã‹ã€ã¯ãŸã¾ãŸbraxã«ã—ã¦ã—ã¾ã†ã¨ã‹ã€åˆ¥ã®é¸æŠè‚¢ã‚’æ¤œè¨ã—ãŸããªã£ã¦ãã¾ã—ãŸã€‚ ã“ã®ãƒ–ãƒ­ã‚°ã§ã¯ã€ã¨ã‚Šã‚ãˆãšbraxã‚’è©¦ã—ã¦ã¿ã‚ˆã†ã¨æ€ã„ã¾ã™ã€‚ . 1. &#12399;&#12376;&#12417;&#12395;: &#12471;&#12511;&#12517;&#12524;&#12540;&#12471;&#12519;&#12531;&#12539;&#35598;&#12525;&#12508;&#12483;&#12488;&#12539;GPU . æœ¬é¡Œã«å…¥ã‚Šã¾ã™ãŒã€ã–ã£ãã‚Šã€å¼·åŒ–å­¦ç¿’ã¨ã¯ã€å ±é…¬ã‹ã‚‰è¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹æ çµ„ã¿ã ã¨è¨€ã†ã“ã¨ãŒã§ãã¾ã™ã€‚ ã§ã¯ä½•ã®è¡Œå‹•ã‚’å­¦ç¿’ã•ã›ãŸã„ã®ã§ã—ã‚‡ã†ã‹ã€‚ ã‚²ãƒ¼ãƒ ã®AIã ã£ãŸã‚Šã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã ã£ãŸã‚Šã€è‰²ã€…ãªé¸æŠè‚¢ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€ã©ã†ã„ã†ã‚ã‘ã‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ä¸Šã§å‹•ãè¬ãƒ­ãƒœãƒƒãƒˆã¨ã„ã†ã®ãŒãƒãƒ”ãƒ¥ãƒ©ãƒ¼ãªé¸æŠè‚¢ã§ã™ã€‚ . ã“ã®ãƒ–ãƒ­ã‚°ã‚’ã”ã‚‰ã‚“ã®æ–¹ã®ä¸­ã«ã¯ã€ã“ã†ã„ã£ãŸãƒŠãƒŠãƒ•ã‚·ã®ã‚ˆã†ãªè¬ãƒ­ãƒœãƒƒãƒˆã®ç”»åƒã‚’ç›®ã«ã—ãŸã“ã¨ãŒã‚ã‚‹æ–¹ã‚‚å¤šã„ã®ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹ã€‚ . {: width=&quot;300&quot; } . ã“ã‚Œã¯Open AI gymã®HalfCheetahã¨ã„ã†ãƒ­ãƒœãƒƒãƒˆã§ã™ã€‚è¶³ãŒ2æœ¬ãªã®ã§ãƒãƒ¼ãƒ•ãªã®ã ã¨æ€ã„ã¾ã™ãŒã€ãªã‚“ã¨ã‚‚æ®‹é…·ãªãƒãƒ¼ãƒŸãƒ³ã‚°ã§ã™ã€‚æ„›ç©ã•ã‚Œã‚‹ãŸã‚ç—…æ°—ã®ã¾ã¾å“ç¨®æ”¹è‰¯ã•ã‚Œã¦ããŸçŠ¬çŒ«ã®ã‚ˆã†ãªå“€æ„ãŒæ¼‚ã„ã¾ã™ã€‚ . MuJoCoã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã«ã€Œã“ã“ã¨ã“ã“ãŒã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã§ã€å¯å‹•åŸŸã¯ã“ã†ã§ã™ã€‚åºŠã¯ç™½é»’ã§ãŠé¡˜ã„ã—ã¾ã™ã€ã¿ãŸã„ãªXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¸¡ã™ã¨ã€ã“ã†ã„ã†ãƒ­ãƒœãƒƒãƒˆã‚’ä½œã£ã¦ãã‚Œã¾ã™ã€‚ ã‚‚ã—ãã¯ã€dm_controlãªã©ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«XMLã‚’ä½œã‚‰ã›ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ ã“ã®ã‚ˆã†ãªè¬ãƒ­ãƒœãƒƒãƒˆãŒå®Ÿé¨“ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹è¦å› ã¨ã—ã¦ã€ . ã¿ã‚“ãªãŒä½¿ã£ã¦ã„ã‚‹ã‹ã‚‰ | Atariãªã©ã®ã‚²ãƒ¼ãƒ ã‚ˆã‚Šé«˜é€Ÿ | ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã®é€Ÿã•ãƒ»ä½ç½®ãªã©ã®å®Œå…¨ãªå†…éƒ¨çŠ¶æ…‹ãŒæ‰‹ã«å…¥ã‚‹ ãƒãƒ«ã‚³ãƒ•æ€§ã«ã¤ã„ã¦å¿ƒé…ã—ãªãã¦ã‚‚ã„ã„ | . | è‰²ã€…ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¦ä¾¿åˆ©ã ã‹ã‚‰ | æ™®é€šã®ãƒ­ãƒœãƒƒãƒˆã‚’è¨“ç·´ã™ã‚‹ãŸã‚ã®ãƒ†ã‚¹ãƒˆã«ã¡ã‚‡ã†ã©ã„ã„ã‹ã‚‰ ãªã©ã®ç†ç”±ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€ãªã‚“ã ã‹ã‚“ã ã¿ã‚“ãªãŒä½¿ã£ã¦ã„ã‚‹ã‹ã‚‰ã¨ã„ã†ã®ãŒå¤§ãã„æ°—ãŒã—ã¾ã™ã€‚ | . ã¨ã“ã‚ã§ã€ã“ã®MuJoCoã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¨ã„ã†ã®ã¯éå¸¸ã«é«˜é€Ÿã«å‹•ä½œã™ã‚‹ã®ã§ã™ãŒã€CPUä¸Šã§ã—ã‹å‹•ä½œã—ã¾ã›ã‚“ã€‚ ä»Šæ—¥ä½¿ã‚ã‚Œã¦ã„ã‚‹æ·±å±¤å­¦ç¿’ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ãã®è¨ˆç®—é‡ã®ã»ã¨ã‚“ã©ã‚’å ã‚ã‚‹è¡Œåˆ—æ¼”ç®—ãŒãƒ™ã‚¯ãƒˆãƒ«ä¸¦åˆ—åŒ–ã¨ã¨ã¦ã‚‚ç›¸æ€§ãŒã„ã„ãŸã‚ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚„ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãããªã‚Œã°ãªã‚‹ã»ã©GPUä¸Šã§é«˜é€Ÿã«å‹•ä½œã—ã¾ã™ã€‚ ã¨ãªã‚‹ã¨ã€GPUã§å­¦ç¿’ã‚’å›ã—ã¦ã„ã‚‹å ´åˆã€ã©ã†ã—ã¦ã‚‚CPUã‹ã‚‰GPUã«ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã™ã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç™ºç”Ÿã—ã€é«˜é€ŸåŒ–ã®å¦¨ã’ã«ãªã‚Šã¾ã™ã€‚ ãã“ã§ã€GPUä¸Šã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œãˆã‚‹ã‚ˆã†ã«ã—ãŸã®ãŒã€ä»Šå›ç´¹ä»‹ã™ã‚‹braxã¨ã„ã†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§ã™ã€‚ . 2. Jax&#12391;numpy&#28436;&#31639;&#12434;&#39640;&#36895;&#21270;&#12375;&#12390;&#12415;&#12427; . ã§ã¯ã€braxã¯CUDAã‹ä½•ã‹ã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã®ã‹ãªï¼Ÿã¨æ€ã£ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ãªã‚“ã¨å…¨ã¦Pythonã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã®ã§ã™ã€‚ ãã®éµã¨ãªã‚‹ã®ãŒjaxã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ ãŠã‚‚ã‚€ã‚ã«ã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . ! pip install jax . ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†’é ­ã«&#39;JAX is Autograd and XLA&#39;ã¨ã‚ã‚Šã¾ã™ãŒã€Jaxã¯ . Numpyæ¼”ç®—ã‚’XLAã«å¤‰æ›ã™ã‚‹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©(Tensorflow) jax.jit | XLAã¯Tensorflowã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦é–‹ç™ºã•ã‚ŒãŸä¸­é–“è¨€èªã§ã€GPU/TPUç”¨ã«ã™ã”ãé€Ÿã„ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã§ãã‚‹ | . | Numpyæ¼”ç®—ã‚’è¿½è·¡ã—ã¦å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹æ©Ÿèƒ½ jax.grad/jax.vjp ãªã© ã®2ã¤ã®ã‚³ã‚¢æ©Ÿèƒ½ã‚’æ ¸ã¨ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ ã“ã®ãƒãƒ£ãƒ—ã‚¿ãƒ¼ã§ã¯ã€ã²ã¨ã¾ãšå‰è€…ã®ã€ŒXLAã«å¤‰æ›ã™ã‚‹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã€ã¨ã—ã¦ã®æ©Ÿèƒ½ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã¿ã¾ã™ã€‚ | . | . ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã¯JITæ–¹å¼ã§å®Ÿè£…ã•ã‚Œã¦ãŠã‚Šã€ . jax.jitã«é–¢æ•°fã‚’æ¸¡ã™ (f_compiled = jax.jit(f)ï¼‰ | ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã‚‹é–¢æ•°f_compiledã‚’æœ€åˆã«å‘¼ã³å‡ºã—ãŸã¨ãã€jaxã¯Pythonã®é–¢æ•°ã‚’XLAã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ | 2å›ç›®ä»¥é™é–¢æ•°å‘¼ã³å‡ºã—ãŒé«˜é€Ÿã«ãªã‚‹ ã¨ã„ã†å‡¦ç†ã®æµã‚Œã«ãªã‚Šã¾ã™ã€‚ | ã§ã¯ã€ã•ã£ããä½•ã‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ é©å½“ã«å¤©äº•ã‹ã‚‰ãƒœãƒ¼ãƒ«ã‚’è½ã¨ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . import typing as t import numpy as np from IPython.display import HTML, clear_output try: import pandas as pd import seaborn as sns from celluloid import Camera from matplotlib import pyplot as plt from matplotlib.animation import ArtistAnimation except ImportError as _e: ! pip isntall pandas seaborn celluloid clear_output() import pandas as pd import seaborn as sns from celluloid import Camera from matplotlib import pyplot as plt from matplotlib.animation import ArtistAnimation sns.set_theme(style=&quot;darkgrid&quot;) Array = np.ndarray GRAVITY = -9.8 def move_balls( ball_positions: Array, ball_velocities: Array, delta_t: float = 0.1, ) -&gt; Array: accel_x = np.zeros(ball_positions.shape[0]) accel_y = np.ones(ball_positions.shape[0]) * GRAVITY * delta_t # yæ–¹å‘ã«GÎ”tåŠ é€Ÿ new_velocities = np.stack((accel_x, accel_y), axis=1) + ball_velocities new_positions = ball_positions + delta_t * new_velocities return new_positions, new_velocities def simulate_balls( n_balls: int, n_steps: int = 100, forward: t.Callable[[Array], Array] = move_balls, ) -&gt; t.List[Array]: p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0) v = np.random.randn(n_balls, 2) results = [p] for _ in range(n_steps): p, v = forward(p, v) results.append(p) return results . é©å½“ã«ãƒœãƒ¼ãƒ«ã‚’20å€‹è½ã¨ã—ã¦ã¿ã¾ã™ã€‚ . def ball_animation(balls: t.Iterable[Array]) -&gt; ArtistAnimation: fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot() ax.set_xlim(-50, 50) ax.set_ylim(-50, 50) camera = Camera(fig) for ball_batch in balls: ax.scatter(ball_batch[:, 0], ball_batch[:, 1], color=&quot;red&quot;, alpha=0.7) camera.snap() return camera.animate() HTML(ball_animation(simulate_balls(20, 40)).to_jshtml()) . . &lt;/input&gt; Once Loop Reflect ã§ã¯ã€ã“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã™ã‚‹ã®ã«ã€ã©ã‚Œãã‚‰ã„æ™‚é–“ãŒã‹ã‹ã‚‹ã§ã—ã‚‡ã†ã‹ã€‚ãƒœãƒ¼ãƒ«ã®æ•°ã‚’å¤‰ãˆã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . def bench( f: t.Callable[..., t.Any], inputs: t.Iterable[t.Any], number: int = 10, ) -&gt; t.List[float]: import timeit return [timeit.Timer(lambda: f(x)).timeit(number=number) for x in inputs] def bench_and_plot(f: t.Callable[..., t.Any], title: str) -&gt; pd.DataFrame: inputs = [4000, 8000, 16000, 32000, 64000] result = pd.DataFrame({&quot;x&quot;: inputs, &quot;y&quot;: bench(f, inputs)}) result[&quot;Method&quot;] = [title] * len(inputs) ax = sns.lineplot(data=result, x=&quot;x&quot;, y=&quot;y&quot;) ax.set_title(title) ax.set_xlabel(&quot;Num. of balls&quot;) ax.set_ylabel(&quot;Time (sec.)&quot;) return result numpy_result = bench_and_plot(simulate_balls, &quot;NumPy&quot;) . ãŠãŠã‚€ã­ç·šå½¢ã«å®Ÿè¡Œæ™‚é–“ãŒå¢—ãˆã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ã€jaxã‚’ä½¿ã£ã¦é«˜é€ŸåŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ åŸºæœ¬çš„ã«ã¯numpyã‚’jax.numpyã«ç½®ãæ›ãˆã‚Œã°ã„ã„ã§ã™ã€‚ . import jax import jax.numpy as jnp JaxArray = jnp.DeviceArray def move_balls_jax( ball_positions: JaxArray, ball_velocities: JaxArray, delta_t: float = 0.1, ) -&gt; JaxArray: accel_x = jnp.zeros(ball_positions.shape[0]) accel_y = jnp.ones(ball_positions.shape[0]) * GRAVITY * delta_t new_velocities = jnp.stack((accel_x, accel_y), axis=1) + ball_velocities new_positions = ball_positions + delta_t * new_velocities return new_positions, new_velocities . ã§ã¯åŒã˜ã‚ˆã†ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ã¨ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . jax_nojit_result = bench_and_plot( lambda n: simulate_balls(n, forward=move_balls_jax), &quot;JAX (without JIT)&quot;, ) . è¬ã®æŒ™å‹•ã‚’è¦‹ã›ã¦ã„ã‚‹ã—ã€ã™ã”ãé…ã„ã§ã™ã­ã€‚ä»Šåº¦ã¯JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ jax.jit(f, backend=&quot;cpu&quot;)ã§é–¢æ•°ã‚’CPUä¸Šã§å‹•ãXLAã‚³ãƒ¼ãƒ‰ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§ãã¾ã™ã€‚ . jax_cpu_result = bench_and_plot( lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=&quot;cpu&quot;)), &quot;JAX (with JIT on CPU)&quot;, ) . ã™ã”ãé€Ÿããªã‚Šã¾ã—ãŸã€‚ä»Šåº¦ã¯GPUã§ã‚„ã£ã¦ã¿ã¾ã™ã€‚ . jax_gpu_result = bench_and_plot( lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=&quot;gpu&quot;)), &quot;JAX (with JIT for GPU)&quot;, ) . åœ§å€’çš„ã«é€Ÿã„ã§ã™ã­ã€‚ä¸€å¿œç·šå½¢ã«å®Ÿè¡Œæ™‚é–“ãŒå¢—ãˆã¦ã¯ã„ã¾ã™ãŒ...ã€‚ ãªãŠã€ä»Šå›ã¯å­¦å†…ã‚¹ãƒ‘ã‚³ãƒ³ã®NVIDIA P100 GPUã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ . ax = sns.lineplot( data=pd.concat( [numpy_result, jax_nojit_result, jax_cpu_result, jax_gpu_result], ignore_index=True, ), x=&quot;x&quot;, y=&quot;y&quot;, style=&quot;Method&quot;, hue=&quot;Method&quot;, ) ax.set_title(&quot;Ball benchmark&quot;) ax.set_xlabel(&quot;Num. of balls&quot;) ax.set_ylabel(&quot;Time (sec.)&quot;) None . . ã“ã®ãƒœãƒ¼ãƒ«ã®æ•°ã ã¨GPUã¯ç·šå½¢ã«è¨ˆç®—æ™‚é–“ãŒå¢—ãˆã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã›ã‚“ã­ã€‚ ã¾ã‚ä½•ã¯ã¨ã‚‚ã‚ã‚Œã€GPUç”¨ã«JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ã‚ã’ã‚‹ã¨é€Ÿãã†ã ãªã‚ã€ã¨ã„ã†æ„Ÿã˜ãŒã—ã¾ã™ã€‚ . 3. Jax&#12391;&#21246;&#37197;&#12434;&#35336;&#31639;&#12375;&#12390;&#12415;&#12427; . Jaxã¯å˜ã«é€Ÿã„NumPyã¨ã—ã¦ã®æ©Ÿèƒ½ã«åŠ ãˆã€è‡ªå‹•å¾®åˆ†ã«ã‚ˆã£ã¦ã€é–¢æ•°$f(x, y, z, ...)$ã®å„$x, y, z,...$ã«ã‚ˆã‚‹åå¾®åˆ†$ frac{ partial f}{ partial x}, frac{ partial f}{ partial y}, frac{ partial f}{ partial z}, ...$ã‚’è¨ˆç®—ã™ã‚‹æ©Ÿèƒ½ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã“ã§ã¯jax.gradã«ã‚ˆã‚‹å‹¾é…ã®è¨ˆç®—ã ã‘ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ . ãªã‚“ã‹ã€é©å½“ã«é–¢æ•°ã‚’æœ€é©åŒ–ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãšã¯ã€é©å½“ã«é–¢æ•°ã‚’æ±ºã‚ã¦ã¿ã¾ã™ã€‚ $z = x^2 + y^2 + y$ ã«ã—ã¾ã—ãŸã€‚ . def f(x, y): return x ** 2 + y ** 2 + y def plot_f(traj: t.Optional[Array] = None) -&gt; None: x, y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100)) fig = plt.figure() ax = fig.add_subplot(111, projection=&quot;3d&quot;) ax.plot_surface( x, y, f(x, y), cmap=sns.color_palette(&quot;flare&quot;, as_cmap=True), alpha=0.8, linewidth=0, ) if traj is not None: ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=&quot;blue&quot;) ax.set_xlabel(&quot;x&quot;, fontsize=14) ax.set_ylabel(&quot;y&quot;, fontsize=14) ax.set_zlabel(&quot;z&quot;, fontsize=14, horizontalalignment=&quot;right&quot;) ax.set_title(&quot;f&quot;, fontsize=15) plot_f() . $(x, y) = (5, 5)$ã§ã®ã“ã®é–¢æ•°ã®å‹¾é…ã‚’è¨ˆç®—ã—ã¦ã¿ã¾ã™ã€‚å‹¾é…ã‚’è¨ˆç®—ã—ã¦ã»ã—ã„å¼•æ•°ã‚’jax.jit(argnums=...)ã§æŒ‡å®šã—ã¾ã™ã€‚ . jax.grad(f, argnums=(0, 1))(jnp.array(5.0), jnp.array(5.0)) . (DeviceArray(10., dtype=float32, weak_type=True), DeviceArray(11., dtype=float32, weak_type=True)) . $ frac{ partial z}{ partial x}, frac{ partial z}{ partial y}$ã‚’è¨ˆç®—ã—ã¦ãã‚Œã¾ã—ãŸã€‚ ã›ã£ã‹ããªã®ã§ã€æœ€æ€¥é™ä¸‹æ³•ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . def steepest_descent(alpha: float = 0.01) -&gt; JaxArray: f_grad = jax.grad(f, argnums=(0, 1)) x, y = jnp.array(5.0), jnp.array(5.0) traj = [] while True: traj.append((x, y, f(x, y))) x_grad, y_grad = f_grad(x, y) if jnp.linalg.norm(jnp.array([x_grad, y_grad])) &lt; 0.05: break x -= alpha * x_grad y -= alpha * y_grad return jnp.array(traj) plot_f(steepest_descent()) . æœ€æ€¥é™ä¸‹æ–¹å‘ã«é€²ã‚“ã§ãã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚ ã¨ã“ã‚ã§ã€gradã¯ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³å‹ãƒªãƒãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰è‡ªå‹•å¾®åˆ†ï¼ˆèª¤å·®é€†ä¼æ’­æ³•ã®é›£ã—ã„è¨€ã„æ–¹ã§ã™ï¼‰ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã®ã§ã€ãƒªãƒãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§Vector Jacobian Productã‚’è¨ˆç®—ã™ã‚‹vjpã¨ã„ã†é–¢æ•°ãŒä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚ ãƒ•ã‚©ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã§è¨ˆç®—ã™ã‚‹jvpã¨ã„ã†é–¢æ•°ã‚‚ã‚ã‚Šã¾ã™ã€‚ ã“ã®ã‚ãŸã‚Šã®æ©Ÿèƒ½ã¯ã€ãŸã ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å­¦ç¿’ã•ã›ãŸã„ã ã‘ãªã‚‰ã»ã¨ã‚“ã©ä½¿ã„ã¾ã›ã‚“ãŒã€ä¸€å¿œã‚„ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . primals, f_vjp = jax.vjp(f, 5.0, 5.0) print(f&quot;VJP value: {primals.item()} grad: {[x.item() for x in f_vjp(1.0)]}&quot;) value, grad = jax.jvp(f, (5.0, 5.0), (1.0, 1.0)) print(f&quot;JVP value: {value.item()} grad: {grad.item()}&quot;) . VJP value: 55.0 grad: [10.0, 11.0] JVP value: 55.0 grad: 21.0 . ãƒ•ã‚©ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆå‹¾é…ã¨ãªã‚“ã‹ã®ãƒ™ã‚¯ãƒˆãƒ«vã¨ã®å†…ç©ãŒã§ã¦ãã¾ã™ã€‚ ã“ã®ã‚ãŸã‚Šã€è‰²ã€…ãªæ•™ç§‘æ›¸ã«æ›¸ã„ã¦ã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€Forward modeã¨Reverse modeã®é•ã„ãªã©ã€Probabilistic Machine Learning: An Introductionã®13ç« ãŒç‰¹ã«ã‚ã‹ã‚Šã‚„ã™ã„ã¨æ€ã„ã¾ã™ã€‚èˆˆå‘³ãŒã‚ã‚‹æ–¹ã¯å‚è€ƒã«ã—ã¦ã¿ã¦ãã ã•ã„ã€‚ . 4. Brax&#12434;&#20351;&#12387;&#12390;&#12415;&#12427; . ã˜ã‚ƒã‚MuJoCoã¿ãŸã„ãªç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚‚Jaxã§æ›¸ã„ã¦ã—ã¾ãˆã°å‹æ‰‹ã«GPUä¸Šã§å‹•ã„ã¦é€Ÿã„ã‚“ã˜ã‚ƒãªã„ï¼Ÿã¨ã„ã†ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã§ä½œã‚‰ã‚ŒãŸã®ãŒbraxã§ã™ã€‚ ç°¡å˜ã«ç‰¹å¾´ã‚’ã¾ã¨ã‚ã¦ã¿ã¾ã™ã€‚ . Jaxã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€jitã§é«˜é€ŸåŒ–ã§ãã‚‹ | Protocol Bufferã§ã‚·ã‚¹ãƒ†ãƒ ã‚’å®šç¾© (cf. MuJoCoã¯XMLï¼‰ | dataclassQPã‚’ä½¿ã£ãŸç°¡æ½”ãªçŠ¶æ…‹è¨˜è¿° Qã¯æ­£æº–åº§æ¨™ã€Pã¯é‹å‹•é‡ã‚‰ã—ã„ | . | OpenAI gymé¢¨ã®Env APIã‚„Antãƒ»Halfcheetahãªã©ã®è¬ãƒ­ãƒœãƒƒãƒˆ | . ãŠã‚‚ã‚€ã‚ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã¿ã¾ã™ã€‚ . try: import brax except ImportError: !pip install git+https://github.com/google/brax.git@main clear_output() import brax . ã•ã£ãã¨åŒã˜ã€ãƒœãƒ¼ãƒ«ã‚’å‹•ã‹ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã•ã£ãã¯xyåº§æ¨™ã§å‹•ã‹ã—ã¾ã—ãŸãŒã€brax . def make_ball() -&gt; None: config = brax.Config(dt=0.1, substeps=4) # ãƒœãƒ¼ãƒ«ã‚’è¿½åŠ  ball = config.bodies.add(name=&quot;ball&quot;, mass=1) capsule = ball.colliders.add().capsule capsule.radius = 0.5 # yåº§æ¨™ã«é‡åŠ› config.gravity.y = GRAVITY return config def make_qp(p, v) -&gt; brax.QP: return brax.QP( pos=jnp.array([[p[0], p[1], 0.0]]), # position vel=jnp.array([[v[0], v[1], 0.0]]), # velocity rot=jnp.zeros((1,4)), # rotation ang=jnp.zeros((1, 3)), # angular velocity ) def simulate_one_ball_brax(n_steps: int = 100) -&gt; t.List[Array]: sys = brax.System(make_ball()) qp = make_qp([0.0, 48.0], [1.0, 0.0]) results = [] for _ in range(n_steps): qp, _ = sys.step(qp, []) results.append(qp.pos[:2]) return results HTML(ball_animation(simulate_one_ball_brax(40)).to_jshtml()) . &lt;/input&gt; Once Loop Reflect ã“ã“ã§ã€4ã¤ã®APIã‚’ä½¿ã„ã¾ã—ãŸã€‚ . brax.Configã§ã‚·ã‚¹ãƒ†ãƒ ã‚’å®šç¾© | brax.System(config)ã§ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ | brax.QPã§åˆæœŸä½ç½®ãƒ»é€Ÿåº¦ãƒ»ã‚¢ãƒ³ã‚°ãƒ«ç­‰ã‚’ä½œæˆ | brax.System.step(qp, ...)ã§1ã‚¹ãƒ†ãƒƒãƒ—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸçµæœã‚’å–å¾— | . ãƒœãƒ¼ãƒ«ãŒä¸€ã¤ã ã¨ãªã‚“ã¨ãªãç‰©è¶³ã‚Šãªã„ã§ã™ã­ã€‚å¢—ã‚„ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ãã®ãŸã‚ã«ã¯ã€jax.vmapã§sys.stepã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã™ã€‚ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã€vmapã¯å¼•æ•°ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¯¾ã™ã‚‹æ¼”ç®—ã‚’axis=0ã§ãƒãƒƒãƒåŒ–ã—ã¾ã™ã€‚ ã“ã®ã‚ãŸã‚Šã¯in_axes=(1, 0, ...)ã¨ã‹ã‚„ã‚Œã°èª¿ç¯€ã§ãã¾ã™ãŒã€ä»Šå›ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§OKã§ã™ã€‚ . [make_qp(*pv) for pv in zip(p, v)]ã§ã€List[brax.QP]ã‚’ä½œã£ã¦ã„ã¾ã™ãŒã€ã“ã‚Œã‚’jax.tree_mapã§ã‚‚ã†ä¸€å›QPã«æˆ»ã—ã¦ã„ã¾ã™ã€‚ . List[QP(p=(0, 0), v(0, 0)), QP(..), ...] . ãŒ . QP( p=[(0, 0), (0.1, 0.2),. ...], v=[(0, 0), (1, 2), ...], ) . ã«å¤‰æ›ã•ã‚Œã‚‹æ„Ÿã˜ã§ã™ã€‚ ã“ã®ã‚¸ãƒ£ãƒ¼ã‚´ãƒ³ã¯ä¾¿åˆ©ãªã®ã§è¦šãˆã¦ã‚‚ã„ã„ã¨æ€ã„ã¾ã™ã€‚ ã¡ãªã¿ã«ã€treemapã®ãƒãƒ¼ãƒ‰ãŒè‘‰ã‹ã©ã†ã‹ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒPyTreeã‹å¦ã‹ã«ã‚ˆã‚Šã¾ã™ã€‚ ã“ã‚Œã¯ã€Œä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’Jaxã¯æš—ã«æœ¨æ§‹é€ ã ã¨ã¿ãªã—ã¾ã™ã€‚ä¸è¶³ãªã‚‰è‡ªåˆ†ã§ç™»éŒ²ã—ã¦ãã ã•ã„ã€ã¨ã„ã†è©±ãªã®ã§ã€æœ€åˆã¯é¢é£Ÿã‚‰ã†ã¨æ€ã„ã¾ã™ã€‚ ã“ã‚Œã‚’é™½ãªAPIã§ã‚„ã‚ã†ã«ã™ã‚‹ã¨Rustã‚„Scalaã«ã‚ã‚‹traitãŒå¿…è¦ãªã®ã§ã€æ‚ªã„è¨­è¨ˆã§ã¯ãªã„ã¨æ€ã„ã¾ã™ãŒã€‚ ã¨ã„ã†ã‚ã‘ã§ã€ã‚³ãƒ¼ãƒ‰ã¯ã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ã€‚ . def simulate_balls_brax(n_balls: int, n_steps: int = 100) -&gt; t.List[Array]: sys = brax.System(make_ball()) p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0) v = np.random.randn(n_balls, 2) qps = [make_qp(*pv) for pv in zip(p, v)] qps = jax.tree_map(lambda *args: jnp.stack(args), *qps) # ã“ã“ã§ step_vmap = jax.jit(jax.vmap(lambda qp: sys.step(qp, []))) results = [] for _ in range(n_steps): qps, _ = step_vmap(qps) results.append(qps.pos[:, 0, :2]) return results HTML(ball_animation(simulate_balls_brax(20, 40)).to_jshtml()) . &lt;/input&gt; Once Loop Reflect jitã‚’ä½¿ã‚ãªã„ã¨braxãŒãªãœã‹numpyã®é–¢æ•°ã‚’å‘¼ã¼ã†ã¨ã—ã¦ã‚¨ãƒ©ãƒ¼ã«ãªã£ãŸã®ã§ã€jitã‚‚ä½µç”¨ã—ã¦ã„ã¾ã™ã€‚ . 5. Haiku&#12391;&#35598;&#12398;&#12525;&#12508;&#12483;&#12488;&#12434;&#23398;&#32722;&#12373;&#12379;&#12390;&#12415;&#12427; . ã¨ã„ã†ã‚ã‘ã§ã€braxã®ä½¿ã„æ–¹ã‚’ã–ã£ã¨è¦‹ã¦ã¿ã¾ã—ãŸãŒã€æ¯å›è‡ªåˆ†ã§ãƒ­ãƒœãƒƒãƒˆã‚’è€ƒãˆã‚‹ã®ã¯å¤§å¤‰ã ã—æŸ»èª­è€…ã«ã‚‚æ–‡å¥ã‚’è¨€ã‚ã‚Œã‚‹ãªã®ã§ã€ä»Šå›ã¯è¬ãƒ­ãƒœãƒƒãƒˆã‚’å­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ open AI gymé¢¨ã®brax.envs.EnvãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚ä»Šå›ã¯Antã‚’è¨“ç·´ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ gym.makeã«ç›¸å½“ã™ã‚‹ã®ãŒbrax.envs.createã§ã™ã€‚ stepã®APIã¯gymã¨é•ã„å†…éƒ¨çŠ¶æ…‹ãƒ»å ±é…¬ãªã©ãŒå…¥ã£ãŸbrax.envs.Stateã¨ã„ã†ã‚¯ãƒ©ã‚¹ã‚’æ¸¡ã—ã¦æ¬¡ã®Stateã‚’å—ã‘å–ã‚‹ã¨ã„ã†ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã§ã™ã€‚ . import brax.envs def render_html(sys: brax.System, qps: t.List[brax.QP]) -&gt; HTML: import uuid import brax.io.html html = brax.io.html.render(sys, qps) # A weired trick to show multiple brax viewers... html = html.replace(&quot;brax-viewer&quot;, f&quot;brax-viewer-{uuid.uuid4()}&quot;) return HTML(html) def random_ant() -&gt; HTML: env = brax.envs.create(env_name=&quot;ant&quot;) prng_key = jax.random.PRNGKey(0) state = env.reset(prng_key) qps = [state.qp] step_jit = jax.jit(env.step) for i in range(10): prng_key, action_key = jax.random.split(prng_key) action = jax.random.normal(action_key, shape=(env.action_size,)) state = step_jit(state, action) qps.append(state.qp) return render_html(env.sys, qps) random_ant() . brax visualizer . . ãªã‚“ã‹ã€è·³ã­ã¦ã„ã¾ã™ã­ã€‚å¬‰ã—ãã†ã€‚ ã§ã¯ã€ã•ã£ããå­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ä»Šå›ã¯ã€æ·±å±¤å¼·åŒ–å­¦ç¿’ã®ä»£è¡¨çš„ãªæ‰‹æ³•ã§ã‚ã‚‹PPOã‚’ä½¿ã£ã¦ã¿ã¾ã™ã€‚ æœ¬å½“ã¯SACã‚‚ç”¨æ„ã—ãŸã‹ã£ãŸã®ã§ã™ãŒã€æ™‚é–“ãŒãªã‹ã£ãŸã®ã§è«¦ã‚ã¾ã—ãŸã€‚ ã¨ã‚Šã‚ãˆãšã€ä¸‰å±¤MLPã‚’ç”¨æ„ã—ã¾ã—ã‚‡ã†ã€‚ ä¾‹ãˆã°ã€ã“ã‚“ãªæ„Ÿã˜ã®ã‚‚ã®ãŒã‚ã‚Œã°ã„ã„ã§ã™ã€‚ . def mlp_v1( observation: JaxArray, w1: JaxArray, b1: JaxArray, w2: JaxArray, b2: JaxArray, w3: JaxArray, b3: JaxArray, ) -&gt; JaxArray: x = jnp.dot(observation, w1) + b1 x = jnp.tanh(x) x = jnp.dot(x, w2) + b2 x = jnp.tanh(x) return jnp.dot(x, w3) + b3 . ã“ã‚Œã‚’jitã—ã¦gradã‚’ã¨ã£ã¦Adamã‹ä½•ã‹ã§ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’æ›´æ–°ã—ã¦...ã¨ã‚„ã‚Œã°MLPãŒå‹•ãã‚ã‘ã§ã™ãŒã€ãƒ‘ãƒ©ãƒ¡ã‚¿ãŒå¤šã™ãã¦ã¡ã‚‡ã£ã¨é¢å€’ã§ã™ã­ã€‚ ãã“ã§ã“ã“ã§ã¯ã€jaxã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨“ç·´ã™ã‚‹éš›ã€ãƒ‘ãƒ©ãƒ¡ã‚¿ã®ç®¡ç†ãªã©ã‚’ã‚„ã£ã¦ãã‚Œã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚ã‚‹Haikuã‚’ä½¿ã£ã¦ã¿ã¾ã™ã€‚ ãªãŠã€braxå…¬å¼ã®examplesã§ã¯Flaxã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚ æ­£ç›´Haikuã‚‚Flaxã‚‚ãã“ã¾ã§å¤‰ã‚ã‚‰ãªã„ã®ã§ã™ãŒã€Flaxã®æ–¹ãŒã‚„ã‚„APIã®æŠ¼ã—ãŒå¼·ã„ï¼ˆPyTorchã§ã„ã†nn.Moduleç›¸å½“ã®ã‚‚ã®ãŒdataclassã§ãªã„ã¨ã„ã‘ãªã‹ã£ãŸã‚Šã¨ã‹ï¼‰å°è±¡ãŒã‚ã‚Šã¾ã™ã€‚ ã¾ãŸã€Haikuã¯DeepmindãŒã€Flaxã¯GoogleãŒé–‹ç™ºã—ã¦ã„ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãªã‚Šã¾ã™ã€‚ ã¨ã‚Šã‚ãˆãšã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . try: import haiku as hk import optax import chex import distrax except ImportError as e: ! pip install git+https://github.com/deepmind/dm-haiku git+https://github.com/deepmind/optax git+https://github.com/deepmind/chex git+https://github.com/deepmind/distrax import haiku as hk import optax import chex import distrax clear_output() . ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯theanoã€tensorflowã¨è‰²ã€…ã‚ã‚Šã¾ã—ãŸãŒã€æœ€è¿‘ã¯torch.nn.Moduleã‚„chainer.Linkã®ã‚ˆã†ã«ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ãƒ»forwardã®å‡ºåŠ›ãƒ»éš£æ¥ã—ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã‚’è¨˜éŒ²ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ã£ã¦ã€å‹•çš„ã«è¨ˆç®—ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã™ã‚‹ã‚‚ã®ãŒå¤šã„ã‹ã¨æ€ã„ã¾ã™ã€‚ ã—ã‹ã—ã€Haikuã«ã‚ˆã‚‹ãã‚Œã¯å°‘ã—ç•°ãªã‚Šã¾ã™ã€‚ãƒã‚¤ãƒ³ãƒˆã¯ã€å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹éƒ¨åˆ†ã¯JaxãŒæ‹…å½“ã™ã‚‹ã®ã§ã€Haikuã¯ãŸã ã€Œãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’ç®¡ç†ã™ã‚‹ã ã‘ã€ã§ã„ã„ã¨ã„ã†ã“ã¨ã§ã™ã€‚ ãã®ãŸã‚ã«ã€Haikuã¯transformã¨ã„ã†APIã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚ ã“ã‚Œã¯è¦‹ãŸã»ã†ãŒæ—©ã„ã§ã—ã‚‡ã†ã€‚ . def mlp_v2(observation: JaxArray) -&gt; JaxArray: w1 = hk.get_parameter(&quot;w1&quot;, shape=[observation.shape[1], 3], init=jnp.ones) b1 = hk.get_parameter(&quot;b1&quot;, shape=[3], init=jnp.zeros) w2 = hk.get_parameter(&quot;w2&quot;, shape=[3, 3], init=jnp.ones) b2 = hk.get_parameter(&quot;b2&quot;, shape=[3], init=jnp.zeros) w3 = hk.get_parameter(&quot;w3&quot;, shape=[3, 2], init=jnp.ones) b3 = hk.get_parameter(&quot;b3&quot;, shape=[2], init=jnp.zeros) x = jnp.dot(observation, w1) + b1 x = jnp.tanh(x) x = jnp.dot(x, w2) + b2 x = jnp.tanh(x) return jnp.dot(x, w3) + b3 prng_seq = hk.PRNGSequence(0) # ã“ã‚Œã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ã™ã‚‹ã®ã¯è‰¯ããªã„ã§ã™ã€‚çœŸä¼¼ã—ãªã„ã§ init, apply = hk.transform(mlp_v2) # transformã™ã‚‹ # initã¯ä¹±æ•°ã‚·ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚’å—ã‘å–ã£ã¦ã€åˆæœŸåŒ–ã—ãŸãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’è¿”ã™é–¢æ•° params = init(next(prng_seq), jnp.zeros((10, 2))) print(params) # applyã¯ãƒ‘ãƒ©ãƒ¡ã‚¿ãƒ»ä¹±æ•°ã‚·ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚’å—ã‘å–ã£ã¦ã€å‡ºåŠ›ã‚’è¿”ã™é–¢æ•° output = apply(params, next(prng_seq), jnp.zeros((10, 2))) . FlatMap({ &#39;~&#39;: FlatMap({ &#39;w1&#39;: DeviceArray([[1., 1., 1.], [1., 1., 1.]], dtype=float32), &#39;b1&#39;: DeviceArray([0., 0., 0.], dtype=float32), &#39;w2&#39;: DeviceArray([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=float32), &#39;b2&#39;: DeviceArray([0., 0., 0.], dtype=float32), &#39;w3&#39;: DeviceArray([[1., 1.], [1., 1.], [1., 1.]], dtype=float32), &#39;b3&#39;: DeviceArray([0., 0.], dtype=float32), }), }) . ã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ã€‚ ã¾ã¨ã‚ã‚‹ã¨ã€ . transform(f)ã¯äºŒã¤ã®é–¢æ•°initã€applyã‚’ã‹ãˆã™ | transformã¯fã‚’ã€fã®ä¸­ã§haiku.get_parameterã‚’ä½¿ã£ã¦å‘¼ã³å‡ºã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’å…¥åŠ›ã¨ã™ã‚‹é–¢æ•°ã«å¤‰æ›ã™ã‚‹ | initã¯ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’åˆæœŸåŒ–ã—ã¦è¿”ã™ã€‚ãƒ‘ãƒ©ãƒ¡ã‚¿ã¯FlatMapã¨ã„ã†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã ãŒã“ã‚Œã¯ã»ã¨ã‚“ã©dictã¨åŒã˜ | applyã¯ä¸ãˆã‚‰ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’ä½¿ã£ã¦æ‰€æœ›ã®è¨ˆç®—ã‚’è¡Œã† ã¨ã„ã†æ„Ÿã˜ã§ã™ã­ã€‚ | . ã¤ã„ã§ã«ã€ä¸Šã®ä¾‹ã§ã¯hk.PRNGSequenceã¨ã„ã†PRNGKeyã®æ›´æ–°ã‚’å‹æ‰‹ã«ã‚„ã£ã¦ãã‚Œã‚‹ã‚‚ã®ã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚ . ã—ã‹ã—ã€ã“ã‚Œã§ã‚‚ã¾ã é¢å€’ã§ã™ã­ã€‚ å®Ÿéš›ã®ã¨ã“ã‚ã€ã‚ˆãä½¿ã†ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ã¾ã¨ã¾ã£ã¦ã„ã‚‹ã®ã§ã€ã“ã‚Œã‚’ä½¿ãˆã°ã„ã„ã§ã™ã€‚ . def mlp_v3(output_size: int, observation: JaxArray) -&gt; JaxArray: x = hk.Linear(64)(observation) x = jnp.tanh(x) x = hk.Linear(64)(observation) x = jnp.tanh(x) return hk.Linear(output_size)(observation) . ã“ã‚Œã‚’ä½¿ã£ã¦ã€PPOã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ›¸ã„ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ æ–¹ç­–ã¯æ¨™æº–åå·®ãŒçŠ¶æ…‹ã«ä¾å­˜ã—ãªã„æ­£è¦åˆ†å¸ƒã«ã—ã¾ã™ã€‚ . class NetworkOutput(t.NamedTuple): mean: JaxArray stddev: JaxArray value: JaxArray def policy_and_value(action_size: int, observation: JaxArray) -&gt; NetworkOutput: mean = mlp_v3(output_size=action_size, observation=observation) value = mlp_v3(output_size=1, observation=observation) logstd = hk.get_parameter(&quot;logstd&quot;, (1, action_size), init=jnp.zeros) stddev = jnp.ones_like(mean) * jnp.exp(logstd + 1e-8) return NetworkOutput(mean, stddev, value) . ã“ã‚Œã ã‘ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ã¯TruncatedNormalã§åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚ä»Šå›ã¯å…¨éƒ¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã¾ã¾ã«ã—ã¾ã—ãŸã€‚ . æ¬¡ã«ã€ã“ã‚Œã‚’ä½¿ã£ã¦ã€ç’°å¢ƒã¨ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ã¾ã™ã€‚ ã„ã¾ã€braxã®åˆ©ç‚¹ã‚’æ´»ã‹ã™ãŸã‚ã«ã€ . ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã¦ | ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã§æ¬¡ã®çŠ¶æ…‹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ ã¨ã„ã†éç¨‹ã‚’ã™ã¹ã¦jax.jitã®ä¸­ã§ã‚„ã‚‹ã®ãŒç†æƒ³ã§ã™ã‚ˆã­ã€‚ | ã§ã™ã‹ã‚‰ã€ãŸã¨ãˆã°ã“ã‚“ãªæ„Ÿã˜ã«ã‚„ã‚Œã°ã„ã„ã§ã™ã€‚ . Action = JaxArray def make_step_function( env: brax.envs.Env, ) -&gt; t.Tuple[t.Callable[..., t.Any], t.Callable[..., t.Any]]: def step(state: brax.envs.State) -&gt; t.Tuple[brax.envs.State, NetworkOutput, Action]: out = policy_and_value(env.action_size, state.obs) policy = distrax.MultivariateNormalDiag(out.mean, out.stddev) action = policy.sample(seed=hk.next_rng_key()) # transformã™ã‚‹ã¨ã“ã‚ŒãŒä½¿ãˆã¾ã™ state = env.step(state, jnp.tanh(action)) return state, out, action init, apply = hk.transform(step) return jax.jit(init), jax.jit(apply) . ã“ã“ã§ã€è¡Œå‹•ã®ã‚µãƒ³ãƒ—ãƒ«ã«ã¯distraxã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã„ã¾ã—ãŸã€‚ å¹³å‡å€¤ã«ãƒã‚¤ã‚ºã‚’ã„ã‚Œã‚‹ã ã‘ãªã®ã§ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦ã‚‚ã‚ã¾ã‚Šå¤‰ã‚ã‚‰ãªã„ã®ã§ã™ãŒ...ã€‚ ã„ã¾ã€å„ã‚¸ãƒ§ã‚¤ãƒ³ãƒˆã«å¯¾ã—ã¦åŠ ãˆã‚‹åŠ›ãŒã€ãã‚Œãã‚Œç‹¬ç«‹ãªæ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã¨ä»®å®šã—ã¦ã„ã‚‹ã®ã§ã€MutliVariateNormDiag(å…±åˆ†æ•£è¡Œåˆ—ãŒå¯¾è§’è¡Œåˆ—ã«ãªã‚‹å¤šå¤‰é‡æ­£è¦åˆ†å¸ƒï¼‰ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚ distrax.Independentã¨distrax.Normalã‚’ä½¿ã£ã¦ã‚‚åŒã˜ã“ã¨ãŒã§ãã¾ã™ã€‚ è¡Œå‹•ã¯ä¸€å¿œtanhã§$[-1, 1]$ã®ç¯„å›²ã«ãªã‚‰ã—ã¦ã„ã¾ã™ã€‚ . ã¡ã‚‡ã£ã¨è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . ant = brax.envs.create(env_name=&quot;ant&quot;, batch_size=1) init, step = make_step_function(ant) initial_state = jax.jit(ant.reset)(next(prng_seq)) params = init(next(prng_seq), initial_state) _next_state, out, action = step(params, next(prng_seq), initial_state) # chexã¯ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ chex.assert_shape((out.mean, out.stddev, action), (1, ant.action_size)) . ã¨ã„ã†ã‚ã‘ã§ç„¡äº‹ã«stepã‚’JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦é«˜é€ŸåŒ–ã§ãã¾ã—ãŸã€‚ resetã¯ã»ã¨ã‚“ã©å‘¼ã°ãªã„ã®ã§åˆ¥ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ãªãã¦ã‚‚ã„ã„ã®ã§ã™ãŒã€jitã—ãªã„ã¨braxãŒjnp.DeviceArrayã®ã‹ã‚ã‚Šã«numpyã‚’ä½¿ã„ãŸãŒã£ã¦å°‘ã—é¢å€’ãªã®ã§jitã—ã¦ã„ã¾ã™ã€‚ . ã‚ã¨ã¯PPOã‚’å®Ÿè£…ã—ã¦ã„ãã¾ã™ãŒã€æ™‚é–“ã®éƒ½åˆã§æ‰‹çŸ­ã‹ã«ã„ãã¾ã™ã€‚ ã¾ãšã¯GAEã§ã™ã­ã€‚ æ™®é€šã«æ›¸ãã¨jax.jitãŒãƒ«ãƒ¼ãƒ—ã‚¢ãƒ³ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã£ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ãŒæ¿€é…ã«ãªã‚‹ã®ã§ã€jax.lax.fori_loopã¨ã„ã†é»’é­”è¡“ã‚’ä½¿ã„ã¾ã™ã€‚ ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚å®šæ•°ã¯static_argnumsã§æŒ‡å®šã—ã¾ã™ã€‚ vmapã§å„ãƒ¯ãƒ¼ã‚«ãƒ¼ç”¨ã«ä¸¦åˆ—åŒ–ã—ã¾ã™ã€‚ . import functools @functools.partial(jax.jit, static_argnums=2) def gae( r_t: JaxArray, discount_t: JaxArray, lambda_: float, values: JaxArray, ) -&gt; chex.Array: chex.assert_rank([r_t, values, discount_t], 1) chex.assert_type([r_t, values, discount_t], float) lambda_ = jnp.ones_like(discount_t) * lambda_ delta_t = r_t + discount_t * values[1:] - values[:-1] n = delta_t.shape[0] def update(i: int, advantage_t: JaxArray) -&gt; JaxArray: t_ = n - i - 1 adv_t = delta_t[t_] + lambda_[t_] * discount_t[t_] * advantage_t[t_ + 1] return jax.ops.index_update(advantage_t, t_, adv_t) advantage_t = jax.lax.fori_loop(0, n, update, jnp.zeros(n + 1)) return advantage_t[:-1] batched_gae = jax.vmap(gae, in_axes=(1, 1, None, 1), out_axes=1) . ãªã‚“ã‹ãƒ–ãƒ­ã‚°ã§æ›¸ãã«ã¯é»’é­”è¡“ã™ãã‚‹æ°—ã‚‚ã—ã¾ã™ãŒ...ã€‚ . æ¬¡ã«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒãƒã‚’æ§‹æˆã™ã‚‹éƒ¨åˆ†ã§ã™ã­ã€‚ ã“ã‚Œã¯ã€æ™®é€šã«PyTorchã¨ã‹ã¨å¤‰ã‚ã‚‰ãªã„ã§ã™ã€‚ . import dataclasses @chex.dataclass class RolloutResult: &quot;&quot;&quot; Required experiences for PPO. &quot;&quot;&quot; observations: t.List[JaxArray] actions: t.List[JaxArray] = dataclasses.field(default_factory=list) rewards: t.List[JaxArray] = dataclasses.field(default_factory=list) terminals: t.List[JaxArray] = dataclasses.field(default_factory=list) outputs: t.List[NetworkOutput] = dataclasses.field(default_factory=list) def append( self, *, observation: JaxArray, action: JaxArray, reward: JaxArray, output: NetworkOutput, terminal: JaxArray, ) -&gt; None: self.observations.append(observation) self.actions.append(action) self.rewards.append(reward) self.outputs.append(output) self.terminals.append(terminal) def clear(self) -&gt; None: self.observations = [self.observations[-1]] self.actions.clear() self.rewards.clear() self.outputs.clear() self.terminals.clear() class Batch(t.NamedTuple): &quot;&quot;&quot;Batch for PPO, also used as minibatch by indexing.&quot;&quot;&quot; observation: JaxArray action: JaxArray reward: JaxArray advantage: JaxArray value_target: JaxArray log_prob: JaxArray def __getitem__(self, idx: Array) -&gt; &quot;Batch&quot;: return self.__class__( observation=self.observation[idx], action=self.action[idx], reward=self.reward[idx], advantage=self.advantage[idx], value_target=self.value_target[idx], log_prob=self.log_prob[idx], ) @jax.jit def make_batch(rollout: RolloutResult, next_value: JaxArray) -&gt; Batch: action = jnp.concatenate(rollout.actions) mean, stddev, value = jax.tree_map(lambda *x: jnp.concatenate(x), *rollout.outputs) log_prob = distrax.MultivariateNormalDiag(mean, stddev).log_prob(action) reward = jnp.stack(rollout.rewards) mask = 1.0 - jnp.stack(rollout.terminals) value = jnp.concatenate( (value.reshape(reward.shape), next_value.reshape(1, -1)), axis=0, ) advantage = batched_gae(reward, mask * 0.99, 0.95, value) value_target = advantage + value[:-1] return Batch( observation=jnp.concatenate(rollout.observations[:-1]), action=action, reward=jnp.ravel(reward), advantage=jnp.ravel(advantage), value_target=jnp.ravel(value_target), log_prob=log_prob, ) . æ™®é€šã®dataclassesã¯jitã§ããªã„ã®ã§ã€chex.dataclassã‚’ä½¿ã„ã¾ã™ã€‚ ã•ã£ãå°‘ã—ã ã‘è§¦ã‚Œã¾ã—ãŸãŒã€chex.dataclassã¯ä½œæˆã—ãŸdataclassã‚’PyTreeã¨ã—ã¦jaxã«ç™»éŒ²ã—ã¦ãã‚Œã¾ã™ã€‚ å®Ÿã¯flax.struct.dataclassã¨ã„ã†ã ã„ãŸã„åŒã˜ã‚‚ã®ã‚‚ã‚ã£ã¦ã€braxã®å†…éƒ¨ã§ã¯ã“ã‚Œã‚’ä½¿ã£ã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚ ã¾ãŸ$ gamma = 0.99, lambda = 0.95$ã¨ã—ã¾ã—ãŸã€‚ . ã„ã‚ˆã„ã‚ˆå­¦ç¿’ã®éƒ¨åˆ†ã§ã™ã­ã€‚ ã¾ãšã€æå¤±é–¢æ•°ã‚’jax.gradã§ãã‚‹ã‚ˆã†ã«æ›¸ãã¾ã™ã€‚ . def ppo_loss(action_size: int, batch: Batch) -&gt; JaxArray: mean, stddev, value = policy_and_value(action_size, batch.observation) # Policy loss policy = distrax.MultivariateNormalDiag(mean, stddev) log_prob = policy.log_prob(batch.action) prob_ratio = jnp.exp(log_prob - batch.log_prob) clipped_ratio = jnp.clip(prob_ratio, 0.8, 1.2) clipped_obj = jnp.fmin(prob_ratio * batch.advantage, clipped_ratio * batch.advantage) policy_loss = -jnp.mean(clipped_obj) # Value loss value_loss = jnp.mean(0.5 * (value - batch.value_target) ** 2) # Entropy regularization entropy_mean = jnp.mean(policy.entropy(), axis=-1) return policy_loss + value_loss - 0.001 * entropy_mean . $ epsilon = 0.2$ã§å›ºå®šã—ã¦ã„ã‚‹ã®ã§ã€$[1 - 0.2, 1 + 0.2]$ã®ç¯„å›²ã§ã‚¯ãƒªãƒƒãƒ—ã—ã¾ã™ã€‚ . ã§ã¯ã“ã‚Œã‚’ä½¿ã£ã¦ã€ä»Šåº¦ã¯ãƒ‘ãƒ©ãƒ¡ã‚¿ã®æ›´æ–°ã‚’å…¨éƒ¨jitã«ã¤ã£ã“ã‚“ã§ã¿ã¾ã—ã‚‡ã†ã€‚ ãƒ‘ãƒ©ãƒ¡ã‚¿ã®æ›´æ–°ã«ã¯optaxã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã„ã¾ã™ã€‚è‰²ã€…ãªSGDã®ãƒãƒªã‚¢ãƒ³ãƒˆã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ãŒã€åƒ•ã¯ã»ã¨ã‚“ã©Adamã—ã‹ä½¿ã„ã¾ã›ã‚“...ã€‚ . import optax def make_update_function( action_size: int, opt_update: optax.TransformUpdateFn, ) -&gt; t.Callable[..., t.Any]: # hk.Paramsã‚’ä½¿ã„å›ã™ã®ã§initã¯æ¨ã¦ã¦ã„ã„ # è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ãªã„ã®ã§ã€without_apply_rngãŒä½¿ãˆã‚‹ _, loss_fn = hk.without_apply_rng(hk.transform(lambda batch: ppo_loss(action_size, batch))) grad_fn = jax.grad(loss_fn) # ã“ã“ã§jitã—ã¦ã„ã„ @jax.jit def update( params: hk.Params, opt_state: optax.OptState, batch: Batch, ) -&gt; t.Tuple[hk.Params, Batch]: grad = grad_fn(params, batch) updates, new_opt_state = opt_update(grad, opt_state) return optax.apply_updates(params, updates), new_opt_state return update . ã•ã¦ã€ã“ã“ã¾ã§æ¥ãŸã‚‰ã‚ã¨ä¸€æ­©ã§ã™ã­ã€‚ æ¬¡ã«é¢å€’ã§ã™ãŒæ¬¡ã®çŠ¶æ…‹ã®valueã‚’ã¨ã£ã¦ãã¦ãƒãƒƒãƒã‚’ä½œã‚‹éƒ¨åˆ†ã‚’æ›¸ãã¾ã™ã€‚ . def make_next_value_function(action_size: int) -&gt; Batch: def next_value_fn(obs: JaxArray) -&gt; JaxArray: output = policy_and_value(action_size, obs) return output.value _, next_value_fn = hk.without_apply_rng(hk.transform(next_value_fn)) return jax.jit(next_value_fn) . é€Ÿåº¦ã‚’æ±‚ã‚ã‚‹ãªã‚‰ã€ã“ã‚Œã¯make_batchã¨ä¸€ç·’ã«jitã—ã¦ã—ã¾ã£ã¦ã‚‚ã„ã„ã§ã™ãŒã€ã¾ã‚é¢å€’ãªã®ã§ã“ã‚Œã§ã‚‚ã„ã„ã§ã—ã‚‡ã†ã€‚ . ã§ã¯ææ–™ãŒãã‚ã£ãŸã®ã§ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã‚’æ›¸ã„ã¦ã„ãã¾ã—ã‚‡ã†ã€‚ é¢å€’ã§ã™ãŒã€è©•ä¾¡ç”¨ã®environmentã‚‚åˆ¥ã«ä½œã‚Šã¾ã™ã€‚ . try: import tqdm except ImportError as _e: ! pip install tqdm import tqdm clear_output() . import datetime from tqdm.notebook import trange def sample_minibatch_indices( n_instances: int, n_minibatches: int, prng_key: chex.PRNGKey, ) -&gt; t.Iterable[JaxArray]: indices = jax.random.permutation(prng_key, n_instances) minibatch_size = n_instances // n_minibatches for start in range(0, n_instances, minibatch_size): yield indices[start : start + minibatch_size] def train_ppo( env_name: str = &quot;ant&quot;, n_workers: int = 32, n_steps: int = 2048, n_training_steps: int = 10000000, n_optim_epochs: int = 10, n_minibatches: int = 64, eval_freq: int = 20, eval_workers: int = 16, seed: int = 0, ) -&gt; HTML: # ç’°å¢ƒã¨ã€ç’°å¢ƒã‚’å«ã‚“ã stepé–¢æ•°ã‚’ä½œã‚‹ env = brax.envs.create(env_name=env_name, episode_length=1000, batch_size=n_workers) eval_env = brax.envs.create( env_name=env_name, episode_length=1000, batch_size=eval_workers, ) network_init, step = make_step_function(env) _, eval_step = make_step_function(eval_env) eval_reset = jax.jit(eval_env.reset) # ä¹±æ•° prng_seq = hk.PRNGSequence(seed) # åˆæœŸçŠ¶æ…‹ state = jax.jit(env.reset)(rng=next(prng_seq)) rollout = RolloutResult(observations=[state.obs]) # Optimizerã¨ãƒ‘ãƒ©ãƒ¡ã‚¿ã‚’åˆæœŸåŒ–ã™ã‚‹ optim = optax.chain(optax.clip_by_global_norm(1.0), optax.adam(3e-4, eps=1e-4)) update = make_update_function(env.action_size, optim.update) params = network_init(next(prng_seq), state) opt_state = optim.init(params) # next_value next_value_fn = make_next_value_function(env.action_size) n_instances = n_workers * n_steps def evaluate(step: int) -&gt; None: eval_state = eval_reset(rng=next(prng_seq)) return_ = jnp.zeros(eval_workers) done = jnp.zeros(eval_workers, dtype=bool) for _ in range(1000): eval_state, _, _ = eval_step(params, next(prng_seq), eval_state) return_ = return_ + eval_state.reward * (1.0 - done) done = jnp.bitwise_or(done, eval_state.done.astype(bool)) print(f&quot;Step: {step} Avg. ret: {jnp.mean(return_).item()}&quot;) for i in trange(n_training_steps // n_instances): for _ in range(n_steps): state, output, action = step(params, next(prng_seq), state) rollout.append( observation=state.obs, action=action, reward=state.reward, output=output, terminal=state.done, ) next_value = next_value_fn(params, state.obs) batch = make_batch(rollout, next_value) rollout.clear() # Batchã‚’ä½œã£ãŸã®ã§ã€ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å­¦ç¿’ for _ in range(n_optim_epochs): for idx in sample_minibatch_indices( n_instances, n_minibatches, next(prng_seq), ): minibatch = batch[idx] params, opt_state = update(params, opt_state, minibatch) # æ™‚ã€…è©•ä¾¡ã™ã‚‹ if (i + 1) % eval_freq == 0: evaluate(i + 1) evaluate(i + 1) # Visualize eval_state = eval_reset(rng=next(prng_seq)) qps = [] while eval_state.done[0] == 0.0: eval_state, _, _ = eval_step(params, next(prng_seq), eval_state) qps.append(jax.tree_map(lambda qp: qp[0], eval_state.qp)) return render_html(eval_env.sys, qps) start_time = datetime.datetime.now() html = train_ppo() elapsed = datetime.datetime.now() - start_time print(f&quot;Train completed after {elapsed.total_seconds() / 60:.2f} min.&quot;) html . Step: 20 Avg. ret: -286.34039306640625 Step: 40 Avg. ret: -273.5491943359375 Step: 60 Avg. ret: -193.0821990966797 Step: 80 Avg. ret: -84.20954132080078 Step: 100 Avg. ret: -45.654090881347656 Step: 120 Avg. ret: -20.323640823364258 Step: 140 Avg. ret: -42.74524688720703 Step: 152 Avg. ret: -5.514527320861816 Train completed after 41.34 min. . brax visualizer . . 100ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã®è¨“ç·´ãŒ41åˆ†ã§çµ‚ã‚ã‚Šã¾ã—ãŸã€‚é€Ÿã„ã§ã™ã­ã‚„ã£ã±ã‚Šã€‚ ãªã‚“ã‹å‰ã®è·³ã­ã™ãã¦ã„ã‚‹å¾®å¦™ãªã®ãŒãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚ºã•ã‚Œã¦ã„ã¾ã™ãŒ...ã€‚ . 6. &#12414;&#12392;&#12417; . ã¨ã„ã†ã‚ã‘ã§ã€ã“ã®ãƒ–ãƒ­ã‚°ã§ã¯Jaxã€Braxã€Haikuã‚’ä½¿ã£ã¦ã€GPUã ã‘ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ä¸Šã®ãƒ­ãƒœãƒƒãƒˆã‚’è¨“ç·´ã™ã‚‹ä¾‹ã‚’ç¤ºã—ã¾ã—ãŸã€‚ ã‹ãªã‚Šé§†ã‘è¶³ã®è§£èª¬ã«ãªã‚Šã¾ã—ãŸãŒã€ãªã‚“ã¨ãªããƒ—ãƒ­ã‚°ãƒ©ãƒ ã®çµ„ã¿æ–¹ã‚’ç†è§£ã—ã¦ã„ãŸã ã‘ãŸã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™ã€‚ . ç·æ‹¬ã™ã‚‹ã¨ã€Jaxã¯ã‹ãªã‚Šåºƒã„ç¯„å›²ã®NumPyæ¼”ç®—ã‚’GPU/TPUä¸Šã§é«˜é€Ÿã«å‹•ä½œã™ã‚‹ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›ã§ãã‚‹ã€éå¸¸ã«å¼·åŠ›ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ ä»Šå›ç´¹ä»‹ã—ãŸvmapã¯ã€ä¾‹ãˆã°ä¸€ã¤ã®GPUä¸Šã§æ¼”ç®—ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹æ©Ÿèƒ½ã§ã™ãŒã€ä»–ã«ã‚‚pmapã«ã‚ˆã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’ã¾ãŸã„ã ä¸¦åˆ—åŒ–ã‚‚ ç‰¹ã«ã€ . âœ” CPUã¨GPUã®é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒæ°—ã«ãªã‚‹ã¨ã | âœ” å¤§è¦æ¨¡ã«ä¸¦åˆ—ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ãŸã„ã¨ã | . ãªã©ã«åŠ¹æœã‚’ç™ºæ®ã™ã‚‹ã¨æ€ã„ã¾ã™ã€‚ã¾ãŸã€jax.lax.fori_loopã‚’ä½¿ã£ã¦ . âœ” Cythonã‚„C++/Rustãªã©ä»–ã®è¨€èªã‚’ä½¿ã‚ãšã«Pythonã®ãƒ«ãƒ¼ãƒ—ã‚’é«˜é€ŸåŒ–ã—ãŸã„ã¨ã | . ã«ã‚‚ä½¿ãˆã¾ã™ã€‚ ä¸€æ–¹ã§ã€å˜ã«æ·±å±¤å­¦ç¿’ã‚’é«˜é€ŸåŒ–ã—ãŸã„å ´åˆã€ä¾‹ãˆã° . ğŸ”º PyTorchãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã£ã¦ã„ã‚‹å ´åˆ | . ãªã©ã¯ã€Jaxã‚„Haiku/Flaxã‚’ä½¿ã†ã“ã¨ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã®æ©æµã¯ã‚ã¾ã‚Šãªã„ã¨æ€ã„ã¾ã™ã€‚ ã†ã¾ãjitã‚’ä½¿ãˆã°Jaxã®æ–¹ãŒé€Ÿã„ã¨æ€ã„ã¾ã™ãŒã€PyTorchã®CUDAã‚³ãƒ¼ãƒ‰ã¯ã‹ãªã‚Šé€Ÿã„ã§ã™ã‹ã‚‰ã­ã€‚ã¾ãŸã€PyTorchã¨æ¯”è¼ƒã—ãŸéš›ã€ . ğŸ”º å­¦ç¿’ã‚³ã‚¹ãƒˆã«ã¤ã„ã¦ã‚‚Jaxã®æ–¹ãŒå¤§ãã„ | . ã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã™ã€‚ ãªã®ã§ã€å€‹äººçš„ã«ã¯å­¦ç¿’ä»¥å¤–ã®éƒ¨åˆ†ã§ãƒ™ã‚¯ãƒˆãƒ«ä¸¦åˆ—åŒ–ãƒ»Jitã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–ã®ä½™åœ°ãŒã‚ã‚‹å ´åˆã«ã€Jaxã¯ä¾¿åˆ©ã«ä½¿ãˆã‚‹ã®ã‹ãªã‚ã¨æ€ã„ã¾ã™ã€‚ ãŸã Deepmindã¯AlphaFold2ã‚’å§‹ã‚ã€å¤šãã®Jax+Haikuè£½æ·±å±¤å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¦ã„ã¾ã™ã—ã€ä¸€å¿œèª­ã‚ã‚‹ç¨‹åº¦ã«è¦ªã—ã‚“ã§ãŠãã ã‘ã§ã‚‚ã‚ã‚‹ç¨‹åº¦ã®ãƒ¡ãƒªãƒƒãƒˆã¯ã‚ã‚‹ã¨æ€ã„ã¾ã™ã€‚ . ã•ã¦ã€å†’é ­ã®å¤§è¦æ¨¡ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦Sim2Realã‚’é ‘å¼µã‚‹ã®ã‹ã€å®Ÿæ©Ÿã®ãƒ‡ãƒ¼ã‚¿ã§é ‘å¼µã‚‹ã®ã‹ã¨ã„ã†è©±ã«æˆ»ã‚Šã¾ã™ãŒã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã•ã›ãŸã„ã®ã§ã‚ã‚Œã°å¤§è¦æ¨¡ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ãŸã„ãªã‚‰Braxã®ã‚ˆã†ã«ã€Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’Jaxã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§ãã‚‹ã‚ˆã†ã«ä½œã‚‹ã€ã¨ã„ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯é¢ç™½ã„ã¨æ€ã„ã¾ã™ã€‚åˆ†å­å‹•åŠ›å­¦è¨ˆç®—ãªã©ã€ç‰©ç†æ¼”ç®—ä»¥å¤–ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¸ã®æ´»ç”¨ã‚‚æœŸå¾…ã•ã‚Œã¾ã™ï¼ˆã¨èã„ãŸã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚åƒ•ã¯åˆ†å­å‹•åŠ›å­¦è¨ˆç®—ãŒä½•ãªã®ã‹ã‚ˆãã‚ã‹ã‚Šã¾ã›ã‚“...ï¼‰ã€‚ ä¸€æ–¹ã§ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãŒå¾®åˆ†å¯èƒ½ã§ã‚ã‚‹ã¨ã„ã†åˆ©ç‚¹ã‚’ã©ã†æ´»ã‹ã™ã®ã‹ã‚‚èˆˆå‘³æ·±ã„ãƒ†ãƒ¼ãƒã§ã™ã€‚åƒ•ã‚‚ä»¥å‰PFNã•ã‚“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ³ã§ã€å ±é…¬ãŒå¾®åˆ†å¯èƒ½ãªã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã€$ sum_{t = 0}^T frac{ partial r_t}{ partial theta}$ã«ã¤ã„ã¦ã®å±±ç™»ã‚Šæ³•ã§æ–¹ç­–ã‚’æ›´æ–°ã™ã‚‹ã®ã‚’è©¦ã—ãŸã“ã¨ãŒã‚ã‚‹ã®ã§ã™ãŒã€å ±é…¬ãŒé ã„ã¨ãªã‹ãªã‹é›£ã—ã„ãªã‚ã¨ã„ã†å°è±¡ã§ã—ãŸã€‚ã†ã¾ã„æ–¹æ³•ãŒã‚ã‚Œã°ã„ã„ã®ã§ã™ãŒ...ã€‚æ„å¤–ã¨å‹¾é…é™ä¸‹ã ã‘ã§ãªãé€²åŒ–è¨ˆç®—ãªã©ã®ãƒ¡ã‚¿ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨é¢ç™½ã„ã‹ã‚‚ã—ã‚Œãªã„ã§ã™ã€‚ . ã•ã¦ã€åƒ•ã¯ã‚‚ã†ä¸€ã¤ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã®è¨˜äº‹ã‚’æ›¸ãäºˆå®šãŒã‚ã£ãŸã®ã§ã™ãŒã€æ™‚é–“ãŒãªã„ã®ã§ä»–ã®äººã«ä»£ã‚ã£ã¦ã‚‚ã‚‰ã†ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“...ã€‚å‡ºãŸã‚‰ãã¡ã‚‰ã‚‚ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚ .",
            "url": "https://kngwyu.github.io/rlog/ja/2021/12/18/jax-brax-haiku.html",
            "relUrl": "/ja/2021/12/18/jax-brax-haiku.html",
            "date": " â€¢ Dec 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "ã‚ˆã‚Šè‰¯ã„å•é¡Œè¨­è¨ˆã¸å‘ã‘ã¦ï¼š ä½•ãŒå¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã™ã‚‹ã®ã‹ã‚’ç†è§£ã—ã‚ˆã†",
            "content": "1. &#12399;&#12376;&#12417;&#12395; . . Note: ã“ã®ãƒ–ãƒ­ã‚°ã¯å¼·åŒ–å­¦ç¿’è‹¦æ‰‹ã®ä¼š Advent Calendar 202020æ—¥ç›®ã®è¨˜äº‹ã¨ã—ã¦æ›¸ã‹ã‚Œã¾ã—ãŸ . å¼·åŒ–å­¦ç¿’ã¯ã€é€æ¬¡çš„ã«æ„æ€æ±ºå®šã™ã‚‹å•é¡Œã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ å¾“æ¥ã¯å¤§å¤‰ã ã£ãŸ1 ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨“ç·´ãŒç°¡å˜ã«ãªã£ãŸã“ã¨ã‚„ã€ Alpha GOãªã©æ·±å±¤å¼·åŒ–å­¦ç¿’(Deep RL)ã®æˆåŠŸã‚’èƒŒæ™¯ã«ã€å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ãƒ»çµŒæ¸ˆ ãªã©ã€æ§˜ã€…ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§å¼·åŒ–å­¦ç¿’ã®åˆ©ç”¨ãŒè©¦ã¿ã‚‰ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«æ€ã„ã¾ã™ã€‚ åƒ•å€‹äººã¨ã—ã¦ã‚‚ã€å¼·åŒ–å­¦ç¿’ã¯æ±ç”¨çš„ã§é¢ç™½ã„ãƒ„ãƒ¼ãƒ«ã ã¨æ€ã†ã®ã§ã€å°†æ¥çš„ã«ã¯è‰²ã€…ãªå¿œç”¨åˆ†é‡ã§åºƒãæ´»ç”¨ã•ã‚Œã‚‹ã¨ã„ã„ãªã€ã¨æ€ã„ã¾ã™ã€‚ . ä¸€æ–¹ã§ã€å¼·åŒ–å­¦ç¿’ã‚’ä½•ã‹ç‰¹å®šã®å•é¡Œã«å¿œç”¨ã—ã¦ã¿ã‚ˆã†ã€ã¨ã„ã†å ´é¢ã§ã¯ã€ ãã®æ±ç”¨æ€§ã‚†ãˆã‹ãˆã£ã¦ã¨ã£ã¤ãã«ãã„ãƒ»æ‰±ã„ã¥ã‚‰ã„é¢ãŒã‚ã‚‹ã‚ˆã†ã«æ€ã„ã¾ã™ã€‚ å®Ÿéš›ã«ã€è‹¦æ‰‹ã®ä¼šãªã©ã§å¿œç”¨ç ”ç©¶ã‚’ã•ã‚Œã¦ã„ã‚‹æ–¹ã‹ã‚‰ã€ . å•é¡Œã‚’å®šç¾©ã™ã‚‹ã®ãŒãã‚‚ãã‚‚å¤§å¤‰ | è‰²ã€…ãªæ‰‹æ³•ãŒã‚ã£ã¦ã€ä½•ãŒãªã‚“ã ã‹ã‚ˆãã‚ã‹ã‚‰ãªã„ | . ãªã©ã®æ„è¦‹ã‚’è¦³æ¸¬ã§ãã¾ã—ãŸã€‚ . ã§ã¯å¿œç”¨ç ”ç©¶ã«å¯¾ã™ã‚‹ã€Œãƒ„ãƒ¼ãƒ«ã€ã¨ã—ã¦å¼·åŒ–å­¦ç¿’ã‚’æ‰±ã†ä¸Šã§ä½•ãŒå¤§äº‹ãªã®ã ã‚ã†ã€ã¨è€ƒãˆãŸã¨ãã€ åƒ•ã¯ç°¡å˜ãªå•é¡Œã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã“ããŒå¤§äº‹ã ã¨ã„ã†ä»®èª¬ã«æ€ã„ã„ãŸã‚Šã¾ã—ãŸã€‚ ç°¡å˜ãªå•é¡Œã‚’è¨­è¨ˆã™ã‚‹ãŸã‚ã«ã¯ã€å¼·åŒ–å­¦ç¿’ã®ä¸­ã§ã‚‚ã©ã†ã„ã†å•é¡ŒãŒé›£ã—ã„ã®ã‹ã€ ã¨ã„ã†ã“ã¨ã‚’ãã¡ã‚“ã¨ç†è§£ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚‹ã‚ˆã†ã«æ€ã„ã¾ã™ã€‚ . ãã“ã§ã“ã®ãƒ–ãƒ­ã‚°è¨˜äº‹ã§ã¯ã€å¼·åŒ–å­¦ç¿’ã®ä¸­ã§ã‚‚ã€Œé›£ã—ã„å•é¡Œã€ãŒã©ã†ã„ã†ã‚‚ã®ãªã®ã‹ã€ ãã†ã„ã†å•é¡Œã¯ãªãœé›£ã—ã„ã®ã‹ã«ã¤ã„ã¦ã€ä¾‹ã‚’é€šã—ã¦ãªã‚‹ã¹ãç›´æ„Ÿçš„ã«èª¬æ˜ã™ã‚‹ã“ã¨ã‚’è©¦ã¿ã¾ã™ã€‚ å¼·åŒ–å­¦ç¿’ã®é›£ã—ã•ãŒã‚ã‹ã£ãŸæšã«ã¯ã€ãã£ã¨ . ãã‚‚ãã‚‚å¼·åŒ–å­¦ç¿’ã‚’ä½¿ã‚ãªã„ã¨ã„ã†é¸æŠãŒã§ãã‚‹ã—ã€ | ãªã‚‹ã¹ãç°¡å˜ã«è§£ã‘ã‚‹ã‚ˆã†ãªå¼·åŒ–å­¦ç¿’ã®å•é¡Œã‚’è¨­è¨ˆã§ãã‚‹ã—ã€ | å•é¡Œã«åˆã‚ã›ã¦æ‰‹æ³•ã‚’é¸æŠã§ãã‚‹ | . ã“ã¨ã§ã—ã‚‡ã†ã€‚ . è¨˜äº‹ã®æ§‹æˆã¨ã—ã¦ã€å¼·åŒ–å­¦ç¿’ã®é›£ã—ã•ã«ã¤ã„ã¦ã€Œå ´åˆåˆ†ã‘ã€ã‚’è¡Œã„ã€ . MDPã‚’è§£ãã“ã¨ã®é›£ã—ã• | ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã“ã¨ã®é›£ã—ã• | . ã¨ã„ã†2ã¤ã®è¦³ç‚¹ã‹ã‚‰æ•´ç†ã—ã¦ã„ãã¾ã™ã€‚ . å‰æçŸ¥è­˜ã«ã¤ã„ã¦ã€åˆå¿ƒè€…ã®æ–¹ã§ã‚‚èª­ã‚ã‚‹ã‚ˆã†ã«ã€ å¼·åŒ–å­¦ç¿’ã«ã¤ã„ã¦ã®çŸ¥è­˜ã«ã¤ã„ã¦ã¯ãªã‚‹ã¹ãè¨˜äº‹ã®ä¸­ã§è£œè¶³ã—ã¾ã™ã€‚ ã—ã‹ã—ã€ã™ã”ãé›‘ã«æ›¸ãã®ã§ã€è©³ç´°ã¯Reinforcement Learning: An Introduction ãªã©ã®æ•™ç§‘æ›¸ã‚’å‚ç…§ã•ã‚Œã‚‹ã¨ã„ã„ã¨æ€ã„ã¾ã™ã€‚ ã¾ãŸã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’è¦‹ãŸã»ã†ãŒã‚¤ãƒ¡ãƒ¼ã‚¸ã—ã‚„ã™ã„ï¼ˆæ–¹ã‚‚ã„ã‚‹ï¼‰ã‹ã¨æ€ã£ã¦Pythonã®ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ãŸã¾ã«å‡ºã—ã¦ã„ã¾ã™ã€‚ ã‚³ãƒ¼ãƒ‰ä¾‹ã§ã¯ã€$ sum_{s} f(s) g(s, a)$ã®ã‚ˆã†ã«ãƒ†ãƒ³ã‚½ãƒ«ã®é©å½“ãªè»¸ã§æ›ã‘ç®—ã—ã¦è¶³ã—è¾¼ã‚€æ¼”ç®—ã« numpy.einsum ã‚’å¤šç”¨ã—ã¦ã„ã‚‹ã®ã§ã€çŸ¥ã£ã¦ã„ãŸã»ã†ãŒèª­ã¿ã‚„ã™ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ . . Suttonã‚‚èª¤å·®é€†ä¼æ’­ã‚’ä½¿ã†ã®ã«ã¯ãƒˆãƒªãƒƒã‚­ãƒ¼ãªå·¥å¤«ãŒå¿…è¦ã ã¨è¨€ã£ã¦ã„ã¾ã™ã€‚&#8617; . | 2. &#26368;&#36969;&#21270;&#21839;&#38988;&#12392;&#12375;&#12390;&#12398;&#20596;&#38754;: MDP&#12434;&#35299;&#12367;&#12371;&#12392;&#12398;&#38627;&#12375;&#12373; . å¼·åŒ–å­¦ç¿’ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒé›†ã‚ã¦ããŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã€ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ä¸Šã§ã€Œã„ã„æ„Ÿã˜ã€ã«è¡Œå‹•ã™ã‚‹æ–¹ç­–ã‚’ è¨“ç·´ã™ã‚‹å•é¡Œã®ã“ã¨ã§ã™ã€‚ ã—ã‹ã—ã“ã®ç« ã§ã¯ã€ã„ã£ãŸã‚“ã€Œå­¦ç¿’ã€ã®ã“ã¨ã¯å¿˜ã‚Œã¾ã—ã‚‡ã†ã€‚ å­¦ç¿’ã—ãªã„ã§ã„ã„ã®ã¯ã€ç’°å¢ƒã®æƒ…å ±ã‚’å…¨éƒ¨çŸ¥ã£ã¦ã„ã‚‹ã‹ã‚‰ã§ã™ã€‚ ã“ã®ã¨ãã€ã©ã®ãã‚‰ã„å•é¡Œã‚’è§£ãã®ãŒå¤§å¤‰ãªã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ . ã§ã¯ã¾ãšã€ãƒãƒ«ã‚³ãƒ•æ±ºå®šéç¨‹ (Markov Decision Process, ä»¥å¾ŒMDPã¨å‘¼ã¶)ã‚’å°å…¥ã—ã¾ã™ã€‚ . . å®šç¾©1: MDP . MDPã¯ã€ çŠ¶æ…‹é›†åˆ $ mathcal{S}$ã€ è¡Œå‹•é›†åˆ $ mathcal{A}$ã€ çŠ¶æ…‹é·ç§»ç¢ºç‡ $P: mathcal{S} times mathcal{A} rightarrow mathcal{S}$, å ±é…¬é–¢æ•° $r: mathcal{S} times mathcal{A} rightarrow mathbb{R}$1, å‰²å¼•å ±é…¬ç‡ $0 leq gamma &lt; 1$ ã‹ã‚‰æˆã‚‹ . . ãªã‚“ã ã‹ã€è¨˜å·ã§è¨€ã‚ã‚Œã¦ã‚‚ã‚ˆãã‚ã‹ã‚‰ãªã„ã§ã™ã­ã€‚ ã›ã£ã‹ããªã®ã§ã€ãŠçµµæãã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . . ã“ã®å ±é…¬é–¢æ•°ã¯æœ€ã‚‚ç°¡å˜ãªå®šç¾©ã§ã™ã€‚ä»–ã«$r: mathcal{S} times mathcal{A} times mathcal{S} rightarrow mathbb{R}$(é·ç§»å…ˆã«ä¾å­˜)ã€$r: mathcal{S} times mathcal{A} rightarrow mathbb{E}[R_{s, a}]$ï¼ˆç¢ºç‡çš„ï¼‰ãŒã‚ã‚Šã¾ã™ã€‚&#8617; . | from typing import Dict, List, Optional, Sequence, Tuple, TypeVar import numpy as np from matplotlib import pyplot as plt from matplotlib.axes import Axes from matplotlib.text import Annotation # Some types for annotations T = TypeVar(&quot;T&quot;) class Array(Sequence[T]): pass Array1 = Array[float] Array2 = Array[Array1] Array3 = Array[Array2] Point = Tuple[float, float] def a_to_b( ax: Axes, a: Point, b: Point, text: str = &quot;&quot;, style: str = &quot;normal&quot;, **kwargs, ) -&gt; Annotation: &quot;&quot;&quot;Draw arrow from a to b. Optionally&quot;&quot;&quot; STYLE_ALIASES: Dict[str, str] = { &quot;normal&quot;: &quot;arc3,rad=-0.4&quot;, &quot;self&quot;: &quot;arc3,rad=-1.6&quot;, } arrowkwargs = {} for arrowkey in list(filter(lambda key: key.startswith(&quot;arrow&quot;), kwargs)): arrowkwargs[arrowkey[5:]] = kwargs.pop(arrowkey) if len(text) &gt; 0: bbox = dict( boxstyle=&quot;round&quot;, fc=&quot;w&quot;, ec=arrowkwargs.get(&quot;color&quot;, &quot;k&quot;), alpha=arrowkwargs.get(&quot;alpha&quot;, 1.0), ) else: bbox = None return ax.annotate( text, xy=b, xytext=a, arrowprops=dict( shrinkA=10, shrinkB=10, width=1.0, headwidth=6.0, connectionstyle=STYLE_ALIASES.get(style, style), **arrowkwargs, ), bbox=bbox, **kwargs, ) class ChainMDP: &quot;&quot;&quot;Chain MDP with N states and two actions.&quot;&quot;&quot; ACT_COLORS: List[str] = [&quot;xkcd:vermillion&quot;, &quot;xkcd:light royal blue&quot;] INTERVAL: float = 1.2 OFFSET: float = 0.8 SHIFT: float = 0.5 HEIGHT: float = 4.0 def __init__( self, success_probs: Sequence[Sequence[float]], reward_function: Sequence[Sequence[float]], ) -&gt; None: success_probs = np.array(success_probs) self.n_states = success_probs.shape[0] assert success_probs.shape[1] == 2 np.testing.assert_almost_equal(success_probs &gt;= 0, np.ones_like(success_probs)) np.testing.assert_almost_equal(success_probs &lt;= 1, np.ones_like(success_probs)) self.p = np.zeros((self.n_states, 2, self.n_states)) for si in range(self.n_states): left, right = max(0, si - 1), min(self.n_states - 1, si + 1) # Action 0 is for right self.p[si][0][right] += success_probs[si][0] self.p[si][0][si] += 1.0 - success_probs[si][0] # Action 1 is for left self.p[si][1][left] += success_probs[si][1] self.p[si][1][si] += 1.0 - success_probs[si][1] self.r = np.array(reward_function) # |S| x 2 assert self.r.shape == (self.n_states, 2) # For plotting self.circles = [] self.cached_ax = None def figure_shape(self) -&gt; Tuple[int, int]: width = self.n_states + (self.n_states - 1) * self.INTERVAL + self.OFFSET * 2.5 height = self.HEIGHT return width, height def show(self, title: str = &quot;&quot;, ax: Optional[Axes] = None) -&gt; Axes: if self.cached_ax is not None: return self.cached_ax from matplotlib.patches import Circle width, height = self.figure_shape() circle_position = height / 2 - height / 10 if ax is None: fig = plt.figure(title or &quot;ChainMDP&quot;, (width, height)) ax = fig.add_axes([0, 0, 1, 1], aspect=1.0) ax.set_xlim(0, width) ax.set_ylim(0, height) ax.set_xticks([]) ax.set_yticks([]) def xi(si: int) -&gt; float: return self.OFFSET + (1.0 + self.INTERVAL) * si + 0.5 self.circles = [ Circle((xi(i), circle_position), 0.5, fc=&quot;w&quot;, ec=&quot;k&quot;) for i in range(self.n_states) ] for i in range(self.n_states): x = self.OFFSET + (1.0 + self.INTERVAL) * i + 0.1 ax.text(x, height * 0.85, f&quot;State {i}&quot;, fontsize=16) def annon(act: int, prob: float, *args, **kwargs) -&gt; None: # We don&#39;t hold references to annotations (i.e., we treat them immutable) a_to_b( ax, *args, **kwargs, arrowcolor=self.ACT_COLORS[act], text=f&quot;P: {prob:.02}&quot;, arrowalpha=prob, fontsize=11, ) for si in range(self.n_states): ax.add_patch(self.circles[si]) x = xi(si) # Action 0: y = circle_position + self.SHIFT if si &lt; self.n_states - 1 and 1e-3 &lt; self.p[si][0][si + 1]: p_right = self.p[si][0][si + 1] annon( 0, p_right, (x + self.SHIFT, y), (xi(si + 1) - self.SHIFT * 1.2, y - self.SHIFT * 0.3), verticalalignment=&quot;center_baseline&quot;, ) else: p_right = 0.0 if p_right + 1e-3 &lt; 1.0: annon( 0, 1.0 - p_right, (x - self.SHIFT * 1.2, y), (x + self.SHIFT * 0.5, y - self.SHIFT * 0.1), style=&quot;self&quot;, verticalalignment=&quot;bottom&quot;, ) ax.text( x - self.SHIFT * 1.2, y + self.SHIFT * 1.4, f&quot;r({si}, 0): {self.r[si][0]:+.02}&quot;, color=self.ACT_COLORS[0], fontsize=14, ) # Action 1: y = circle_position - self.SHIFT if 0 &lt; si and 1e-3 &lt; self.p[si][1][si - 1]: p_left = self.p[si][1][si - 1] annon( 1, self.p[si][1][si - 1], (x - self.SHIFT * 1.6, y), (xi(si - 1) + self.SHIFT * 1.4, y + self.SHIFT * 0.2), verticalalignment=&quot;top&quot;, ) else: p_left = 0.0 if p_left + 1e-3 &lt; 1.0: annon( 1, 1.0 - p_left, (x + self.SHIFT * 0.4, y), (x - self.SHIFT * 0.45, y + self.SHIFT * 0.1), style=&quot;self&quot;, verticalalignment=&quot;top&quot;, ) ax.text( x - self.SHIFT * 1.2, y - self.SHIFT * 1.4, f&quot;r({si}, 1): {self.r[si][1]:+.02}&quot;, color=self.ACT_COLORS[1], fontsize=14, ) for i in range(2): ax.plot([0.0], [0.0], color=self.ACT_COLORS[i], label=f&quot;Action {i}&quot;) ax.legend(fontsize=11, loc=&quot;upper right&quot;) if len(title) &gt; 0: ax.text(0.06, height * 0.9, title, fontsize=18) self.cached_ax = ax return ax . . mdp1 = ChainMDP( [[0.6, 0.7], [0.2, 0.4], [0.3, 0.7]], [[0.6, 0.2], [0.0, 0.1], [-0.1, 0.5]] ) ax = mdp1.show(title=&quot;MDP1&quot;) . MDPã¯ã“ã®ã‚ˆã†ã«ã€çŠ¶æ…‹ã‚’é ‚ç‚¹ã¨ã™ã‚‹ã‚°ãƒ©ãƒ•ã¨ã—ã¦è¡¨ç¾ã§ãã¾ã™ã€‚ ã“ã®MDPã«ã¯3ã¤ã®çŠ¶æ…‹(å›³ä¸­ã®State 0, 1, 2)ãŒã‚ã‚Šã€å„çŠ¶æ…‹ã§ã€2ã¤ã®è¡Œå‹•ã®ã©ã¡ã‚‰ã‹ã‚’é¸æŠã§ãã¾ã™ã€‚ ã‚ˆã£ã¦ $ mathcal{S} = {s_0, s_1, s_2 }, mathcal{A} = {a_0, a_1 }$ã§ã™ã€‚ å„è¡Œå‹•ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ç‰¹å¾´ã¥ã‘ã‚‰ã‚Œã¾ã™ã€‚ . èµ¤ã®è¡Œå‹•(Action 0) ã‚’ã¨ã‚‹ã¨ã€ä¸€ã¤å³ã®é ‚ç‚¹ã¸é·ç§»ã™ã‚‹ã‹ã€å¤±æ•—ã—ã¦ä»Šã®é ‚ç‚¹ã«ã¨ã©ã¾ã‚‹ | é’ã®è¡Œå‹•(Action 1) ã‚’ã¨ã‚‹ã¨ã€ä¸€ã¤å·¦ã®é ‚ç‚¹ã¸é·ç§»ã™ã‚‹ã‹ã€å¤±æ•—ã—ã¦ä»Šã®é ‚ç‚¹ã«ã¨ã©ã¾ã‚‹ | . ã“ã®ã‚ˆã†ãªç¢ºç‡çš„ãªã‚°ãƒ©ãƒ•ã®ä¸Šã§ã€ã‚ã‚‹è¡Œå‹•æ–¹é‡ $ pi: mathcal{S} times mathcal{A} rightarrow mathbb{R}$ ã«å¾“ã£ã¦è¡Œå‹•ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è€ƒãˆã¾ã™ã€‚ è¡Œå‹•æ–¹é‡ãŒæ±ºã¾ã‚Œã°ã€è²°ãˆã‚‹å‰²å¼•å ±é…¬å’Œã®æœŸå¾…å€¤ãŒã€ä»¥ä¸‹ã®ã‚ˆã†ã«å®šã¾ã‚Šã¾ã™ã€‚ ã“ã‚Œã‚’çŠ¶æ…‹ä¾¡å€¤ã¨å‘¼ã³ã¾ã™ã€‚ . . å®šç¾©2: çŠ¶æ…‹ä¾¡å€¤é–¢æ•° $V^ pi$ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã™ã‚‹ã€‚ . $V^ pi(s) := sum_{a in mathcal{A}} pi(a|s) left( r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s, a) V^ pi(s&#39;) right)$ . . ã“ã®æ™‚ã€å¼·åŒ–å­¦ç¿’ã¯ $ sum_{s in mathcal{S}} V^ pi(s)$ ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ãª $ pi$ã‚’æ±‚ã‚ã‚‹å•é¡Œã«ãªã‚Šã¾ã™ã€‚1 ãã®ã‚ˆã†ãª $ pi$ ã‚’ æœ€é©æ–¹ç­– $ pi^*$ ã¨å‘¼ã³ã¾ã™ã€‚ã¾ãŸã€$V^{ pi^*}$ ã‚’ç•¥è¨˜ã—ã¦ $V^*$ ã¨æ›¸ãã¾ã™ã€‚ . æ‰‹å§‹ã‚ã«ã€ç°¡å˜ãªå•é¡Œã§ $V^*$ ã‚’æ±‚ã‚ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . . åˆæœŸçŠ¶æ…‹åˆ†å¸ƒ(é›‘ã«è¨€ã†ã¨ã€ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã®åˆ†å¸ƒ)ã‚’$ mu(s)$ã¨ã™ã‚‹ã¨ã€$ sum_{s in mathcal{S}} mu(s)V_ pi(s)$ ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç²å¾—ã™ã‚‹å‰²å¼•å ±é…¬å’Œã®æœŸå¾…å€¤ã§ã™ã€‚$V_ pi(s)$ ãŒæœ€å¤§ãªã‚‰ã“ã‚Œã‚‚æœ€å¤§ã«ãªã‚Šã¾ã™ã€‚&#8617; . | mdp2 = ChainMDP( [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0]], [[0.0, 0.0], [0.0, 0.0], [1.0, 0.0]] ) _ = mdp2.show(&quot;MDP2&quot;) . . ã“ã®å•é¡Œã®æœ€é©æ–¹ç­– $ pi^*$ ã¯ã€æ˜ã‚‰ã‹ã§ã™ã­ã€‚ ä¸€ç•ªå³ã®çŠ¶æ…‹ã§ã—ã‹å ±é…¬ãŒå‡ºãªã„ã®ã§ã€å…¨ã¦ã®çŠ¶æ…‹ã«ãŠã„ã¦å³ã«è¡Œã‘ã°ã„ã„ã§ã™ã€‚ . ã¾ãŸã€ã“ã®æ™‚ã® $V^*$ ã‚‚æ±‚ã‚ã¦ã¿ã¾ã™ã€‚ . ã¾ãšã€çŠ¶æ…‹2ã¯ å¸å¼•çŠ¶æ…‹ ã§ã‚ã‚Šã€çŠ¶æ…‹2ã«è¾¿ã‚Šã¤ã„ã¦ã—ã¾ã£ãŸã‚‰ã‚‚ã†ä»–ã®çŠ¶æ…‹ã«ã¯é·ç§»ã§ãã¾ã›ã‚“ã€‚ ã‚ˆã£ã¦ã€ . $$ begin{aligned} V^*(2) &amp;= 1.0 + gamma V^*(2) ã€€ &amp;= 1.0 + gamma (1.0 + gamma V^*(2)) &amp;= 1.0 + gamma + gamma^2 + gamma^3 + ... &amp;= lim_{n to infty} frac{1 - gamma ^ n}{1 - gamma} &amp;= frac{1}{1 - gamma} ~~~~~( gamma &lt; 1ã‚ˆã‚Š) end{aligned} $$ã¨ãªã‚Šã¾ã™ã€‚ ãŸã¨ãˆã° $ gamma = 0.9$ ãªã‚‰ã€ . $$ begin{aligned} V^*(2) &amp;= frac{1}{1 - gamma} = 10 V^*(1) &amp;= 0.0 + gamma * V^*(2) = 9 V^*(0) &amp;= 0.0 + gamma * V^*(1) = 8.1 end{aligned} $$ã§ã™ã­ã€‚ . ã“ã®ä¾‹ã‹ã‚‰ã€$ pi$ã«ã‚‚ã¨ã¥ã„ã¦è¡Œå‹•ã—ãŸæ™‚ã€å¸å¼•çŠ¶æ…‹ä»¥å¤–ã®ãƒ«ãƒ¼ãƒ—ã«ã¯ã¾ã‚Šãˆãªã„ãªã‚‰ã€ $O(| mathcal{S}|^2| mathcal{A}|)$ (çŠ¶æ…‹æ•°ã®2ä¹—ã¨è¡Œå‹•æ•°ã«æ¯”ä¾‹ã™ã‚‹ã‚ªãƒ¼ãƒ€ãƒ¼)ã§ $V^ pi$ ãŒè©•ä¾¡ã§ãã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ ã“ã®å ´åˆã¯$ pi$ã‚‚$P$ã‚‚æ±ºå®šçš„ãªã®ã§å®Ÿè³ª$O(|S|)$ã«ãªã£ã¦ã„ã¾ã™ã€‚ . 2.1: $V^ pi cdot V^*$&#12434;&#35413;&#20385;&#12377;&#12427;&#38627;&#12375;&#12373; . ã§ã¯ã€ã‚‚ã†å°‘ã—é›£ã—ã„MDPã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . mdp3 = ChainMDP( [[1.0, 0.0], [0.8, 1.0], [1.0, 0.9]], [[0.0, 0.0], [0.5, 0.0], [0.0, 1.0]] ) _ = mdp3.show(&quot;MDP3&quot;) . . ä»Šåº¦ã¯ã€State 1ã§å³ã«ã€State 2ã§å·¦ã«è¡Œã‘ã°è‰¯ã•ãã†ã§ã™ã€‚ $$ begin{aligned} V^* (1) = 0.5 + gamma (0.1 * V^*(1) + 0.9 * V^*(2)) V^* (2) = 1.0 + gamma (0.8 * V^*(1) + 0.2 * V^*(2)) end{aligned} $$ . å…ˆã»ã©ã®å•é¡Œã¨é•ã£ã¦1ã‚‚2ã‚‚å¸å¼•çŠ¶æ…‹ã§ã¯ãªã„ã®ã§ã€$V(1)$ã¨$V(2)$ãŒãŠäº’ã„ã«ä¾å­˜ã™ã‚‹é¢å€’ãª æ–¹ç¨‹å¼ãŒå‡ºã¦ãã¦ã—ã¾ã„ã¾ã—ãŸã€‚ ã“ã®ã‚ˆã†ãªãƒ«ãƒ¼ãƒ—ã®å­˜åœ¨ãŒã€å¼·åŒ–å­¦ç¿’ã‚’é›£ã—ãã—ã¦ã„ã‚‹è¦ç´ ã®ä¸€ã¤ã§ã™ã€‚ . ã¨ã¯ã„ãˆã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã§æ•°å€¤çš„ã«è§£ãã®ã¯ç°¡å˜ã§ã™ã€‚ çŠ¶æ…‹$s$ã«ã„ã¦ã€ã‚ã¨$n$å›è¡Œå‹•ã§ãã‚‹æ™‚ã®ä¾¡å€¤é–¢æ•°ã‚’$V_n^ pi(s)$ã¨æ›¸ãã¾ã™ã€‚ ä»»æ„ã®$s$ã«ã¤ã„ã¦ã€$V_0^ pi(s) = 0$ã§ã™ï¼ˆ1å›ã‚‚è¡Œå‹•ã§ããªã„ã®ã§!ï¼‰ã€‚ $V_i^ pi$ ã‹ã‚‰ $V_{i + 1}^ pi$ ã‚’æ±‚ã‚ã‚‹ã«ã¯ã€1ã‚¹ãƒ†ãƒƒãƒ—ã ã‘å…ˆèª­ã¿ã™ã‚Œã°ã„ã„ã®ã§ã€ $$ V_{i + 1}^ pi(s) = sum_{a in mathcal{A}} pi(a|s) left( r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s, a) V_{i}^ pi(s&#39;) right) $$ ã§è¨ˆç®—ã§ãã¾ã™ã€‚$ gamma &lt; 1$ã«ã‚ˆã‚Šã“ã®åå¾©è¨ˆç®—ã¯åæŸã—ã€$V^ pi$ ãŒæ±‚ã¾ã‚Šã¾ã™ã€‚ å®Ÿéš›ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§æ›¸ã„ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . MAX_ITER_V_PI: int = int(1e5) def v_pi( r: Array2, p: Array3, pi: Array2, gamma: float, epsilon: float, ) -&gt; Tuple[Array1, int]: v = np.zeros(r.shape[0]) # VÏ€ r_pi = np.einsum(&quot;sa,sa-&gt;s&quot;, pi, r) # |S|, Ï€ã‚’ä½¿ã£ãŸã¨ãã«è²°ã†å ±é…¬ã®ãƒ™ã‚¯ãƒˆãƒ« p_pi = np.einsum(&quot;saS,sa-&gt;sS&quot;, p, pi) # |S| x |S|, Ï€ã‚’ä½¿ã£ãŸã¨ãã®çŠ¶æ…‹é·ç§»ç¢ºç‡ for n_iter in range(MAX_ITER_V_PI): v_next = r_pi + gamma * np.einsum(&quot;s,sS&quot;, v, p_pi.T) if np.all(np.absolute(v_next - v) &lt; epsilon): return v_next, n_iter + 1 v = v_next # ç†è«–çš„ã«ã¯å¿…ãšåæŸã™ã‚‹ã®ã§ã€ãƒã‚°äºˆé˜² raise RuntimeError(&quot;Policy Evaluation did not converge &gt;_&lt;&quot;) pi_star = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]) v_star_mdp3, n_iter = v_pi(mdp3.r, mdp3.p, pi_star, gamma=0.9, epsilon=1e-4) print(f&quot;åå¾©å›æ•°: {n_iter}&quot;) print(&quot; &quot;.join([f&quot;V({i}): {v:.3}&quot; for i, v in enumerate(v_star_mdp3)])) . åå¾©å›æ•°: 86 V(0): 6.49 V(1): 7.21 V(2): 7.51 . 86å›ã“ã®è¨ˆç®—ã‚’åå¾©ã—ãŸå¾Œã€ãªã‚“ã‹ãã‚Œã£ã½ã„æ•°å­—ãŒå‡ºã¦ãã¾ã—ãŸã€‚ ã“ã®åå¾©å›æ•°ã¯ã€ä½•ã«ã‚ˆã£ã¦æ±ºã¾ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ . ä»»æ„ã® $s$ ã«ã¤ã„ã¦ $|V_{i+1}^ pi(s) - V_i^ pi(s)| &lt; epsilon$ ãªã‚‰è¨ˆç®—çµ‚ã‚ã‚Šã€ã¨ã—ã¾ã™1ã€‚ $V_n^ pi(s)$ã¯ã€Œã‚ã¨$n$ã‚¹ãƒ†ãƒƒãƒ—è¡Œå‹•ã§ãã‚‹æ™‚ã®çŠ¶æ…‹ä¾¡å€¤ã®æœŸå¾…å€¤ã€ãªã®ã§ã€$i$ ã‚¹ãƒ†ãƒƒãƒ—ç›®ã«ã‚‚ã‚‰ã£ãŸå ±é…¬ã‚’ $R_i$ã¨ã™ã‚‹ã¨ã€ $$ V_n^ pi(s) = mathbb{E}_{s, pi} left[ R_1 + gamma R_2 + gamma^2 R_3 + ... right] $$ ã¨æ›¸ã‘ã¾ã™ã€‚ ãªã®ã§ã€å ±é…¬ã®ç¯„å›²ãŒ$0 leq R_t &lt; R_ textrm{max}$ã ã¨ä»®å®šã™ã‚‹ã¨ã€ $ gamma^{k - 1} R_ textrm{max} &lt; epsilon$ãªã‚‰ã“ã®æ•°å€¤è¨ˆç®—ãŒåæŸã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ ç°¡å˜ã®ãŸã‚$R_ textrm{max}=1$ã¨ã—ã¦ã¿ã‚‹ã¨ã€$k$ãŒæº€ãŸã™ã¹ãæ¡ä»¶ã¯ $$ gamma^{k-1} &lt; epsilon Leftrightarrow k &lt; frac{ log epsilon}{ log gamma} + 1 $$ ã¨ãªã‚Šã¾ã™ã€‚ ã‚³ãƒ¼ãƒ‰ã®ä¸­ã§ $ gamma = 0.9, epsilon = 0.0001$ ã¨ã—ãŸã®ã§ã€ãŸã‹ã ã‹89å›ã®åå¾©ã§åæŸã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ å®Ÿé¨“çµæœã§ã¯86å›ã ã£ãŸã®ã§ã€ã ã„ãŸã„åŒã˜ãã‚‰ã„ã§ã™ã­ã€‚ . ã‚ˆã£ã¦ã€$V^ pi$ã‚’åå¾©æ³•ã«ã‚ˆã‚Šè©•ä¾¡ã—ãŸæ™‚ã€ãã®åå¾©å›æ•°ã¯å ±é…¬ãƒ»$ epsilon$ãƒ»$ gamma$ã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ å ±é…¬ã¨$ epsilon$ã«ã¯$ log$ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã§ã—ã‹ä¾å­˜ã—ãªã„ã®ã«å¯¾ã—ã€$ gamma$ã«å¯¾ã—ã¦ã¯ $O((- log gamma)^{-1})$ã®ã‚ªãƒ¼ãƒ€ãƒ¼ã§ä¾å­˜ã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ è©¦ã—ã«ã€$ epsilon=0.0001$ã®æ™‚ã®$ frac{ log epsilon}{ log gamma}$ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . . ã“ã®$ epsilon$ã¯$ epsilon$-Optimal Policyã®$ epsilon$ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚&#8617; . | ã“ã®ã‚ˆã†ã«ã€$ gamma$ãŒå¤§ãããªã‚‹ã¨ä¸€æ°—ã«åå¾©å›æ•°ãŒå¢—ãˆã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚ ã¾ãŸã€ã“ã®æ•°å€¤è¨ˆç®—ãŒåæŸã—ãŸæ™‚ã€çœŸã®$V^ pi$ã¨ã®å·®ãŒ $$ begin{aligned} V^ pi(s) - V_k^ pi(s) &amp;= mathbb{E}_{s, pi} left[ gamma^k R_{k + 1} + gamma^{k + 1} R_{k + 2} ... right] &amp;&lt; frac{ gamma^k R_ textrm{max}}{1 - gamma} &lt; frac{ gamma epsilon}{1 - gamma} end{aligned} $$ ã§æŠ‘ãˆã‚‰ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‹ã‚Šã¾ã™ã€‚ . æ¬¡ã¯ã€ã„ããªã‚Š$V^*$ã‚’æ±‚ã‚ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ $V^ pi$ã‚’æ±‚ã‚ãŸæ™‚ã¨åŒã˜ã‚ˆã†ã«ã€çŠ¶æ…‹$s$ã«ã„ã¦ã€ ã‚ã¨$n$å›è¡Œå‹•ã§ãã‚‹æ™‚ã®æœ€é©ä¾¡å€¤é–¢æ•°ã‚’$V_n^*(s)$ã¨æ›¸ãã¾ã™ã€‚ å…ˆã»ã©ã¨åŒæ§˜ã«ã€$V_i^*$ã‹ã‚‰1ã‚¹ãƒ†ãƒƒãƒ—å…ˆèª­ã¿ã—ã¦$V^*_{i + 1}$ã‚’æ±‚ã‚ã¾ã™ã€‚ æ®‹ã‚Š$i + 1$ã‚¹ãƒ†ãƒƒãƒ—ã‚ã‚‹æ™‚ã€ $r(s, a) + sum_{s&#39; in mathcal{S}} P(s&#39;|s, a) V_i^*(s)$ ãŒæœ€å¤§ã«ãªã‚‹ã‚ˆã†ãªè¡Œå‹•$a$ã‚’é¸ã¶ã®ãŒæœ€é©ã§ã™ã€‚ ã§ã™ã‹ã‚‰ã€$V^*_{i + 1}$ã¯ $$ V_{i + 1}^*(s) = max_a left( r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s, a) V_{i}^ pi(s&#39;) right) $$ ã§æ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚ ã•ã£ãã‚ˆã‚Šç°¡å˜ãªå¼ã«ãªã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§æ›¸ã„ã¦ã¿ã¾ã™ã€‚ . MAX_ITER_VI: int = int(1e6) def value_iteration( r: Array2, p: Array3, gamma: float, epsilon: float, ) -&gt; Tuple[Array1, int]: v = np.zeros(p.shape[0]) for n_iter in range(MAX_ITER_VI): # ã“ã‚Œâ†“ã¯Q Valueã¨ã‚‚è¨€ã„ã¾ã™ r_plus_gamma_pv = r + gamma * np.einsum(&quot;saS,S-&gt;sa&quot;, p, v) v_next = r_plus_gamma_pv.max(axis=1) if np.all(np.absolute(v_next - v) &lt; epsilon): return v_next, n_iter + 1 v = v_next raise RuntimeError(&quot;Value Iteration did not converge &gt;_&lt;&quot;) v_star_mdp3_vi, n_iter = value_iteration(mdp3.r, mdp3.p, 0.9, 1e-4) print(f&quot;åå¾©å›æ•°: {n_iter}&quot;) print(&quot; &quot;.join([f&quot;V({i}): {v:.3}&quot; for i, v in enumerate(v_star_mdp3_vi)])) . åå¾©å›æ•°: 86 V(0): 6.49 V(1): 7.21 V(2): 7.51 . å…ˆç¨‹ã¨åŒã˜ãã€86å›ã®åå¾©ã§$V^*$ãŒæ±‚ã¾ã‚Šã¾ã—ãŸã€‚ ã“ã®åå¾©å›æ•°ã‚‚ã€å…ˆã»ã©ã®$V^ pi$ã¨åŒã˜ã‚ˆã†ã«$ gamma, epsilon$ã‚’ç”¨ã„ã¦æŠ‘ãˆã‚‰ã‚Œã¾ã™ã€‚ . ã—ã‹ã—ã€$ epsilon$ã¯äººæ‰‹ã§è¨­å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ã‚¿ã§ã™ã€‚ æœ€é©æ–¹ç­–ãŒæ±‚ã¾ã‚Œã°$V^*$ã¯å¤§ã—ã¦æ­£ç¢ºã§ãªãã¨ã‚‚å›°ã‚‰ãªã„ã¨ã„ã†å ´åˆã¯ã€ã‚‚ã£ã¨$ epsilon$ã‚’å¤§ããã—ã¦ã€ è¨ˆç®—ã‚’æ—©ãçµ‚ã‚ã‚‰ã›ãŸã„æ°—ãŒã—ã¾ã™ã€‚ ã§ã¯ã€ã€Œã©ã‚“ãªå ´åˆã«$ epsilon$ã‚’å¤§ããã§ãã‚‹ã‹ã€ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . ç°¡å˜ã®ãŸã‚ã€ $Q^ pi(s, a) = r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s, a) V^ pi(s&#39;)$ (Qã¯Qualityã®Qã‚‰ã—ã„)ã‚’å°å…¥ã—ã¾ã™ã€‚ æ®‹ã‚Š$k$ã‚¹ãƒ†ãƒƒãƒ—ã‚ã‚‹æ™‚ã®æœ€é©è¡Œå‹•ã‚’$a_k^* = textrm{argmax}_a Q_k^*(s, a)$ã¨ã—ã¾ã™ã€‚ ã™ã‚‹ã¨ã€$k+1$ã‚¹ãƒ†ãƒƒãƒ—ç›®ä»¥é™ã®å‰²å¼•å ±é…¬å’Œã¯ $ frac{ gamma^{k}R_ textrm{max}}{1 - gamma}$ã§æŠ‘ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ $$ Q_k^*(s, a_k^*) - max_{a neq a_k^*} Q_k^*(s, a) &lt; frac{ gamma^k R_ textrm{max}}{1 - gamma} $$ ãŒæˆã‚Šç«‹ã¤ãªã‚‰ã€$a_k^*$ãŒä»Šå¾Œä»–ã®è¡Œå‹•ã«é€†è»¢ã•ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ãªã®ã§$a_k^*$ãŒæœ€é©ã§ã„ã„ã‚ˆã­ã€ã¯ã„ã“ã®è©±çµ‚ã‚ã‚Šã€ã¨ã„ã†ã“ã¨ã«ãªã‚Šã¾ã™ã€‚ ä»¥ä¸‹ç•¥è¨˜ã—ã¦ $A_ textrm{min}^*(s, a_k) = Q_k^*(s, a_k^*) - max_{a neq a_k^*} Q_k^*(s, a)$ ã¨æ›¸ãã¾ã™ï¼ˆä»–ã®è¡Œå‹•ã«å¯¾ã™ã‚‹ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã®æœ€å°å€¤ã¨ã„ã†æ„å‘³ï¼‰ã€‚ ã„ã¾$ gamma^{k-1} R_ textrm{max}&lt; epsilon$ãŒçµ‚äº†æ¡ä»¶ãªã®ã§ã€ $$ A_ textrm{min}^*(s, a_k) &lt; frac{ epsilon gamma}{1 - gamma} Leftrightarrow frac{A_ textrm{min}^*(s, a_k)(1 - gamma)}{ gamma}&lt; epsilon $$ ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚ ã“ã‚ŒãŒæ„å‘³ã™ã‚‹ã®ã¯ã€$V*$ã¨äºŒç•ªç›®ã«ã„ã„$Q^*(s, a)$ã¨ã®å·®ãŒå¤§ãã„ã»ã©$ epsilon$ã‚’å¤§ããã§ãã‚‹ã¨ã„ã†ã“ã¨ã§ã™ã€‚ . ã“ã“ã¾ã§ã®è­°è«–ã‹ã‚‰ã€è¨ˆç®—é‡ã®è¦³ç‚¹ã§ã¯ã€ . $ gamma$ãŒå¤§ãã„ã»ã©MDPã‚’è§£ãã®ãŒé›£ã—ã„ | æœ€é©è§£ã¨äºŒç•ªç›®ã«ã„ã„è§£ã¨ã®å·®ãŒå°ã•ã„ã»ã©MDPã‚’è§£ãã®ãŒé›£ã—ã„ | . ã¨ã„ã†2ç‚¹ãŒè¨€ãˆãã†ã§ã™ã­ã€‚ . 2.2: &#26041;&#31574;&#26368;&#36969;&#21270;&#12398;&#38627;&#12375;&#12373; . å‰ç¯€ã§ç”¨ã„ãŸå†å¸°çš„ãªæ•°å€¤è¨ˆç®—ã¯å‹•çš„è¨ˆç”»æ³•(DP)ã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã§ã™ã€‚ Qå­¦ç¿’ãªã©ã€å¤šãã®å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒDPã‚’ã‚‚ã¨ã«ã—ã¦ã„ã¾ã™ã€‚ ä¸€æ–¹ã§ã€å˜ã«å¼·åŒ–å­¦ç¿’ã‚’ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹æœ€é©åŒ–ã ã¨è€ƒãˆã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ ç‰¹ã«ã€æ–¹ç­–ãƒ‘ãƒ©ãƒ¡ã‚¿$ theta$ã‚’æœ€é©åŒ–ã—ã¦è§£ãæ–¹æ³•ã‚’æ–¹ç­–æœ€é©åŒ–ã¨å‘¼ã³ã¾ã™ã€‚ . ã„ã¾ã€$ pi(0|s) = theta(s), pi(1|s) = 1.0 - theta(s)$ã«ã‚ˆã£ã¦$ pi$ã‚’ãƒ‘ãƒ©ãƒ¡ã‚¿$ theta$ã«ã‚ˆã‚Šè¡¨ã™ã“ã¨ã«ã—ã¾ã™ ï¼ˆã“ã‚Œã‚’direct parameterizationã¨å‘¼ã³ã¾ã™ï¼‰ã€‚ ãŸã‚ã—ã«ã€å…ˆã»ã©ã®MDP3ã§$ pi(0|0)=1.0$ã‚’å›ºå®šã—ã¦ã€$ theta(1), theta(2)$ã‚’å‹•ã‹ã—ãŸæ™‚ã®$ sum_{s in mathcal{S}} V^ pi(s)$ã®å¤‰å‹•ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . def v_pi_sum_2dim( r: Array2, p: Array3, gamma: float, epsilon: float, initial_pi: Array2, states: Tuple[int, int], n_discretization: int, ) -&gt; Array2: res = [] for i2 in range(n_discretization + 1): p2 = (1.0 / n_discretization) * i2 for i1 in range(n_discretization + 1): p1 = (1.0 / n_discretization) * i1 pi = initial_pi.copy() pi[states[0]] = p1, 1 - p1 pi[states[1]] = p2, 1 - p2 res.append(v_pi(r, p, pi, gamma, epsilon)[0].sum()) return np.array(res).reshape(n_discretization + 1, -1) def plot_piv_heatmap( data: Array2, xlabel: str = &quot;&quot;, ylabel: str = &quot;&quot;, title: str = &quot;&quot;, ax: Optional[Axes] = None, ) -&gt; Axes: from matplotlib.ticker import LinearLocator if ax is None: fig = plt.figure() ax = fig.add_subplot(111, projection=&quot;3d&quot;) n_discr = data.shape[0] x, y = np.meshgrid(np.linspace(0, 1, n_discr), np.linspace(0, 1, n_discr)) ax.plot_surface(x, y, data, cmap=&quot;inferno&quot;, linewidth=0, antialiased=False) ax.zaxis.set_major_locator(LinearLocator(10)) ax.zaxis.set_major_formatter(&#39;{x:.01f}&#39;) ax.set_xlabel(xlabel, fontsize=14) ax.set_ylabel(ylabel, fontsize=14) ax.set_zlabel(&quot;âˆ‘VÏ€&quot;, fontsize=14, horizontalalignment=&quot;right&quot;) ax.set_title(title, fontsize=15) return ax initial_pi = np.array([[1.0, 0.0], [0.5, 0.5], [0.5, 0.5]]) v_pi_sums = v_pi_sum_2dim(mdp3.r, mdp3.p, 0.9, 1e-4, initial_pi, (1, 2), 20) ax = plot_piv_heatmap(v_pi_sums, &quot;Î¸(1)&quot;, &quot;Î¸(2)&quot;, &quot;MDP3&quot;) _ = ax.set_xlim(tuple(reversed(ax.get_xlim()))) _ = ax.set_ylim(tuple(reversed(ax.get_ylim()))) . . ãªã‚“ã‹ã„ã„æ„Ÿã˜ã«å±±ã«ãªã£ã¦ã„ã¾ã™ã­ã€‚ ã“ã®å•é¡Œã®å ´åˆã¯ã€å±±ç™»ã‚Šæ³•ï¼ˆå‹¾é…ä¸Šæ˜‡æ³•ï¼‰ã§$ theta$ã‚’æ›´æ–°ã—ã¦ã„ã‘ã°å¤§åŸŸè§£ $ theta(1) = 0.0, theta(2) = 1.0$ã«ãŸã©ã‚Šç€ããã†ã§ã™1ã€‚ . ã—ã‹ã—ã€$f( theta) = sum_{s in mathcal{S}} V^{ pi_ theta}(s)$ã¯ã€ã„ã¤ã‚‚ã“ã®ã‚ˆã†ãªæ€§è³ª ã®ã„ã„é–¢æ•°ã«ãªã£ã¦ã„ã‚‹ã®ã§ã—ã‚‡ã†ãŒï¼Ÿ çµè«–ã‹ã‚‰è¨€ã†ã¨ãã†ã§ã¯ãªã„ã§ã™ã€‚ ä¾‹ãˆã°ã€ä»¥ä¸‹ã®ã‚ˆã†ãªMDPã§ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ($ gamma=0.95$ã«ã—ã¦ã„ã¾ã™) . . ã“ã®è¨˜äº‹ã§ã¯å‹¾é…ã®å°å‡ºã«ã¤ã„ã¦ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã®ã§ã€åˆ¥é€”è³‡æ–™ãªã©ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚&#8617; . | mdp4 = ChainMDP( [[1.0, 0.0], [0.6, 0.9], [0.9, 0.6], [1.0, 1.0]], [[0.0, 1.0], [0.0, 0.0], [0.0, 0.0], [0.9, 0.0]], ) width, height = mdp4.figure_shape() fig = plt.figure(&quot;MDP4-pi-vis&quot;, (width * 1.25, height)) mdp_ax = fig.add_axes([0.42, 0.0, 1.0, 1.0]) _ = mdp4.show(&quot;MDP4&quot;, ax=mdp_ax) pi_ax = fig.add_axes([0.0, 0.0, 0.4, 1.0], projection=&quot;3d&quot;) initial_pi = np.array([[0.0, 1.0], [0.5, 0.5], [0.5, 0.5], [1.0, 0.0]]) v_pi_sums = v_pi_sum_2dim(mdp4.r, mdp4.p, 0.95, 1e-4, initial_pi, (1, 2), 24) _ = plot_piv_heatmap(v_pi_sums, &quot;Î¸(1)&quot;, &quot;Î¸(2)&quot;, ax=pi_ax) print( f&quot;f(Î¸(1) = 0.0, Î¸(2) = 0.0): {v_pi_sums[0][0]} n&quot; f&quot;f(Î¸(1) = 0.5, Î¸(2) = 0.5): {v_pi_sums[12][12]} n&quot; f&quot;f(Î¸(1) = 1.0, Î¸(2) = 1.0): {v_pi_sums[24][24]}&quot; ) . . f(Î¸(1) = 0.0, Î¸(2) = 0.0): 74.25901721830479 f(Î¸(1) = 0.5, Î¸(2) = 0.5): 72.01388270994806 f(Î¸(1) = 1.0, Î¸(2) = 1.0): 70.6327625115528 . ä¸€ç•ªå³ã ã¨æ°¸é ã«0.9ãŒã‚‚ã‚‰ãˆã¦ã€ä¸€ç•ªå·¦ã ã¨1.0ãŒã‚‚ã‚‰ãˆã‚‹ã®ã§ã€ã‚ˆã‚Šæœ€é©æ–¹ç­–ã‚’è¦‹åˆ†ã‘ã‚‹ã®ãŒé›£ã—ãã†ãªæ„Ÿã˜ãŒã—ã¾ã™ã€‚ . ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ã¿ã‚‹ã¨ã€$f( theta)$ã¯å…ˆç¨‹ã¨ã¯é€†ã«è°·ã®ã‚ˆã†ãªå½¢ã«ãªã£ã¦ã„ã¦ã€å±±ç™»ã‚Šæ³•ã§è§£ã„ã¦ã‚‚ å¿…ãšã—ã‚‚å¤§åŸŸè§£ã«åæŸã—ãªãã†ã«è¦‹ãˆã¾ã™ã€‚ ã“ã‚Œã‚’ã‚‚ã£ã¨å°‚é–€çš„ãªè¨€è‘‰ã§è¨€ã†ã¨ã€$f(0.0) + f(1.0) &gt; 2 * f(0.5)$ã‚ˆã‚Šã“ã‚Œã¯å‡¹é–¢æ•°ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ã‚ã¾ã‚Šè©³ã—ãèª¬æ˜ã—ã¾ã›ã‚“ãŒã€å‡¹é–¢æ•°ã ã¨å±±ç™»ã‚Šæ³•ãŒå¤§åŸŸè§£ã«åæŸã™ã‚‹ãªã©å¬‰ã—ã„ç‚¹ãŒã‚ã‚‹ã®ã§ã€ ã“ã‚Œã¯æœ€é©åŒ–ã™ã‚‹ä¸Šã§å„ä»‹ãªç‰¹å¾´ã ã¨è¨€ãˆã¾ã™ã€‚ . ä»¥ä¸Šã‚ˆã‚Šã€æ–¹ç­–æœ€é©åŒ–ã§å•é¡Œã‚’è§£ãæ™‚ã¯$ sum_{s in mathcal{S}} V(s)$ãŒå‡¹é–¢æ•°ã‹ã©ã†ã‹ãŒã€ å•é¡Œã®é›£ã—ã•ã«å½±éŸ¿ã‚’ä¸ãˆãã†ã ã¨ã„ã†ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ . 2.A &#26041;&#31574;&#21453;&#24489;&#27861;&#12398;&#38627;&#12375;&#12373; . . Note: ã“ã®ç¯€ã¯ç‰¹ã«å†…å®¹ãŒãªã„ã®ã§ã‚¢ãƒšãƒ³ãƒ‡ã‚£ã‚¯ã‚¹æ‰±ã„ã«ãªã£ã¦ã„ã¾ã™ã€‚é£›ã°ã—ã¦ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚ . ã¨ã“ã‚ã§2.1ã§$V^*$ã‚’æ±‚ã‚ãŸã¨ãã«ä½¿ã£ãŸæ‰‹æ³•ã‚’ä¾¡å€¤åå¾©æ³•ã¨è¨€ã„ã¾ã™ã€‚ ã‚‚ã†ä¸€ã¤ã€æ–¹ç­–åå¾©æ³•ã¨ã„ã†æ‰‹æ³•ã§$V^*$ã‚’æ±‚ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ . $ pi^*$ãŒæº€ãŸã™ã¹ãæ€§è³ªã«ã¤ã„ã¦è€ƒãˆã¦ã¿ã¾ã™ã€‚ $ pi$ãŒæœ€é©ã§ã‚ã‚‹ã¨ãã€ $$ V^ pi(s) geq max_{a in mathcal{A}} r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s,a) V^ pi(s&#39;) $$ ãŒæˆã‚Šç«‹ã¡ã¾ã™ã€‚ ã“ã‚ŒãŒæˆã‚Šç«‹ãŸãªã„ã¨ã™ã‚‹ã¨ã€ $$ pi&#39;(s, a) = begin{cases} 1.0 &amp;( textrm{if}~a = textrm{argmax}_a r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s,a) V^ pi(s&#39;)) 0.0 &amp;( textrm{otherwise}) end{cases} $$ ã®æ–¹ãŒæ€§èƒ½ãŒè‰¯ããªã‚Šã€$ pi$ãŒæœ€é©ã§ã‚ã‚‹ã“ã¨ã¨çŸ›ç›¾ã—ã¾ã™ã€‚ . ã§ã¯ã€ã“ã®æ€§è³ªãŒæˆã‚Šç«‹ã¤ã¾ã§æ–¹ç­–ã‚’æ”¹å–„ã—ç¶šã‘ã‚‹ã¨ã„ã†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ã•ã£ãæ›¸ã„ãŸv_pié–¢æ•°ã‚’ä½¿ã£ã¦å®Ÿè£…ã§ãã¾ã™ã€‚ . MAX_ITER_PI: int = 10000 def policy_iteration( r: Array2, p: Array3, gamma: float, epsilon: float, ) -&gt; Tuple[Array1, Array2, int]: pi = np.zeros(p.shape[:2]) # |S| x |A| pi[:, 1] = 1.0 # æœ€åˆã®æ–¹ç­–ã¯æ±ºå®šçš„ãªã‚‰ãªã‚“ã§ã‚‚ã„ã„ãŒã€è¡Œå‹•1ã‚’é¸ã¶æ–¹ç­–ã«ã—ã¦ã¿ã‚‹ state_indices = np.arange(0, p.shape[0], dtype=np.uint) for n_iter in range(MAX_ITER_PI): v_pi_, _ = v_pi(r, p, pi, gamma, epsilon) q_pi = r + gamma * np.einsum(&quot;saS,S-&gt;sa&quot;, p, v_pi_) greedy_actions = np.argmax(q_pi, axis=1) pi_next = np.zeros_like(pi) pi_next[state_indices, greedy_actions] = 1.0 # pi == pi_next ãªã‚‰åæŸ if np.linalg.norm(pi - pi_next) &lt; 1.0: return v_pi_, pi_next, n_iter + 1 pi = pi_next raise RuntimeError(&quot;Policy Iteration did not converge &gt;_&lt;&quot;) v_star_mdp3_vi, _, n_iter = policy_iteration(mdp3.r, mdp3.p, 0.9, 1e-4) print(f&quot;åå¾©å›æ•°: {n_iter}&quot;) print(&quot; &quot;.join([f&quot;V({i}): {v:.3}&quot; for i, v in enumerate(v_star_mdp3_vi)])) . åå¾©å›æ•°: 2 V(0): 6.49 V(1): 7.21 V(2): 7.51 . ãªã‚“ã‹2å›åå¾©ã—ãŸã ã‘ã§æ±‚ã¾ã£ã¦ã—ã¾ã„ã¾ã—ãŸãŒ...ã€‚ ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æ–¹ç­–åå¾©æ³•ã¨å‘¼ã°ã‚Œã€ãªã‚“ã‚„ã‹ã‚“ã‚„ã§æœ€é©æ–¹ç­–ã«åæŸã™ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ ã§ã¯ã€ã“ã®åå¾©å›æ•°ã¯ã€ä½•ã«ã‚ˆã£ã¦æ±ºã¾ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ æ–¹ç­–ã®çµ„ã¿åˆã‚ã›ã¯$|A|^{|S|}$é€šã‚Šã‚ã‚Šã¾ã™ãŒã€ä¸Šã®å®Ÿé¨“ã ã¨ãšã£ã¨é€ŸãåæŸã—ã¦ã„ã‚‹ã®ã§ã€ã‚‚ã£ã¨ã„ã„ãƒã‚¦ãƒ³ãƒ‰ãŒã‚ã‚Šãã†ã«æ€ãˆã¾ã™ã€‚ ã—ã‹ã—ã€å®Ÿéš›ã®ã¨ã“ã‚æœ€æ‚ªã‚±ãƒ¼ã‚¹ã§ã¯æŒ‡æ•°æ™‚é–“ã‹ã‹ã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ ã“ã®è¨˜äº‹ã§ã¯ã€ã“ã®æ–¹ç­–åå¾©æ³•ãŒé›£ã—ããªã‚‹å ´åˆã«ã¤ã„ã¦ã‚‚è§£èª¬ã—ãŸã‹ã£ãŸã®ã§ã™ãŒã€ ç†è§£ã§ããªã‹ã£ãŸã®ã§ã€è«¦ã‚ã¾ã—ãŸã€‚ãƒ¾(ï½¡&gt;ï¹&lt;ï½¡)ï¾‰ . 2.B &#21442;&#32771;&#25991;&#29486;&#12394;&#12393; . On the Complexity of Solving Markov Decision Problems | Cournell CS 6789: Foundations of Reinforcement Learning | . å‚è€ƒæ–‡çŒ®ã§ã¯$ frac{1}{1 - gamma}$ã§åå¾©å›æ•°ã‚’æŠ‘ãˆã¦ã„ã‚‹ã˜ã‚ƒãªã„ã‹ã€è©±ãŒé•ã†ã˜ã‚ƒãªã„ã‹ã€ã¨ã„ã†æ°—ãŒä¸€è¦‹ã—ã¦ã—ã¾ã„ã¾ã™ã€‚ ã“ã‚Œã¯æœ‰åä¸ç­‰å¼$ log x leq x - 1$ ã‹ã‚‰ãªã‚“ã‚„ã‹ã‚“ã‚„ã§$ frac{1}{1 - gamma} geq - frac{1}{ log gamma}$ ã ã‹ã‚‰ã€œã¨ã„ã†æ„Ÿã˜ã§è€ƒãˆã‚Œã°ãªã‚“ã¨ã‹ãªã‚‹ã¨æ€ã„ã¾ã™ã€‚ ã“ã®ä¸ç­‰å¼ã¯$x=1$ã§ç­‰å·ãªã®ã§ã€ã‚ˆãä½¿ã†$ gamma=0.99$ã¨ã‹ã®è¨­å®šãªã‚‰ã‹ãªã‚Šå·®ã¯è¿‘ããªã‚Šã¾ã™ã€‚ . 3. &#12458;&#12531;&#12521;&#12452;&#12531;&#23398;&#32722;&#12392;&#12375;&#12390;&#12398;&#20596;&#38754;: &#12487;&#12540;&#12479;&#12434;&#21454;&#38598;&#12377;&#12427;&#12371;&#12392;&#12398;&#38627;&#12375;&#12373; . å‰ç¯€ã§ã¯ã€ç’°å¢ƒã®æƒ…å ±ã‹ã‚‰$V^*$ã‚„$ pi^*$ã‚’è¨ˆç®—ã—ã¦ã¿ã¾ã—ãŸã€‚ ã“ã®ç¯€ã§ã¯ã€æº€ã‚’æŒã—ã¦å­¦ç¿’ã®å•é¡Œã«å–ã‚Šçµ„ã‚“ã§ã„ã“ã†ã¨æ€ã„ã¾ã™ã€‚ . å­¦ç¿’ã™ã‚‹ç†ç”±ã¨ã—ã¦ã€ . $P$ã‚„$r$ãŒã‚ã‹ã‚‰ãªã„ ã‚ã‹ã‚‰ãªã„ãŒã€ãªãœã‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã“ã¨ã¯ã§ãã‚‹ | $P$ã‚‚$r$ã‚‚ã‚ã‹ã£ã¦ã„ã‚‹ãŒã€å•é¡ŒãŒå¤§ãã™ãã¦ $O(|S|^2 |A| frac{...}{1 - gamma})$ ã¨ã‹è§£ã‘ãªã•ãã† | ...ãªã©ãŒã‚ã‚‹ã¨æ€ã„ã¾ã™ãŒã€ã“ã“ã§ã¯æœ€ã‚‚ã‚†ã‚‹ã„è¨­å®šã‚’è€ƒãˆã¾ã™ã€‚ MDPã®ãƒ‘ãƒ©ãƒ¡ã‚¿ã¯ã‚ã‹ã‚‰ãªã„ãŒã€ã‚ã‚‹MDPã‚’ä½¿ã£ã¦è¡Œå‹•ã§ãã‚‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ã€‚ ã“ã‚Œã‚’ç’°å¢ƒã¨å‘¼ã¶ã“ã¨ã«ã—ã¾ã—ã‚‡ã†ã€‚ . ç’°å¢ƒã®ä¸­ã§è¡Œå‹•$a_t$ã‚’ã¨ã‚‹ã¨ã€ æ¬¡ã®çŠ¶æ…‹$s_{t+1}$ãŒ$P(s_{t+1}|s_t,a_t)$ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã¦ä¸ãˆã‚‰ã‚Œã¾ã™ã€‚ åŒæ™‚ã«å ±é…¬$r(s_t, a_t)$ã‚‚ä¸ãˆã‚‰ã‚Œã¾ã™ã€‚ ã“ã®ç’°å¢ƒã®ä¸­ã§ã€é©å½“ã«è¡Œå‹•ã—ã¦ã„ã‚‹ã¨ãªã‚“ã‚„ã‹ã‚“ã‚„ã§æƒ…å ±ãŒé›†ã¾ã£ã¦ãã¦å•é¡ŒãŒè§£ã‘ã‚‹ ã¨ã„ã†ã®ãŒå¼·åŒ–å­¦ç¿’ã®ä»•çµ„ã¿ã§ã™ã€‚ ã—ã‹ã—ã€æœ¬å½“ã«é©å½“ã«è¡Œå‹•ã—ã¦ã—ã¾ã£ã¦ã„ã„ã®ã§ã—ã‚‡ã†ã‹ã€‚ . 3.1 &#22577;&#37228;&#12394;&#12375;&#25506;&#26619;&#12398;&#21839;&#38988; . ã¨ã„ã†ã‚ã‘ã§ã€ã¨ã‚Šã‚ãˆãšåˆ¥ã«å­¦ç¿’ã—ãªãã¦ã„ã„ã®ã§ã€ç’°å¢ƒã‹ã‚‰æƒ…å ±ã‚’é›†ã‚ã¦ã“ã‚ˆã†ã€ã¨ã„ã†å•é¡Œã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . from matplotlib.figure import Figure from matplotlib.image import FigureImage class GridMDP: from matplotlib.colors import ListedColormap #: Up, Down, Left, Right ACTIONS = np.array([[-1, 0], [+1, 0], [0, -1], [0, +1]]) #: Symbols EMPTY, BLOCK, START, GOAL = range(4) DESCRIPTIONS = [&quot;Empty&quot;, &quot;Block&quot;, &quot;Start&quot;, &quot;Goal&quot;] #: Colormap for visualizing the map CM = ListedColormap([&quot;w&quot;, &quot;.75&quot;, &quot;xkcd:leaf green&quot;, &quot;xkcd:vermillion&quot;]) REWARD_COLORS = [&quot;xkcd:light royal blue&quot;, &quot;xkcd:vermillion&quot;] FIG_AREA = 28 # Returns PIL.Image def __download_agent_image(): from io import BytesIO from urllib import request from PIL import Image fd = BytesIO( request.urlopen( &quot;https://2.bp.blogspot.com/-ZwYKR5Zu28s/U6Qo2qAjsqI&quot; + &quot;/AAAAAAAAhkM/HkbDZEJwvPs/s400/omocha_robot.png&quot; ).read() ) return Image.open(fd) AGENT_IMAGE = __download_agent_image() def __init__( self, map_array: Sequence[Sequence[int]], reward_array: Optional[Sequence[Sequence[float]]] = None, action_noise: float = 0.1, horizon: Optional[int] = None, seed: int = 123456789, legend_loc: str = &quot;upper right&quot;, ) -&gt; None: def add_padding(seq: Sequence[Sequence[T]], value: T) -&gt; list: width = len(seq[0]) + 2 ret_list = [[value for _ in range(width)]] for col in seq: ret_list.append([value] + list(col) + [value]) ret_list.append([value for _ in range(width)]) return ret_list self.map_array = np.array(add_padding(map_array, 1), dtype=np.uint8) assert self.map_array.max() &lt;= 3 assert 0 &lt;= self.map_array.min() self.rows, self.cols = self.map_array.shape if reward_array is None: self.reward_array = np.zeros((self.rows, self.cols), np.float64) else: self.reward_array = np.array( add_padding(reward_array, 0.0), np.float64 ) self.action_noise = action_noise self.horizon = horizon self.start_positions = np.argwhere(self.map_array == self.START) if len(self.start_positions) == 0: raise ValueError(&quot;map_array needs at least one start posiiton&quot;) self.random_state = np.random.RandomState(seed) _ = self.reset() # Visualization stuffs self.legend_loc = legend_loc self.map_fig, self.map_ax, self.map_img = None, None, None self.agent_img, self.agent_fig_img = None, None def n_states(self) -&gt; int: return np.prod(self.map_array.shape) @staticmethod def n_actions() -&gt; int: return 4 def reset(self) -&gt; Array1: idx = self.random_state.randint(self.start_positions.shape[0]) self.state = self.start_positions[idx] self.n_steps = 0 return self.state.copy() def state_index(self, state: Array1) -&gt; int: y, x = state return y * self.map_array.shape[1] + x def _load_agent_img(self, fig_height: float) -&gt; None: from io import BytesIO from urllib import request fd = BytesIO(request.urlopen(self.ROBOT).read()) img = Image.open(fd) scale = fig_height / img.height self.agent_img = img.resize((int(img.width * scale), int(img.height * scale))) def _fig_inches(self) -&gt; Tuple[int, int]: prod = self.rows * self.cols scale = np.sqrt(self.FIG_AREA / prod) return self.cols * scale, self.rows * scale def _is_valid_state(self, *args) -&gt; bool: if len(args) == 2: y, x = args else: y, x = args[0] return 0 &lt;= y &lt; self.rows and 0 &lt;= x &lt; self.cols def _possible_actions(self) -&gt; Array1: possible_actions = [] for i, act in enumerate(self.ACTIONS): y, x = self.state + act if self._is_valid_state(y, x) and self.map_array[y, x] != self.BLOCK: possible_actions.append(i) return np.array(possible_actions) def _reward(self, next_state: Array1) -&gt; float: y, x = next_state return self.reward_array[y, x] def _is_terminal(self) -&gt; bool: if self.horizon is not None and self.n_steps &gt; self.horizon: return True y, x = self.state return self.map_array[y, x] == self.GOAL def step(self, action: int) -&gt; Tuple[Tuple[int, int], float, bool]: self.n_steps += 1 possible_actions = self._possible_actions() if self.random_state.random_sample() &lt; self.action_noise: action = self.random_state.choice(possible_actions) if action in possible_actions: next_state = self.state + self.ACTIONS[action] else: next_state = self.state.copy() reward = self._reward(next_state) self.state = next_state is_terminal = self._is_terminal() return next_state.copy(), reward, is_terminal def _draw_agent(self, fig: Figure) -&gt; FigureImage: unit = self.map_img.get_window_extent().y1 / self.rows y, x = self.state return fig.figimage( self.agent_img, unit * (x + 0.3), unit * (self.rows - 0.8 - y), ) def _draw_rewards(self) -&gt; None: for y in range(self.rows): for x in range(self.cols): rew = self.reward_array[y, x] if abs(rew) &lt; 1e-3: continue if self.map_array[y, x] == self.GOAL: color = &quot;w&quot; else: color = self.REWARD_COLORS[int(rew &gt;= 0)] self.map_ax.text( x + 0.1, y + 0.5, f&quot;{rew:+.2}&quot;, color=color, fontsize=12, ) def show(self, title: str = &quot;&quot;, explicit: bool = False) -&gt; Axes: if self.map_fig is None: self.map_fig = plt.figure(title or &quot;GridMDP&quot;, self._fig_inches()) ax = self.map_fig.add_axes([0, 0, 1, 1]) ax.set_aspect(&quot;equal&quot;) ax.set_xticks([]) ax.set_yticks([]) self.map_img = ax.imshow( self.map_array, cmap=self.CM, extent=(0, self.cols, self.rows, 0), vmin=0, vmax=4, alpha=0.6, ) for i in range(1, 4): if np.any(self.map_array == i): ax.plot([0.0], [0.0], color=self.CM(i), label=self.DESCRIPTIONS[i]) ax.legend(fontsize=12, loc=self.legend_loc) ax.text(0.1, 0.8, title or &quot;GridMDP&quot;, fontsize=16) self.map_ax = ax imw, imh = self.AGENT_IMAGE.width, self.AGENT_IMAGE.height scale = (self.map_img.get_window_extent().y1 / self.rows) / imh self.agent_img = self.AGENT_IMAGE.resize( (int(imw * scale), int(imh * scale)) ) if np.linalg.norm(self.reward_array) &gt; 1e-3: self._draw_rewards() if self.agent_fig_img is not None: self.agent_fig_img.remove() self.agent_fig_img = self._draw_agent(self.map_fig) if explicit: from IPython.display import display self.map_fig.canvas.draw() display(self.map_fig) return self.map_ax . . grid_mdp1 = GridMDP( [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 2, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], horizon=50, ) _ = grid_mdp1.show(&quot;GridMDP1&quot;) . GridMDPã¨é¡Œã•ã‚ŒãŸã“ã¡ã‚‰ãŒã€ä»Šå›ä½¿ç”¨ã™ã‚‹ã€Œç’°å¢ƒã€ã«ãªã‚Šã¾ã™ã€‚ ç’°å¢ƒã®ä¸­ã§è¡Œå‹•ã™ã‚‹ä¸»ä½“ã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨å‘¼ã³ã¾ã™ã€‚ ä»Šå›ã¯ã€ã„ã‚‰ã™ã¨ã‚„æ§˜ã®ãƒ­ãƒœãƒƒãƒˆã®ç”»åƒã‚’ä½¿ç”¨ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã€‚ å„ãƒã‚¹ç›®ã®ä¸­ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è¡Œå‹•ã¯ä¸Šä¸‹å·¦å³ã«ç§»å‹•ã®4ç¨®é¡ã®è¡Œå‹•ã‚’é¸æŠã§ãã¾ã™ã€‚ è¡Œå‹•ã¯æ™‚ã€…å¤±æ•—ã—ã¦ã€ä¸€æ§˜ãƒ©ãƒ³ãƒ€ãƒ ãªçŠ¶æ…‹é·ç§»ãŒç™ºç”Ÿã—ã¾ã™ã€‚ ã“ã“ã§ã€å‰ç« ã§ç”¨ã„ãªã‹ã£ãŸã„ãã¤ã‹ã®æ–°ã—ã„æ¦‚å¿µã‚’å°å…¥ã—ã¾ã™ã€‚ . ã€Œã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã€ã®å­˜åœ¨ ãƒ­ãƒœãƒƒãƒˆãã‚“ã¯ã€æ±ºã‚ã‚‰ã‚ŒãŸå ´æ‰€ã‹ã‚‰è¡Œå‹•ã‚’å§‹ã‚ãªãã¦ã¯ãªã‚Šã¾ã›ã‚“ã€‚ã“ã®å•é¡Œã§ã¯ã€åˆæœŸçŠ¶æ…‹ã¯ã‚ã‚‰ã‹ã˜ã‚æ±ºã¾ã‚‰ã‚ŒãŸã„ãã¤ã‹ã®ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã‹ã‚‰å‡ç­‰ã«é¸ã³ã¾ã™ã€‚ç†è«–çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯ã€åˆæœŸçŠ¶æ…‹åˆ†å¸ƒ$ mu: mathcal{S} rightarrow mathbb{R}$ã¨ã—ã¦è¡¨ç¾ã™ã‚Œã°ã„ã„ã§ã™ã€‚ | . | ã€Œçµ‚äº†åœ°ç‚¹ã€ã®å­˜åœ¨ ãƒ­ãƒœãƒƒãƒˆãã‚“ã¯ã€ã„ãã¤ã‹ã®æ±ºã‚ã‚‰ã‚ŒãŸå ´æ‰€ã«åˆ°é”ã—ãŸã‚‰å¼·åˆ¶çš„ã«ã‚¹ã‚¿ãƒ¼ãƒˆã¾ã§æˆ»ã•ã‚Œã¾ã™ã€‚ | . | ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ ã‚¹ã‚¿ãƒ¼ãƒˆã‹ã‚‰çµ‚äº†ã™ã‚‹ã¾ã§ã®ä¸€é€£ã®æµã‚Œã‚’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨å‘¼ã³ã¾ã™ã€‚ | . | ã€Œã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã•ï¼ˆãƒ›ãƒ©ã‚¤ã‚¾ãƒ³ï¼‰ã€ã®å­˜åœ¨ ä¸€å®šã®ã‚¿ãƒ¼ãƒ³ãŒçµŒéã—ãŸæ™‚ã€ãƒ­ãƒœãƒƒãƒˆãã‚“ã¯ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã¾ã§æˆ»ã•ã‚Œã¾ã™ã€‚å¼·åŒ–å­¦ç¿’ã§ã¯ã—ã°ã—ã°ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½•åº¦ã‚‚ãƒªã‚»ãƒƒãƒˆã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚ç†è«–ã‚’å®Ÿéš›ã«è¿‘ã¥ã‘ã‚‹ãŸã‚ã€MDPã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚‚ã“ã‚ŒãŒå°å…¥ã•ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ | . | . ã§ã¯ã•ã£ããã€é©å½“ã«è¡Œå‹•ã—ã¦ã‚‚ã‚‰ã„ã¾ã—ã‚‡ã†ã€‚ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã•ã›ã¦ã€è¨ªå•ã—ãŸå ´æ‰€ã«è‰²ã‚’ã¤ã‘ã¦ã„ãã¾ã™ã€‚ ãªãŠã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã•ã¯50ã¨ã—ã¾ã™ã€‚ . from abc import ABC, abstractmethod from typing import Callable class VisitationHeatmap: def __init__( self, map_shape: Tuple[int, int], figsize: Tuple[float, float], ax: Optional[Axes] = None, max_visit: int = 1000, title: str = &quot;&quot;, ) -&gt; None: from matplotlib import colors as mc from mpl_toolkits.axes_grid1 import make_axes_locatable self.counter = np.zeros(map_shape, np.int64) self.title = title self.fig = plt.figure(self.title, figsize, facecolor=&quot;w&quot;) self.ax = self.fig.add_axes([0, 0, 1, 1]) self.ax.set_aspect(&quot;equal&quot;) self.ax.set_xticks([]) self.ax.set_yticks([]) r, g, b = mc.to_rgb(&quot;xkcd:fuchsia&quot;) cdict = { &quot;red&quot;: [(0.0, r, r), (1.0, r, r)], &quot;green&quot;: [(0.0, g, g), (1.0, g, g)], &quot;blue&quot;: [(0.0, b, b), (1.0, b, b)], &quot;alpha&quot;: [(0.0, 0.0, 0.0), (1.0, 1.0, 1.0)], } self.img = self.ax.imshow( np.zeros(map_shape), cmap=mc.LinearSegmentedColormap(&quot;visitation&quot;, cdict), extent=(0, map_shape[1], map_shape[0], 0), vmin=0, vmax=max_visit, ) divider = make_axes_locatable(self.ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;4%&quot;, pad=0.1) self.fig.colorbar(self.img, cax=cax, orientation=&quot;vertical&quot;) cax.set_ylabel(&quot;Num. Visitation&quot;, rotation=0, position=(1.0, 1.1), fontsize=14) self._update_text() self.agent = None def _update_text(self) -&gt; None: self.text = self.ax.text( 0.1, -0.5, f&quot;{self.title} After {self.counter.sum()} steps&quot;, fontsize=16, ) def _draw_agent(self, draw: Callable[[Figure], FigureImage]) -&gt; None: if self.agent is not None: self.agent.remove() self.agent = draw(self.fig) def visit(self, state: Array1) -&gt; int: y, x = state res = self.counter[y, x] self.counter[y, x] += 1 self.img.set_data(self.counter) self.text.remove() self._update_text() return res def show(self) -&gt; None: from IPython.display import display display(self.fig) def do_nothing( _state: int, _action: int, _next_state: int, _reward: float, _is_terminal: bool, ) -&gt; None: return def simulation( mdp: GridMDP, n: int, act: Callable[[int], int], learn: Callable[[int, int, int, float, bool], None] = do_nothing, max_visit: Optional[int] = None, vis_freq: Optional[int] = None, vis_last: bool = False, title: str = &quot;&quot;, ) -&gt; None: visitation = VisitationHeatmap( mdp.map_array.shape, mdp._fig_inches(), max_visit=max_visit or n // 10, title=title, ) state = mdp.reset() visitation.visit(state) vis_interval = n + 1 if vis_freq is None else n // vis_freq for i in range(n): if (i + 1) % vis_interval == 0 and (vis_last or i &lt; n - 1): visitation._draw_agent(mdp._draw_agent) visitation.show() action = act(mdp.state_index(state)) next_state, reward, terminal = mdp.step(action) visitation.visit(next_state) learn( mdp.state_index(state), action, mdp.state_index(next_state), reward, terminal, ) if terminal: state = mdp.reset() else: state = next_state visitation._draw_agent(mdp._draw_agent) simulation(grid_mdp1, 1000, lambda _: np.random.randint(4), vis_freq=2) . . ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã•ã›ãŸã ã‘ã§ã™ãŒã€ãã‚Œãªã‚Šã«ã¾ã‚“ã¹ã‚“ãªãè‰²ãŒå¡—ã‚‰ã‚Œã¦ã„ã¦ã€ã¾ã‚ã¾ã‚ã„ã„ã®ã§ã¯ãªã„ã‹ã€ã¨ã„ã†æ°—ãŒã—ã¾ã™ã€‚ ã—ã‹ã—ã€ã‚‚ã£ã¨åºƒã„ç’°å¢ƒã§ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ã€‚ . grid_mdp2_map = [[0] * 15 for _ in range(15)] grid_mdp2_map[7][7] = 2 grid_mdp2 = GridMDP(grid_mdp2_map, horizon=50) _ = grid_mdp2.show() random_state = np.random.RandomState(1) simulation( grid_mdp2, 5000, lambda _: random_state.randint(4), max_visit=100, title=&quot;Random Exploration&quot;, vis_freq=None, ) . . ãªã‚“ã‹é§„ç›®ã£ã½ã„æ„Ÿã˜ã§ã™ã­ã€‚ å ´æ‰€ã«ã‚ˆã£ã¦ã¯å…¨ãè‰²ãŒã¤ã„ã¦ã„ã¾ã›ã‚“ã€‚ ç’°å¢ƒãŒåºƒã„ã¨ã€ãƒ©ãƒ³ãƒ€ãƒ ã«æ­©ãå›ã‚‹ã®ã§ã¯ã€åŠ¹ç‡ã‚ˆãæƒ…å ±ã‚’é›†ã‚ã¦ã“ã‚Œãªã„ã‚ˆã†ã§ã™ã€‚ å…·ä½“çš„ã«ã©ã®ãã‚‰ã„é›£ã—ã„ã®ã‹ã¨è¨€ã†ã¨ã€å¹³å‡ä¸€å›è¨ªå•ã™ã‚‹ã®ã«ã‹ã‹ã‚‹æ™‚é–“ãŒã€ã ã„ãŸã„ . ä¸€æ–¹é€šè¡Œã®ç›´ç·š: $O(|S|)$ | äºŒæ¬¡å…ƒãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯: $O(|S|^2)$? (å‚è€ƒ: planeä¸Šã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ï¼‰ | æœ€æ‚ªã‚±ãƒ¼ã‚¹: $O(2^{|S|})$ | . ãã‚‰ã„ã«ãªã‚Šã¾ã™ã€‚ ä¸€æ–¹é€šè¡Œãªã®ã¯ãªã‚“ã¨ãªãã‚ã‹ã‚Šã¾ã™ã­ã€‚ ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®å ´åˆã€åŒã˜å ´æ‰€ã‚’è¡Œã£ãŸã‚ŠããŸã‚Šã§ãã‚‹ã®ã§ã€ãã®ã¶ã‚“æ™‚é–“ãŒã‹ã‹ã£ã¦ã—ã¾ã„ã¾ã™ã€‚ æœ€æ‚ªã‚±ãƒ¼ã‚¹ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ§‹æˆã™ã‚Œã°ã„ã„ã§ã™ã€‚ . ã“ã®ç’°å¢ƒã§çŠ¶æ…‹$0$ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã™ã‚‹ã¨ã€å³ç«¯ã«ãŸã©ã‚Šç€ãã¾ã§ã«å¹³å‡$2^5$ãã‚‰ã„ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒã‹ã‹ã‚Šã¾ã™ã€‚ ãã‚“ãªã‚“ã‚ã‚Šã‹ã‚ˆ...ã£ã¦æ„Ÿã˜ã§ã™ãŒã€‚ . ã“ã®çµæœã‹ã‚‰ã€æœ€æ‚ªã®å ´åˆã ã¨æŒ‡æ•°æ™‚é–“ã‹ã‹ã‚‹ã‹ã‚‰è³¢ããƒ‡ãƒ¼ã‚¿åé›†ã—ãªã„ã¨ã„ã‘ãªã„ã‚ˆã­ã€ æ€ã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ ãã®ä¸€æ–¹ã§ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®ã‚ˆã†ã«é·ç§»ã®å¯¾ç§°æ€§ãŒã‚ã‚‹ç’°å¢ƒãªã‚‰ã€ ãƒ©ãƒ³ãƒ€ãƒ ã«è¡Œå‹•ã—ã¦ã‚‚ãã‚“ãªã«æ‚ªããªã„ã‚“ã˜ã‚ƒãªã„ã‹ãªã€ã¨ã‚‚æ€ãˆã¾ã™ã€‚ . ã•ã¦ãã®è©±ã¯ä¸€æ—¦ãŠã„ã¦ãŠã„ã¦ã€ã‚‚ã£ã¨åŠ¹ç‡ã‚ˆããƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã‚‹æ–¹æ³•ã‚’è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . è¨ªå•ã—ãŸå ´æ‰€ã‚’è¦šãˆã¦ãŠã„ã¦ã€è¨ªå•ã—ã¦ã„ãªã„å ´æ‰€ã‚’å„ªå…ˆã—ã¦æ¢æŸ»ã™ã‚‹ | çŠ¶æ…‹ã¨çŠ¶æ…‹ã®é–“ã«è·é›¢ãŒå®šç¾©ã§ãã‚‹ã¨ä»®å®šã—ã¦ã€é ãã«è¡Œãã‚ˆã†ã«ã™ã‚‹ | ç’°å¢ƒãŒãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“ã ã¨ä»®å®šã—ã¦SLAMã§è‡ªå·±ä½ç½®æ¨å®šã™ã‚‹ | ãªã©ã€è‰²ã€…ãªæ–¹æ³•ãŒè€ƒãˆã‚‰ã‚Œã‚‹ã¨æ€ã„ã¾ã™ãŒã€ã“ã“ã§ã¯1ã®æ–¹æ³•ã‚’ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è€ƒãˆã¾ã™ã€‚ . é©å½“ãªæ–¹ç­–$ pi_0$ã‹ã‚‰é–‹å§‹ã™ã‚‹ | çŠ¶æ…‹è¡Œå‹•è¨ªå•å›æ•°$n(s, a)$ã€çŠ¶æ…‹è¡Œå‹•æ¬¡çŠ¶æ…‹è¨ªå•å›æ•°$n(s, a, s&#39;)$ã‚’è¨˜éŒ²ã—ã¦ãŠã ãŸã ã—ã€åˆæœŸå€¤ã¯$n_0(s, a) = 1.0, n_0(s, a, s) = frac{1}{|S|}$ã¨ã™ã‚‹(0é™¤ç®—é˜²æ­¢ã®ãŸã‚) | . | ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçµ‚ã‚ã£ãŸã¨ãã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ–¹ç­–ã‚’æ›´æ–°ã™ã‚‹ çŠ¶æ…‹é·ç§»é–¢æ•°ã®æ¨å®šå€¤$ hat{P}(s&#39;|s, a) = frac{n(s, a, s&#39;)}{n(s, a}$ã€ç–‘ä¼¼å ±é…¬$r_k(s, a)= frac{1}{n(s, a)}$ã€é©å½“ãª$ gamma$ã‹ã‚‰æˆã‚‹MDP$ mathcal{M}_k$ã‚’è§£ã | $ mathcal{M}_k$ã®æœ€é©ä¾¡å€¤é–¢æ•°$V^*_k,Q^*_k$ã‹ã‚‰ä»¥ä¸‹ã®ã‚ˆã†ã«æ–¹ç­–$pi_{k+1}$ã‚’æ§‹æˆã™ã‚‹ $V^*_k(s) &lt; frac{1}{|S|} sum_{s&#39; in mathcal{S}}V^*_k(s&#39;)$ ãªã‚‰ $ pi_{k+1}(s)$ã¯$Q^*_k$ã«åŸºã¥ãè²ªæ¬²è¡Œå‹• | ãã‚Œä»¥å¤–ã®å ´åˆã€$ pi_{k+1}(s)$ã¯ä¸€æ§˜ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œå‹•ã‚’ã¨ã‚‹ (=æ–¹ç­–ã‚’ç·©å’Œã™ã‚‹) | . | | ç–‘ä¼¼å ±é…¬$r_k= frac{1}{n(s, a)}$ã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã™ã‚‹ã®ãŒã€æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚ ã“ã®å€¤ã¯ã€ä¸€åº¦ã‚‚çŠ¶æ…‹è¡Œå‹•ãƒšã‚¢$(s,a)$ã‚’çµŒé¨“ã—ã¦ã„ãªã„ãªã‚‰$1$ã€ä¸€åº¦çµŒé¨“ã—ãŸã‚‰$1/2$ã€2å›çµŒé¨“ã—ãŸã‚‰$1/3$ã®ã‚ˆã†ã«æ¸›è¡°ã—ã¾ã™ã€‚ ã“ã‚Œã‚’å ±é…¬ã¨ã™ã‚‹MDPã‚’è§£ãã“ã¨ã§ã€ã‚ã¾ã‚ŠçµŒé¨“ã—ã¦ã„ãªã„çŠ¶æ…‹è¡Œå‹•ãƒšã‚¢ã‚’ã¨ã‚ã†ã¨ã™ã‚‹æ–¹ç­–ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚ å®Œå…¨ãªè²ªæ¬²æ–¹ç­–ã§ã¯ãªãç·©å’Œã‚’ã„ã‚Œã¦ã„ã‚‹ã®ã¯ã€é«˜ã„å ±é…¬ã®çŠ¶æ…‹ã‚’ãƒ«ãƒ¼ãƒ—ã—ãªã„ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã§ã™ã€‚ ã§ã¯ã€ã‚„ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . class RewardFreeExplore: def __init__( self, n_states: int, n_actions: int, seed: int = 1, gamma: float = 0.95, ) -&gt; None: self.sa_count = np.ones((n_states, n_actions)) self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states self.pi = np.ones((n_states, n_actions)) / n_actions self.random_state = np.random.RandomState(seed) self.n_states, self.n_actions = n_states, n_actions self.n_updates = 0 self.gamma = gamma self.pi_indices = np.arange(n_states, dtype=np.uint32) def learn( self, state: int, action: int, next_state: int, reward: float, is_terminal: bool, ) -&gt; None: # è¨ªå•è¨˜éŒ²ã‚’æ›´æ–°ã™ã‚‹ self.sa_count[state, action] += 1 if is_terminal: # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçµ‚ã‚ã£ãŸã‚‰ã€Value Iterationã‚’è§£ã„ã¦æ–¹ç­–ã‚’æ›´æ–°ã™ã‚‹ r = 1.0 / self.sa_count p = self.sas_count / np.expand_dims(self.sa_count, axis=-1) v, _n_iter = value_iteration(r, p, self.gamma, 1e-2) v_is_larger_than_mean = v &gt; v.mean() q = r + self.gamma * np.einsum(&quot;saS,S-&gt;sa&quot;, p, v) self.pi.fill(0.0) for state in range(self.n_states): # VãŒå¤§ãã„å ´æ‰€ã§ã¯æ–¹ç­–ã‚’ç·©å’Œã™ã‚‹ if v_is_larger_than_mean[state]: self.pi[state] = 1.0 / self.n_actions # ãã†ã§ãªã„å ´åˆã¯è²ªæ¬² else: self.pi[state][q[state].argmax()] = 1.0 self.n_updates += 1 else: self.sas_count[state, action, next_state] += 1 def act(self, state: int) -&gt; int: return self.random_state.choice(self.n_actions, p=self.pi[state]) agent = RewardFreeExplore(grid_mdp2.n_states(), grid_mdp2.n_actions()) simulation( grid_mdp2, 5000, agent.act, learn=agent.learn, max_visit=100, title=&quot;Strategic Exploration&quot;, vis_freq=None, ) . . ã•ã£ãã‚ˆã‚Šã‚‚æº€éãªãã€è‰²ã€…ãªçŠ¶æ…‹ã‚’è¨ªå•ã—ã¦ãã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã­ã€‚ . 3.2 &#22577;&#37228;&#12354;&#12426;&#25506;&#26619; . æ¬¡ã¯ã€ç’°å¢ƒã‹ã‚‰å ±é…¬ãŒä¸ãˆã‚‰ã‚Œã‚‹ã®ã§ã€ãªã‚‹ã¹ãæ—©ãå­¦ç¿’ã‚’çµ‚ã‚ã‚‰ã›ãŸã„ã€ã¨ã„ã†å•é¡Œã‚’è€ƒãˆã¾ã™ã€‚ è¨ªå•ã—ã¦ã„ãªã„å ´æ‰€ã«ç©æ¥µçš„ã«ã„ã‘ã°ã„ã„ã€ã¨ã„ã†æ–¹é‡ã¯ã•ã£ãã¨å¤‰ã‚ã‚Šã¾ã›ã‚“ã€‚ ä¸€æ–¹ã§ã€ã‚ã¾ã‚Šã«å ±é…¬ãŒã‚‚ã‚‰ãˆãªã•ãã†ãªçŠ¶æ…‹ã¯ã¨ã£ã¨ã¨è«¦ã‚ã‚‹ã“ã¨ã‚’ã—ãªãã¦ã¯ã„ã‘ãªã„ç‚¹ãŒç•°ãªã£ã¦ã„ã¾ã™ã€‚ . ä¾‹ãˆã°ã€å ±é…¬ã®æ¨å®šå€¤ã‚’$ hat{r}(s, a)$ã¨ã™ã‚‹ã¨ãã€å…ˆã»ã©ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç–‘ä¼¼å ±é…¬ã‚’ $r_k(s, a)= hat{r}(s, a)+ frac{ beta}{ sqrt{n(s, a)}}$ã¨ã™ã‚Œã°ã„ã„ã§ã™ã€‚ ã“ã‚Œã‚’ã€å˜ã«$r_k(s, a)= hat{r}(s, a)$ã¨ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ (Approximate Value Iteration)ã¨æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ ã“ã¡ã‚‰ã¯ã€ç¢ºç‡$ epsilon$ã§ä¸€æ§˜åˆ†å¸ƒã‹ã‚‰è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€$1- epsilon$ã§$Q^*_k$ãŒä¸€ç•ªå¤§ãã„è¡Œå‹•ã‚’é¸æŠã™ã‚‹ã¨ã„ã†è¡Œå‹•æ–¹ç­–ã‚’ä½¿ã£ã¦ã¿ã¾ã—ã‚‡ã†ï¼ˆ$ epsilon$-Greedyã¨è¨€ã„ã¾ã™ï¼‰ã€‚ ä»Šå›ã¯$ epsilon=0.9 rightarrow 0.4$ã¨ã—ã¾ã™ã€‚ . grid_mdp3 = GridMDP( [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0]], horizon=100, legend_loc=&quot;lower right&quot; ) _ = grid_mdp3.show(&quot;GridMDP3&quot;) . . ã¾ãšã€ã“ã¡ã‚‰ã®ç’°å¢ƒã§å®Ÿé¨“ã—ã¦ã¿ã¾ã™ã€‚ç´ ç›´ã«$0.1$ã®å ±é…¬â†’$9.0$ã®å ±é…¬ã‚’ç›®æŒ‡ã›ã°ã„ã„æ„Ÿã˜ã§ã™ã€‚ ã¾ãŸã€$ gamma=0.99$ã¨ã—ã¾ã™ã€‚ . class EpsgApproxVI: def __init__( self, n_states: int, n_actions: int, seed: int = 1, gamma: float = 0.99, epsilon: float = 0.9, epsilon_delta: float = 0.0001, ) -&gt; None: self.sa_count = np.ones((n_states, n_actions)) self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states self.r_sum = np.zeros((n_states, n_actions)) self.pi = np.ones((n_states, n_actions)) / n_actions self.random_state = np.random.RandomState(seed) self.n_states, self.n_actions = n_states, n_actions self.n_updates = 0 self.gamma = gamma self.epsilon = epsilon self.epsilon_delta = epsilon_delta self.pi_indices = np.arange(n_states, dtype=np.uint32) def learn( self, state: int, action: int, next_state: int, reward: float, is_terminal: bool, ) -&gt; None: self.sa_count[state, action] += 1 self.r_sum[state, action] += reward if is_terminal: r = self.r_sum / self.sa_count p = self.sas_count / np.expand_dims(self.sa_count, axis=-1) v, _n_iter = value_iteration(r, p, self.gamma, 1e-2) q = r + self.gamma * np.einsum(&quot;saS,S-&gt;sa&quot;, p, v) self.pi.fill(0) for state in range(self.n_states): self.pi[state][q[state].argmax()] = 1.0 self.n_updates += 1 else: self.sas_count[state, action, next_state] += 1 def act(self, state: int) -&gt; int: if self.random_state.rand() &lt; self.epsilon: self.epsilon -= self.epsilon_delta return self.random_state.choice(self.n_actions) else: return self.random_state.choice(self.n_actions, p=self.pi[state]) class MBIB_EB: def __init__( self, n_states: int, n_actions: int, seed: int = 1, gamma: float = 0.99, beta: float = 0.1, ) -&gt; None: self.sa_count = np.ones((n_states, n_actions)) self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states self.r_sum = np.zeros((n_states, n_actions)) self.pi = np.ones((n_states, n_actions)) / n_actions self.random_state = np.random.RandomState(seed) self.n_states, self.n_actions = n_states, n_actions self.n_updates = 0 self.gamma = gamma self.beta = beta self.pi_indices = np.arange(n_states, dtype=np.uint32) def learn( self, state: int, action: int, next_state: int, reward: float, is_terminal: bool, ) -&gt; None: self.sa_count[state, action] += 1 self.r_sum[state, action] += reward if is_terminal: r = self.r_sum / self.sa_count + self.beta / np.sqrt(self.sa_count) p = self.sas_count / np.expand_dims(self.sa_count, axis=-1) v, _n_iter = value_iteration(r, p, self.gamma, 1e-2) v_is_larger_than_mean = v &gt; v.mean() q = r + self.gamma * np.einsum(&quot;saS,S-&gt;sa&quot;, p, v) self.pi.fill(0.0) for state in range(self.n_states): if v_is_larger_than_mean[state]: self.pi[state] = 1.0 / self.n_actions else: self.pi[state][q[state].argmax()] = 1.0 self.n_updates += 1 else: self.sas_count[state, action, next_state] += 1 def act(self, state: int) -&gt; int: return self.random_state.choice(self.n_actions, p=self.pi[state]) epsg_vi = EpsgApproxVI(grid_mdp3.n_states(), grid_mdp3.n_actions()) simulation( grid_mdp3, 10000, epsg_vi.act, learn=epsg_vi.learn, max_visit=100, title=&quot;Îµ-Greedy&quot;, vis_freq=None, ) mbib_eb = MBIB_EB(grid_mdp3.n_states(), grid_mdp3.n_actions()) simulation( grid_mdp3, 10000, mbib_eb.act, learn=mbib_eb.learn, max_visit=100, title=&quot;Startegic Exploration&quot;, vis_freq=None, ) . . ä¸¡æ–¹ã¨ã‚‚ã€ã„ã„æ„Ÿã˜ã«æ¢æŸ»ã—ã¦ãã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚ $ epsilon$-Greedyã®æ–¹ãŒã€$R_t=9.0$ãŒã‚‚ã‚‰ãˆã‚‹ã‚´ãƒ¼ãƒ«ã®å‘¨è¾ºã‚’å¤šãæ¢æŸ»ã—ã¦ã„ã¦ã€ è‰¯ã•ãã†ã«è¦‹ãˆã¾ã™ã€‚ ä¸€æ–¹ã§ã€ã‚‚ã†å°‘ã—æ„åœ°æ‚ªãªç’°å¢ƒã®å ´åˆã¯ã©ã†ã§ã—ã‚‡ã†ã‹ï¼Ÿ . grid_mdp4 = GridMDP( [[0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 3], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 9.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], horizon=100, legend_loc=&quot;lower right&quot; ) _ = grid_mdp4.show(&quot;GridMDP3&quot;) . . ã“ã®ç’°å¢ƒã§ã¯ã€$+5.0$ã¨ã‹ã„ã†é‚ªé­”ãã•ã„å ±é…¬ãŒã‚ã‚Šã¾ã™ã€‚ ã—ã‹ã‚‚ã“ã“ã¯ã‚´ãƒ¼ãƒ«ãªã®ã§ã€ã“ã“ã«è¡Œãã¨ã¾ãŸãƒªã‚»ãƒƒãƒˆã—ã¦ã‚„ã‚Šç›´ã—ã§ã™ã€‚ ã“ã“ã‚’ç›®æŒ‡ã™ã‚ˆã†ã«å­¦ç¿’ã—ã¦ã—ã¾ã†ã¨ã€ãªã‹ãªã‹$+9.0$ã®æ–¹ã«è¡Œãã®ã¯å³ã—ãã†ã«è¦‹ãˆã¾ã™ã€‚ å®Ÿé¨“ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ . epsg_vi = EpsgApproxVI(grid_mdp4.n_states(), grid_mdp4.n_actions()) simulation( grid_mdp4, 10000, epsg_vi.act, learn=epsg_vi.learn, max_visit=100, title=&quot;Îµ-Greedy&quot;, vis_freq=None, ) mbib_eb = MBIB_EB(grid_mdp4.n_states(), grid_mdp4.n_actions()) simulation( grid_mdp4, 10000, mbib_eb.act, learn=mbib_eb.learn, max_visit=100, title=&quot;Startegic Exploration&quot;, vis_freq=None, ) . . äºˆæƒ³é€šã‚Šã€$ epsilon$-Greedyã®æ–¹ã¯å³ä¸Šã°ã‹ã‚Šè¡Œã£ã¦ã—ã¾ã£ã¦ã‚¤ãƒã‚¤ãƒãªæ„Ÿã˜ã«ãªã‚Šã¾ã—ãŸã€‚ . ä»¥ä¸Šã®çµæœã‹ã‚‰ã€ . é‚ªé­”ãŒãªãé·ç§»é–¢æ•°ãŒå¯¾ç§°ãªçŠ¶æ…‹ç©ºé–“ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã®ã‚ˆã†ã«è€ƒãˆã‚‰ã‚Œã‚‹ã‚‚ã®ï¼‰ã§ã¯ã€ã‚ã‚Šã‚ã„ç°¡å˜ã«ãƒ‡ãƒ¼ã‚¿åé›†ãŒã§ãã‚‹ | é‚ªé­”ãªå ±é…¬ãŒãªã„ç’°å¢ƒã§ã¯ã€ã‚ã‚Šã‚ã„ç°¡å˜ã«ãƒ‡ãƒ¼ã‚¿åé›†ãŒã§ãã‚‹ | . ã¨ã„ã†2ç‚¹ãŒè¨€ãˆã‚‹ã‹ã¨æ€ã„ã¾ã™ã€‚ ãƒ¯ãƒ¼ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’è€ƒãˆã‚‹ã¨æ¢æŸ»ãŒé›£ã—ã„ã®ã‚‚äº‹å®Ÿã§ã™ãŒã€å®Ÿç”¨ä¸Šã¯é›£ã—ã„ã‚±ãƒ¼ã‚¹ã‚’è€ƒãˆã‚‹ã‚ˆã‚Šé‚ªé­”ãªå ±é…¬ã‚’æ’é™¤ã™ã‚‹ ã“ã¨ã‚’è€ƒãˆã‚‹ã®ãŒé‡è¦ã§ã™ã€‚ . 3.A &#21442;&#32771;&#25991;&#29486;&#12394;&#12393; . On the Sample Complexity of Reinforcement Learning | Reward-Free Exploration for Reinforcement Learning | Sample Complexity Bounds of Exploration | An analysis of model-based Interval Estimation for Markov Decision Processes | 3.1ã§ç´¹ä»‹ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ä¸€å¿œ2.ã®æ–‡çŒ®ã‚’å‚è€ƒã«ã—ã¦ã„ã¾ã™ãŒã€åƒ•ãŒã•ã£ãé©å½“ã«è€ƒãˆãŸ(ã¯ï¼Ÿ)ã‚‚ã®ã§ã™ã€‚ ç†è«–ä¿è¨¼ãŒã‚ã‚‹ã‹ã¯ã‚ã‚„ã—ã„ã¨æ€ã„ã¾ã™ã€‚ 3.2ã®ã‚„ã¤ã¯MBIB-EB(4.)ã«ä¼¼ã¦ã„ã¾ã™ãŒã€æ–¹ç­–ã®ç·©å’ŒãŒå…¥ã£ã¦ã„ã‚‹ç‚¹ãŒé•ã„ã¾ã™ã€‚ ç·©å’Œã‚‚åƒ•ãŒé©å½“ã«è€ƒãˆãŸã‚‚ã®ãªã®ã§ã™ãŒã€å…¥ã‚ŒãŸæ–¹ãŒæ€§èƒ½ãŒè‰¯ã‹ã£ãŸã®ã§å…¥ã‚Œã¦ã¿ã¾ã—ãŸã€‚ è‰¯ã„å­ã®çš†ã•ã‚“ã¯çœŸä¼¼ã—ãªã„ã§ãã ã•ã„ã€‚ . 4. &#32080;&#35542; . å¼·åŒ–å­¦ç¿’ã§é‡è¦ãªã®ã¯1ã«å ±é…¬ã€2ã«å ±é…¬ã€3ã€4ãŒãªãã¦5ã«$ gamma$(ã‚‚ã—ãã¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã•$H$)ã§ã™ã€‚ . æœ€é©è§£ã¨2ç•ªç›®ã«ã„ã„è§£ã®å·®ãŒå¤§ãããªã‚‹ã‚ˆã†ã«å•é¡Œã‚’è¨­è¨ˆã—ã¾ã—ã‚‡ã† | $ gamma$ã¯ã§ãã‚‹é™ã‚Šå°ã•ãã—ã¾ã—ã‚‡ã† | . ã“ã‚Œã‚‰ã®ç‚¹ã«ã¤ã„ã¦ç´å¾—ã„ãŸã ã‘ã‚Œã°ã€ã“ã®è¨˜äº‹ã¯æˆåŠŸã¨è¨€ãˆã‚‹ã®ã§ã¯ãªã„ã‹ãªã€ã¨æ€ã„ã¾ã™ã€‚ æœ¬å½“ã¯ã‚‚ã†å°‘ã—æ›¸ããŸã„ã“ã¨ã‚‚ã‚ã‚‹ã®ã§ã™ãŒ...ã€‚ . ä½•ã‹è³ªå•ãƒ»èª¤ã‚Šã®æŒ‡æ‘˜ãªã©ã‚ã‚Œã°ã‚³ãƒ¡ãƒ³ãƒˆæ¬„ã‹ã‚‰ãŠé¡˜ã„ã—ã¾ã™ã€‚ . 5. &#35613;&#36766; . å¼·åŒ–å­¦ç¿’è‹¦æ‰‹ã®ä¼šã‚‚ãã‚‚ãä¼šä¸»å‚¬ã®å“å·ã•ã‚“ãŠã‚ˆã³ã€å‚åŠ è€…ã®æ–¹ã€…ã¨ã®æœ‰æ„ç¾©ãªè­°è«–ã«æ„Ÿè¬ã—ã¾ã™ã€‚ .",
            "url": "https://kngwyu.github.io/rlog/ja/2020/12/22/understanding-what-makes-rl-difficult.html",
            "relUrl": "/ja/2020/12/22/understanding-what-makes-rl-difficult.html",
            "date": " â€¢ Dec 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Series of blog posts mainly about reinforcement learning (RL). See this for my profile. .",
          "url": "https://kngwyu.github.io/rlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://kngwyu.github.io/rlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}