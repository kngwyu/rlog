{
  
    
        "post0": {
            "title": "Exercise 5.12 Racetrack from the Reinforcement Learning textbook",
            "content": "Here I demonstrate the execise 5.12 of the textbook Reinforcement Learning: An Introduction by Richard Sutton and Andrew G. Barto, using both of planning method and Monte Carlo method. Basic knowledge of Python (&gt;= 3.7) and NumPy are assumed. Some konwledge of matplotlib and Python typing library also helps. . Contact: yuji.kanagawa@oist.jp . Modeling the problem in code . Let&#39;s start from writing the problem in code. What are important in this phase? Here, I&#39;d like to emphasize the importantness of looking back at the definition of the environment. I.e., in reinforcement learning (RL), environments are modelled by Markov decision process (MDP), consisting of states, actions, transition function, and reward function. So first let&#39;s check the definition of states and actions in the problem statement. It&#39;s (somehow) not very straightforward, but we can find . In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. . So, a state consists of position and velocity of the car (a.k.a. agent). What about actions? . The actions are increments to the velocity components. Each may be changed by +1, −1, or 0 in each step, for a total of nine ($3 times 3$) actions. . So there are 9 actions for each direction (↓↙←↖↑↗→↘ or no acceleration). Here, we can also notice that the total number of states is given by $ textrm{Num. positions} times textrm{Num. choices of velocity}$. And the texbook also says . Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. . So there are 24 possible velocity at the non-starting positions: . import itertools list(itertools.product(range(5), range(5)))[1:] . [(0, 1), (0, 2), (0, 3), (0, 4), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (3, 0), (3, 1), (3, 2), (3, 3), (3, 4), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4)] . Again, number of states is given by (roughly) $24 times textrm{Num. positions}$ and number of actions is $9$. Sounds not very easy problem with many positions. . So, let&#39;s start the coding from representing the state and actions. There are multiple ways, but I prefer to NumPy array for representing everything. . Let&#39;s consider a ASCII representation of the map (or track) like this: . SMALL_TRACK = &quot;&quot;&quot; ### F ## F ## F # ### # ### # ### #SSSSS#### &quot;&quot;&quot; . Here, S denotes a starting positiona, F denotes a finishing position, # denotes a wall, and denotes a road. We have this track as a 2D NumPy Array, and encode agent&#39;s position as an index of this array. . import numpy as np def ascii_to_array(ascii_track: str) -&gt; np.ndarray: &quot;&quot;&quot;Convert the ascii (string) map to a NumPy array.&quot;&quot;&quot; lines = [line for line in ascii_track.split(&quot; n&quot;) if len(line) &gt; 0] byte_lines = [list(bytes(line, encoding=&quot;utf-8&quot;)) for line in lines] return np.array(byte_lines, dtype=np.uint8) track = ascii_to_array(SMALL_TRACK) print(track) position = np.array([0, 0]) track[tuple(position)] == int.from_bytes(b&#39;#&#39;, &quot;big&quot;) . [[35 35 35 32 32 32 32 32 32 70] [35 35 32 32 32 32 32 32 32 70] [35 35 32 32 32 32 32 32 32 70] [35 32 32 32 32 32 32 35 35 35] [35 32 32 32 32 32 32 35 35 35] [35 32 32 32 32 32 32 35 35 35] [35 83 83 83 83 83 35 35 35 35]] . True . Then, agent&#39;s velocity and acceleration are also naturally represented by an array. And, we represent an action as an index of an array consisting of all possible acceleration vetors: . np.array(list(itertools.product([-1, 0, 1], [-1, 0, 1]))) . array([[-1, -1], [-1, 0], [-1, 1], [ 0, -1], [ 0, 0], [ 0, 1], [ 1, -1], [ 1, 0], [ 1, 1]]) . The next step is to represent a transition function as a black box simulator. Note that we will visit another representation by transition matrix, but implementing the simulator is easier. Basically, the simulator should take an agent&#39;s action and current state, and then return the next state. Let&#39;s call this function step. However, let&#39;s make it return some other things to make the implementation easier. Reward function sounds fairly easy to implement given the agent&#39;s position. . The rewards are −1 for each step until the car crosses the finish line. . Also, we have to handle the termination of the episode. . Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. . So the resulting step function should return a tuple (state, reward, termination). The below cell contains my implementation of the simulator with matplotlib visualization. The step function is so complicated to handle the case where the agent goes through a wall, so readers are encouraged to just run their eyes through. . from typing import List, NamedTuple, Optional, Tuple from IPython.display import display from matplotlib import pyplot as plt from matplotlib.axes import Axes from matplotlib.colors import ListedColormap class State(NamedTuple): position: np.ndarray velocity: np.ndarray class RacetrackEnv: &quot;&quot;&quot;Racetrack environment&quot;&quot;&quot; EMPTY = int.from_bytes(b&quot; &quot;, &quot;big&quot;) WALL = int.from_bytes(b&quot;#&quot;, &quot;big&quot;) START = int.from_bytes(b&quot;S&quot;, &quot;big&quot;) FINISH = int.from_bytes(b&quot;F&quot;, &quot;big&quot;) def __init__( self, ascii_track: str, noise_prob: float = 0.1, seed: int = 0, ) -&gt; None: self._track = ascii_to_array(ascii_track) self._max_height, self._max_width = self._track.shape self._noise_prob = noise_prob self._actions = np.array(list(itertools.product([-1, 0, 1], [-1, 0, 1]))) self._no_accel = 4 self._random_state = np.random.RandomState(seed=seed) self._start_positions = np.argwhere(self._track == self.START) self._state_indices = None self._ax = None self._agent_fig = None self._arrow_fig = None def state_index(self, state: State) -&gt; int: &quot;&quot;&quot;Returns a state index&quot;&quot;&quot; (y, x), (vy, vx) = state return y * self._max_width * 25 + x * 25 + vy * 5 + vx def _all_passed_positions( self, start: np.ndarray, velocity: np.ndarray, ) -&gt; Tuple[List[np.ndarray], bool]: &quot;&quot;&quot; List all positions that the agent passes over. Here we assume that the y-directional velocity is already flipped by -1. &quot;&quot;&quot; maxv = np.max(np.abs(velocity)) if maxv == 0: return [start], False one_step_vector = velocity / maxv pos = start + 0.0 traj = [] for i in range(maxv): pos = pos + one_step_vector ceiled = np.ceil(pos).astype(int) if self._is_out(ceiled): return traj, True traj.append(ceiled) # To prevent numerical issue traj[-1] = start + velocity return traj, False def _is_out(self, position: np.ndarray) -&gt; bool: &quot;&quot;&quot;Returns whether the given position is out of the map.&quot;&quot;&quot; y, x = position return y &lt; 0 or x &gt;= self._max_width def step(self, state: State, action: int) -&gt; Tuple[State, float, bool]: &quot;&quot;&quot; Taking the current state and an agents&#39; action, returns the next state, reward and a boolean flag that indicates that the current episode terminates. &quot;&quot;&quot; position, velocity = state if self._random_state.rand() &lt; self._noise_prob: accel = self._actions[self._no_accel] else: accel = self._actions[action] # velocity is clipped so that only ↑→ directions are possible next_velocity = np.clip(velocity + accel, a_min=0, a_max=4) # If both of velocity is 0, cancel the acceleration if np.sum(next_velocity) == 0: next_velocity = velocity # List up trajectory. y_velocity is flipped to adjust the coordinate system. traj, went_out = self._all_passed_positions( position, next_velocity * np.array([-1, 1]), ) passed_wall, passed_finish = False, False for track in map(lambda pos: self._track[tuple(pos)], traj): passed_wall |= track == self.WALL passed_finish |= track == self.FINISH if not passed_wall and passed_finish: # Goal! return State(traj[-1], next_velocity), 0, True elif passed_wall or went_out: # Crasshed to the wall or run outside return self.reset(), -1.0, False else: return State(traj[-1], next_velocity), -1, False def reset(self) -&gt; State: &quot;&quot;&quot;Randomly assigns a start position of the agent.&quot;&quot;&quot; n_starts = len(self._start_positions) initial_pos_idx = self._random_state.choice(n_starts) initial_pos = self._start_positions[initial_pos_idx] initial_velocity = np.array([0, 0]) return State(initial_pos, initial_velocity) def render( self, state: Optional[State] = None, movie: bool = False, ax: Optional[Axes] = None, ) -&gt; Axes: &quot;&quot;&quot;Render the map and (optinally) the agents&#39; position and velocity.&quot;&quot;&quot; if self._ax is None or ax is not None: if ax is None: _, ax = plt.subplots(1, 1, figsize=(8, 8)) ax.set_aspect(&quot;equal&quot;) ax.set_xticks([]) ax.set_yticks([]) # Map the track to one of [0, 1, 2, 3] to that simple colormap works map_array = np.zeros_like(track) symbols = [self.EMPTY, self.WALL, self.START, self.FINISH] for i in range(track.shape[0]): for j in range(track.shape[1]): map_array[i, j] = symbols.index(self._track[i, j]) cm = ListedColormap( [&quot;w&quot;, &quot;.75&quot;, &quot;xkcd:reddish orange&quot;, &quot;xkcd:kermit green&quot;] ) map_img = ax.imshow( map_array, cmap=cm, vmin=0, vmax=4, alpha=0.8, ) if ax.get_legend() is None: descriptions = [&quot;Empty&quot;, &quot;Wall&quot;, &quot;Start&quot;, &quot;Finish&quot;] for i in range(1, 4): if np.any(map_array == i): ax.plot([0.0], [0.0], color=cm(i), label=descriptions[i]) ax.legend(fontsize=12, loc=&quot;lower right&quot;) self._ax = ax if state is not None: if not movie and self._agent_fig is not None: self._agent_fig.remove() if not movie and self._arrow_fig is not None: self._arrow_fig.remove() pos, vel = state self._agent_fig = self._ax.plot(pos[1], pos[0], &quot;k^&quot;, markersize=20)[0] # Show velocity self._arrow_fig = self._ax.annotate( &quot;&quot;, xy=(pos[1], pos[0] + 0.2), xycoords=&quot;data&quot;, xytext=(pos[1] - vel[1], pos[0] + vel[0] + 0.2), textcoords=&quot;data&quot;, arrowprops={&quot;color&quot;: &quot;xkcd:blueberry&quot;, &quot;alpha&quot;: 0.6, &quot;width&quot;: 2}, ) return self._ax smalltrack = RacetrackEnv(SMALL_TRACK) state = smalltrack.reset() print(state) display(smalltrack.render(state=state).get_figure()) next_state, reward, termination = smalltrack.step(state, 7) print(next_state) smalltrack.render(state=next_state) . . State(position=array([6, 5]), velocity=array([0, 0])) . State(position=array([5, 5]), velocity=array([1, 0])) . &lt;AxesSubplot:&gt; . Note that the vertical velocity is negative, so that we can simply represent the coordinate by an array index. . Solve a small problem by dynamic programming . OK, now we have a simulator, so let&#39;s solve the problem! However, before stepping into reinforcement learning, it&#39;s better to compute an optimal policy in a small problem for sanity check. To do so, we need a transition matrix p, which is a $| mathcal{S}| times | mathcal{A}| times | mathcal{S}|$ NumPy array where p[i][j][k] is the probability of transiting from i to k when action j is taken. Also, we need a $| mathcal{S}| times | mathcal{A}| times | mathcal{S}|$ reward matrix r. Note that this representation is not general as $R_t$ can be stochastic, but since the only stochasticity of rewards is the noise to actions in this problem, this notion suffices. It is often not very straightforward to get p and r from the problem definition, but basically we need to give 0-indexd indices to each state (0, 1, 2, ...) and fill the array. Here, I index each state by $ textrm{Idx(S)} = y times 25 times W + x times 25 + vy times 5 + vx$, where $(x, y)$ is a position, $(vx, vy$) is a velocity, and $W$ is the width of the map. . from typing import Iterable def get_p_and_r(env: RacetrackEnv) -&gt; Tuple[np.ndarray, np.ndarray]: &quot;&quot;&quot;Taking RacetrackEnv, returns the transition probability p and reward fucntion r of the env.&quot;&quot;&quot; n_states = env._max_height * env._max_width * 25 n_actions = len(env._actions) p = np.zeros((n_states, n_actions, n_states)) r = np.ones((n_states, n_actions, n_states)) * -1 noise = env._noise_prob def state_prob(*indices): &quot;&quot;&quot;Returns a |S| length zero-initialized array where specified elements are filled&quot;&quot;&quot; prob = 1.0 / len(indices) res = np.zeros(n_states) for idx in indices: res[idx] = prob return res # List up all states and memonize starting states states = [] starting_states = [] for y in range(env._max_height): for x in range(env._max_width): track = env._track[y][x] for y_velocity in range(5): for x_velocity in range(5): state = State(np.array([y, x]), np.array([y_velocity, x_velocity])) states.append(state) if track == env.START: starting_states.append(env.state_index(state)) for state in states: position, velocity = state i = env.state_index(state) track = env._track[tuple(position)] # At a terminating state or unreachable, the agent cannot move if ( track == env.FINISH or track == env.WALL or (np.sum(velocity) == 0 and track != env.START) ): r[i] = 0 p[i, :] = state_prob(i) # Start or empty states else: # First, compute next state probs without noise next_state_prob = [] for j, action in enumerate(env._actions): next_velocity = np.clip(velocity + action, a_min=0, a_max=4) if np.sum(next_velocity) == 0: next_velocity = velocity traj, went_out = env._all_passed_positions( position, next_velocity * np.array([-1, 1]), ) passed_wall, passed_finish = False, False for track in map(lambda pos: env._track[tuple(pos)], traj): passed_wall |= track == env.WALL passed_finish |= track == env.FINISH if passed_wall or (went_out and not passed_finish): # Run outside or crasheed to the wall next_state_prob.append(state_prob(*starting_states)) else: next_state_idx = env.state_index(State(traj[-1], next_velocity)) next_state_prob.append(state_prob(next_state_idx)) if passed_finish: r[i, j, next_state_idx] = 0.0 # Then linearly mix the transition probs with noise for j in range(n_actions): p[i][j] = ( noise * next_state_prob[env._no_accel] + (1.0 - noise) * next_state_prob[j] ) return p, r . . Then, let&#39;s compute the optimal Q value by value iteration. So far, we only learned dynamic programming with discount factor $ gamma$, so let&#39;s use $ gamma =0.95$ that is sufficiently large for this small problem. $ epsilon = 0.000001$ is used as a convergence threshold. Let&#39;s show the elapsed time and the required number of iteration. . import datetime class ValueIterationResult(NamedTuple): q: np.ndarray v: np.ndarray elapsed: datetime.timedelta n_iterations: int def value_iteration( p: np.ndarray, r: np.ndarray, discount: float, epsilon: float = 1e-6, ) -&gt; ValueIterationResult: n_states, n_actions, _ = p.shape q = np.zeros((n_states, n_actions)) v = np.zeros(n_states) n_iterations = 0 start = datetime.datetime.now() while True: n_iterations += 1 v_old = v.copy() for s in range(n_states): # Q(s, a) = ∑ p(s, a, s&#39;) * (r(s, a, s&#39;) + γ v(s&#39;) for a in range(n_actions): q[s, a] = np.dot(p[s, a], r[s, a] + discount * v) # V(s) = max_a Q(s, a) v[s] = np.max(q[s]) if np.linalg.norm(v - v_old, ord=np.inf) &lt; epsilon: break return ValueIterationResult(q, v, datetime.datetime.now() - start, n_iterations) p, r = get_p_and_r(smalltrack) vi_result = value_iteration(p, r, discount=0.95) print(f&quot;Elapsed: {vi_result.elapsed.total_seconds()} n_iter: {vi_result.n_iterations}&quot;) . Elapsed: 1.196624 n_iter: 10 . It took longer that a second on my laptop, even for this small problem. Some technical notes on value iteration ($R_ textrm{max} = 1.0$ is assumed for simplicity): . Each iteration takes $O(| mathcal{S}| ^ 2 | mathcal{A}|)$ time | The required iteration number is bounded by $ frac{ log epsilon}{ gamma - 1}$ In our case, $ frac{ log epsilon}{ gamma - 1} approx 270$, so our computation converged quicker than theory | Thus the total computation time is bounded by $O(| mathcal{S}| ^ 2 | mathcal{A}| frac{ log epsilon}{ gamma - 1})$ | . | For a convergence threshold $ epsilon$, $ max |V(s) - V^*(s)| &lt; frac{ gamma epsilon}{1 - gamma}$ is guranteed In our case, $ frac{ gamma epsilon}{1 - gamma} approx 0.00002$ | This is called relative error | . | . Let&#39;s visualize the V value and an optimal trajectory. celluloid) is used for making an animation. . from typing import Callable from IPython.display import HTML try: from celluloid import Camera except ImportError as _e: ! pip install celluloid --user from celluloid import Camera Policy = Callable[[int], int] def smalltrack_optimal_policy(state_idx: int) -&gt; int: return np.argmax(vi_result.q[state_idx]) def show_rollout( env: RacetrackEnv, policy: Policy, v: np.ndarray = vi_result.v, title: Optional[str] = None, ) -&gt; HTML: state = env.reset() prev_termination = False fig, ax = plt.subplots(1, 1, figsize=(8, 8)) camera = Camera(fig) initial = True while True: env.render(state=state, movie=True, ax=ax) state_idx = env.state_index(state) ax.text(3, 0.5, f&quot;V(s): {v[state_idx]:02}&quot;, c=&quot;red&quot;) camera.snap() if prev_termination: break state, _, prev_termination = env.step(state, policy(state_idx)) if title is not None: ax.text(3, 0.1, title, c=&quot;k&quot;) return HTML(camera.animate(interval=1000).to_jshtml()) show_rollout(smalltrack, smalltrack_optimal_policy) . &lt;/input&gt; Once Loop Reflect The computed optimal policy and value seems correct. . Monte-Carlo prediction . Then let&#39;s try &#39;reinforcement learning&#39;. First, I implemeted &#39;First visit Monte-Carlo prediction&#39;, which evaluates a (Markovian) policy $ pi$ by doing a simulation multiple times, and calculates the average of received returns. Here, I evaluate the optimal policy $ pi^*$ obtained by value iteration. . from typing import Union def first_visit_mc_prediction( policy: Policy, env: RacetrackEnv, n_episodes: int, discount: float = 0.95, record_all_values: bool = False, ) -&gt; Tuple[np.ndarray, List[np.ndarray]]: &quot;&quot;&quot;Predict value function corresponding to the policy by First-visit MC prediction&quot;&quot;&quot; n_states = env._max_width * env._max_height * 25 v = np.zeros(n_states) all_values = [] # Note that we have to have a list of returns for each state! # So the maximum memory usage would be Num.States x Num.Episodes all_returns = [[] for _ in range(n_states)] for i in range(n_episodes): if record_all_values: all_values.append(v.copy()) state = env.reset() visited_states = [env.state_index(state)] received_rewards = [] # Rollout the policy until the episode ends while True: # Sample an action from the policy action = policy(env.state_index(state)) # Step the simulator state, reward, termination = env.step(state, action) visited_states.append(env.state_index(state)) received_rewards.append(reward) if termination: break # Compute return traj_len = len(received_rewards) returns = np.zeros(traj_len) # Gt = Rt when t = T returns[-1] = received_rewards[-1] # Iterating from T - 2, T - 1, ..., to 0 for t in reversed(range(traj_len - 1)): # Gt = Rt + γGt+1 returns[t] = received_rewards[t] + discount * returns[t + 1] updated = set() # Update the value for i, state in enumerate(visited_states[: -1]): # If the state is already visited, skip it if state in updated: continue updated.add(state) all_returns[state].append(returns[i].item()) # V(St) ← average(Returns(St)) v[state] = np.mean(all_returns[state]) return v, all_values v, all_values = first_visit_mc_prediction( smalltrack_optimal_policy, smalltrack, 1000, record_all_values=True, ) value_diff = [] for i, mc_value in enumerate(all_values + [v]): value_diff.append(np.mean(np.abs(mc_value - vi_result.v))) plt.plot(value_diff) plt.xlabel(&quot;Num. Episodes&quot;) plt.ylabel(&quot;Avg. |V* - V|&quot;) . Text(0, 0.5, &#39;Avg. |V* - V|&#39;) . It looks like that the difference between $V^*$ and the value function estimated by MC prediction converges after 600 steps, but it&#39;s still larger than $0$, because $ pi^*$ doesn&#39;t visit all states. Let&#39;s plot the difference between value functions only on starting states. . start_states = [] for x in range(1, 6): idx = smalltrack.state_index(State(position=np.array([6, x]), velocity=np.array([0, 0]))) start_states.append(idx) start_values = [] for i, mc_value in enumerate(all_values + [v]): start_values.append(np.mean(np.abs(mc_value - vi_result.v)[start_states])) plt.plot(start_values) plt.xlabel(&quot;Num. Episodes&quot;) plt.ylabel(&quot;|V*(start) - V(start)| only on possible states&quot;) plt.ylim((0.0, 0.5)) . (0.0, 0.5) . Here, we can confirm that the estimated value certainly converged close to 0.0, while fractuating a bit. Note that the magnitude of fractuation is larger than the relative error we allowed for value iteration ($0.00002$), implying the difficulty of convergence. . Monte-Carlo Control . Now we successfully estimate $V^ pi$ using Monte Carlo method, so then let&#39;s try to learn a sub-optimal $ pi$ directly using Monte Carlo method. In the textbook, three methods are introduced: . Monte Carlo ES (Exploring Starts) | On-policy first visit Monte Carlo Control | Off-policy first visit Monte Carlo Control | . Here, let&#39;s try all three methods and compare the resulting value functions. However, we cannot naively implement the pseudo code in the textbook, due to a &#39;loop&#39; problem. Since the car that crashed into the wall is returned to a starting point, the episode length can be infinitte depending on a policy. As a remedy for this problem, I limit the length of the episode as $H$. Supposing that we ignore the future rewards smaller than $ epsilon$, how to set $H$? Just by solving $ gamma^H R &lt; epsilon$, we get $H &gt; frac{ log epsilon}{ log gamma}$, which is about $270$ in case $ gamma = 0.95$ and $ epsilon = 0.000001$. . Below are the implementation of three methods. A few notes about implementation: . Monte Carlo ES requires a set of all possible states, which is implemented in valid_states function. | For On-Policy MC, $ epsilon$ is decreased from 0.5 to 0.01 | We can use arbitary policy in Off-Policy MC, but I used the same $ epsilon$-soft policy as On-Policy MC. | . def valid_states(env: RacetrackEnv) -&gt; List[State]: states = [] for y in range(env._max_height): for x in range(env._max_width): track = env._track[y][x] if track == env.WALL: continue for y_velocity in range(5): for x_velocity in range(5): state = State(np.array([y, x]), np.array([y_velocity, x_velocity])) if track != env.START and (x_velocity &gt; 0 or y_velocity &gt; 0): states.append(state) return states def mc_es( env: RacetrackEnv, n_episodes: int, discount: float = 0.95, record_all_values: bool = False, seed: int = 999, ) -&gt; Tuple[np.ndarray, List[np.ndarray]]: &quot;&quot;&quot;Monte-Carlo Control with Exploring Starts&quot;&quot;&quot; n_states = env._max_width * env._max_height * 25 n_actions = len(env._actions) random_state = np.random.RandomState(seed=seed) q = np.zeros((n_states, n_actions)) pi = random_state.randint(9, size=n_states) all_values = [] all_returns = [[[] for _ in range(n_actions)] for _ in range(n_states)] possible_starts = valid_states(env) max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount))) for i in range(n_episodes): if record_all_values: all_values.append(q.copy()) state = possible_starts[random_state.choice(len(possible_starts))] visited_states = [env.state_index(state)] taken_actions = [] received_rewards = [] initial = True for _ in range(max_episode_length): if initial: # Randomly sample the first action action = random_state.randint(9) initial = False else: # Take an action following the policy action = pi[env.state_index(state)] taken_actions.append(action) # Step the simulator state, reward, termination = env.step(state, action) visited_states.append(env.state_index(state)) received_rewards.append(reward) if termination: break # Compute return traj_len = len(received_rewards) returns = np.zeros(traj_len) # Gt = Rt when t = T returns[-1] = received_rewards[-1] # Iterating from T - 2, T - 1, ..., to 0 for t in reversed(range(traj_len - 1)): # Gt = Rt + γGt+1 returns[t] = received_rewards[t] + discount * returns[t + 1] updated = set() # Update the value for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)): # If the state is already visited, skip it if (state, action) in updated: continue updated.add((state, action)) all_returns[state][action].append(returns[i].item()) # Q(St, At) ← average(Returns(St, At)) q[state, action] = np.mean(all_returns[state][action]) pi[state] = np.argmax(q[state]) return q, all_values def on_policy_fist_visit_mc( env: RacetrackEnv, n_episodes: int, discount: float = 0.95, epsilon: float = 0.1, epsilon_final: float = 0.1, record_all_values: bool = False, seed: int = 999, ) -&gt; Tuple[np.ndarray, List[np.ndarray]]: &quot;&quot;&quot;On-policy first visit Monte-Carlo&quot;&quot;&quot; n_states = env._max_width * env._max_height * 25 n_actions = len(env._actions) random_state = np.random.RandomState(seed=seed) q = np.zeros((n_states, n_actions)) pi = random_state.randint(9, size=n_states) all_values = [] all_returns = [[[] for _ in range(n_actions)] for _ in range(n_states)] possible_starts = valid_states(env) max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount))) epsilon_decay = (epsilon - epsilon_final) / n_episodes for i in range(n_episodes): if record_all_values: all_values.append(q.copy()) state = env.reset() visited_states = [env.state_index(state)] taken_actions = [] received_rewards = [] for _ in range(max_episode_length): # ε-soft policy if random_state.rand() &lt; epsilon: action = random_state.randint(9) else: action = pi[env.state_index(state)] taken_actions.append(action) # Step the simulator state, reward, termination = env.step(state, action) visited_states.append(env.state_index(state)) received_rewards.append(reward) if termination: break # Below code is the same as mc_es # Compute return traj_len = len(received_rewards) returns = np.zeros(traj_len) # Gt = Rt when t = T returns[-1] = received_rewards[-1] # Iterating from T - 2, T - 1, ..., to 0 for t in reversed(range(traj_len - 1)): # Gt = Rt + γGt+1 returns[t] = received_rewards[t] + discount * returns[t + 1] updated = set() # Update the value for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)): # If the state is already visited, skip it if (state, action) in updated: continue updated.add((state, action)) all_returns[state][action].append(returns[i].item()) # Q(St, At) ← average(Returns(St, At)) q[state, action] = np.mean(all_returns[state][action]) pi[state] = np.argmax(q[state]) epsilon -= epsilon_decay return q, all_values def off_policy_mc( env: RacetrackEnv, n_episodes: int, discount: float = 0.95, record_all_values: bool = False, epsilon: float = 0.1, epsilon_final: float = 0.1, seed: int = 999, ) -&gt; Tuple[np.ndarray, List[np.ndarray]]: &quot;&quot;&quot;Off-policy MC control&quot;&quot;&quot; n_states = env._max_width * env._max_height * 25 n_actions = len(env._actions) random_state = np.random.RandomState(seed=seed) q = np.zeros((n_states, n_actions)) c = np.zeros_like(q) pi = np.argmax(q, axis=1) all_values = [] possible_starts = valid_states(env) max_episode_length = int(np.ceil(np.log(1e-6) / np.log(discount))) epsilon_decay = (epsilon - epsilon_final) / n_episodes for i in range(n_episodes): if record_all_values: all_values.append(q.copy()) state = env.reset() visited_states = [env.state_index(state)] taken_actions = [] received_rewards = [] acted_optimally = [] for _ in range(max_episode_length): # ε-soft policy if random_state.rand() &lt; epsilon: action = random_state.randint(9) else: action = pi[env.state_index(state)] acted_optimally.append(action == pi[env.state_index(state)]) taken_actions.append(action) # Step the simulator state, reward, termination = env.step(state, action) visited_states.append(env.state_index(state)) received_rewards.append(reward) if termination: break g = 0 w = 1.0 for i, (state, action) in enumerate(zip(visited_states[:-1], taken_actions)): g = discount * g + received_rewards[i] c[state, action] += w q[state, action] += w / c[state, action] * (g - q[state, action]) pi[state] = np.argmax(q[state]) if action == pi[state]: break else: if acted_optimally: w *= 1.0 - epsilon + epsilon / n_actions else: w *= epsilon / n_actions epsilon -= epsilon_decay return q, all_values mces_result = mc_es(smalltrack, 3000, record_all_values=True) on_mc_result = on_policy_fist_visit_mc( smalltrack, 3000, epsilon=0.5, epsilon_final=0.01, record_all_values=True, ) off_mc_result = off_policy_mc( smalltrack, 3000, epsilon=0.5, epsilon_final=0.01, record_all_values=True, ) . Let&#39;s plot the results. Here I plotted the difference from the optimal value function and the number of states that the policy choices the optimal action. . def value_diff(q_values: List[np.ndarray]) -&gt; List[float]: diffs = [] for i, q in enumerate(q_values): diff = np.abs(np.max(q, axis=-1) - vi_result.v)[start_states] diffs.append(np.mean(diff)) return diffs def n_optimal_actions(q_values: List[np.ndarray]) -&gt; List[int]: n_optimal = [] optimal_actions = np.argmax(vi_result.q, axis=-1) for i, q in enumerate(q_values): greedy = np.argmax(q, axis=-1) n_optimal.append(np.sum(greedy == optimal_actions)) return n_optimal _, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6)) mc_es_value_diff = value_diff([mces_result[0]] + mces_result[1]) on_mc_value_diff = value_diff([on_mc_result[0]] + on_mc_result[1]) off_mc_value_diff = value_diff([off_mc_result[0]] + off_mc_result[1]) ax1.plot(mc_es_value_diff, label=&quot;MC-ES&quot;) ax1.plot(on_mc_value_diff, label=&quot;On-Policy&quot;) ax1.plot(off_mc_value_diff, label=&quot;Off-Policy&quot;) ax1.set_xlabel(&quot;Num. Episodes&quot;) ax1.set_ylabel(&quot;Avg. |V* - V|&quot;) ax1.set_title(&quot;Diff. from V*&quot;) mc_es_nopt = n_optimal_actions([mces_result[0]] + mces_result[1]) on_mc_nopt = n_optimal_actions([on_mc_result[0]] + on_mc_result[1]) off_mc_nopt = n_optimal_actions([off_mc_result[0]] + off_mc_result[1]) ax1.legend(fontsize=12, loc=&quot;upper right&quot;) ax2.plot(mc_es_nopt, label=&quot;MC-ES&quot;) ax2.plot(on_mc_nopt, label=&quot;On-Policy&quot;) ax2.plot(off_mc_nopt, label=&quot;Off-Policy&quot;) ax2.set_xlabel(&quot;Num. Episodes&quot;) ax2.set_ylabel(&quot;Num. Optimal Actions&quot;) ax2.legend(fontsize=12, loc=&quot;upper right&quot;) ax2.set_title(&quot;Num. of optimal actions&quot;) . Text(0.5, 1.0, &#39;Num. of optimal actions&#39;) . Some observations from the results: . On-Policy MC converges to the optimal policy the fastest, though the convergence of its value function is the slowest | MC-ES struggles to distinguish optimal and non-optimal actions at some states, probably because of the lack of exploration during an episode. | Compared to MC-ES and On-Policy MC, the peak of value differences of Off-Policy MC is much milder. A randomly initialized policy is often caught in a loop and cannot reach to the goal. The value of such a policy is really small ($-1 -1 * 0.95 - 1 * 0.95^2 - ... approx -20$). However, Off-Policy MC uses important sampling to decay the rewards by uncertain actions, resulting the smaller value differences. | . | . Here I visualized sampled plays from all three methods. On-Policy MC looks the most efficient. . for q, name in zip([mces_result[0], on_mc_result[0], off_mc_result[0]], [&quot;MC-ES&quot;, &quot;On-Policy&quot;, &quot;Off-Policy&quot;]): display(show_rollout(smalltrack, lambda i: np.argmax(q[i]), np.argmax(q, axis=-1), name)) . &lt;/input&gt; Once Loop Reflect &lt;/input&gt; Once Loop Reflect &lt;/input&gt; Once Loop Reflect",
            "url": "https://kngwyu.github.io/rlog/en/2022/03/07/sandb-exercise-racetrack.html",
            "relUrl": "/en/2022/03/07/sandb-exercise-racetrack.html",
            "date": " • Mar 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "マルコフゲーム入門",
            "content": "1. &#12510;&#12523;&#12467;&#12501;&#12466;&#12540;&#12512;&#12387;&#12390;&#65311; . . Note: このブログは強化学習若手の会 Advent Calendar 2021 20日目の記事として書かれました。遅れてすみません。 . 通常の強化学習では、エージェントは一人しかおらず、世界は固定されています。 遷移確率Pは変化しません。 この設定では、エージェントは例えば期待割引報酬和 $ mathbb{E}_{ pi}[ sum_{t=0}^ infty gamma^t r(s_t)] $を最大化するような方策を学習します。 . 一方で、学習するエージェントがいくつかいる場合はどうでしょうか？ すると、エージェントがもらえる期待報酬和は他のエージェントの方策$ pi_0, pi_1, ..., pi_k$に依存してしまいます。 では、$ mathbb{E}_{ pi_0, pi_1, ..., pi_k}[ sum_{t=0}^ infty gamma^t r(s_t)] $を最大化すればいいのかというと、必ずしもそうではありません。 利害関係が対立する場合があるからです。 このような場合に対し、どのように学習の目標を設計し、また実際に学習することができるのでしょうか？ . このブログでは、このようなマルチエージェント設定での学習を扱うフレームワークの一つであるマルコフゲームに注目し、その性質について概観します。特に、 . MDPにおける価値関数の基本定理のナッシュ均衡解への拡張 | Hannan consistent | 協力ゲームの一般化であるMarkov Potential GameとそのMDPへの帰着 について扱います。 論文については適宜ポインタを示しますが、以下の2つの論文/ブログ記事については広範に参照しています。 | Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms | Recent Progresses in Multi-Agent RL Theory | . MDPについての説明はアルバータ大学の講義資料およびその日本語訳を参照しています。 . なお、このブログは自分が勉強用に書いたものなので、やや不親切な記述があるかと思いますが、ご了承願います。 . 2. &#12510;&#12523;&#12467;&#12501;&#27770;&#23450;&#36942;&#31243;&#12392;&#22522;&#26412;&#23450;&#29702; . マルコフゲームの説明に入る前に、一人で学習する設定であるマルコフ決定過程 (Markov Decision Process, 以後MDPと呼ぶ)を使って、少しウォーミングアップしましょう。 . . 定義1: MDP . MDP $ mathcal{M}$は、以下から成る。 . (有限)状態集合 $ mathcal{S}$ | (有限)行動集合 $ mathcal{A}$ | 状態遷移確率 $P: mathcal{S} times mathcal{A} rightarrow Delta( mathcal{S})$ $ Delta( mathcal{S})$は$ mathcal{S}$上の確率分布の空間（確率単体） | 状態$s$で行動$a$をとったとき$s&#39;$に遷移する確率を$P(s&#39;|s, a)$と書く | . | 報酬関数 $r: mathcal{S} times mathcal{A} rightarrow [0, 1]$ 状態$s$で行動$a$をもらえる即時報酬を$r(s, a)$と書く | . | 割引報酬率 $0 leq gamma &lt; 1$ | 初期状態分布 $ mu in Delta( mathcal{S})$ | . . いま、エージェントとして、「記憶のない」方策$ pi: mathcal{S} rightarrow Delta( mathcal{A})$を考え、$ pi(a|s)$により状態$s$で行動$a$をとる確率を表記します。 ある方策$ pi$を固定したとき、エージェントがもらう報酬の期待値を以下のように書きます。 . . 定義2: 状態価値関数 $V^ pi$・状態行動価値関数 $Q^ pi(s, a)$・期待リターン$G^ pi$ . $$ begin{align*} V^ pi(s) &amp;= mathbb{E}_{ pi} left[ sum_{t = 0} ^ infty gamma^t r(S_t, A_t) right] &amp;= sum_{a in mathcal{A}} pi(a|s) left(r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s,a) V^ pi(s) right) Q^ pi(s, a) &amp;= r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s,a) V^ pi(s) G^ pi &amp;= sum_{s in mathcal{S}} mu(s) V^ pi(s) end{align*} $$ . ここで、$S_t, A_t$は時刻$t$での状態・行動に対応する確率変数です。 このとき、(割引)報酬和の期待値$G^ pi$を最大化することがエージェントの目標であり、その目標を達成する方策を最適方策$ pi^*$と書きます。 . $ pi^*$を求めるための非常に強力な道具がベルマン作用素(Bellman operator)と呼ばれる一連のオペレーターです。 . . 定義3: 方策$ pi$についてのベルマン演算子 . $V$を任意の価値関数とする。このとき、方策$ pi$に付随するベルマン演算子$ mathcal{T}^ pi$は$V$を$ mathcal{T}^ pi V$に写像する。$ mathcal{T}^ pi V$は以下のように与えられる。 . $( mathcal{T}^ pi V)(s) = sum_{a in mathcal{A}} pi(a|s) left( r(s, a) + sum_{s&#39; in mathcal{S}} P(s&#39;|s,a) V(s&#39;) right)$ . . これだけだとだから何?という感じがしますが、この演算子は一回の適用で真の$V^ pi$との差1が(最大ノルムにおいて)$ gamma$ずつ縮まるという性質を持っているので、 これを繰り返し適用すると$V^ pi$を得ることができます。 少しやってみましょう。 . . $V$を$| mathcal{S}|$次元のベクトルだとみなすと、最大ノルムの差が縮まります。&#8617; . |",
            "url": "https://kngwyu.github.io/rlog/ja/2021/12/22/markov-game.html",
            "relUrl": "/ja/2021/12/22/markov-game.html",
            "date": " • Dec 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Jax・Brax・HaikuでGPU引きこもり学習",
            "content": "0. &#26178;&#21218;&#12398;&#12354;&#12356;&#12373;&#12388;&#12392;&#12363; . . Note: このブログは強化学習若手の会 Advent Calendar 2021 18日目の記事として書かれました . こんにちは。 コロナ禍も終わりが見えつつある（と思ったらオミクロン株が...）2021年もあとわずか。寒さも厳しくなってきましたが、皆さん如何お過ごしでしょうか。 とは言ったものの、僕は沖縄にいるので、それほど寒くはないのですが...。 . 若手の会のアドベントカレンダーということで、国内でのコミュニティの活動について、最初に総括してみたいと思います。 . 若手の会では模倣学習の勉強会をしましたが、結局2回しか続きませんでした。 | 苦手の会のもくもく会はフリスビーやマラソンの練習とバッティングしてやらなくなってしまいましたが、最近日程を変えて、火曜日の夜に始めました。暇な方一緒にもくもくしましょう。 | 強化学習の講義資料の翻訳をしています。難しいですが、けっこう勉強になります。有限サンプルでのバウンドを、初めて勉強しました。興味がある方は、ぜひ一緒にやりましょう。 | 昨年のブログを書いてからはや一年、せっかく専用のブログを作ったので今年もいい感じにmatplotlib芸強化学習の記事を書いていきたいと思っていましたが、結局何も書きませんでした。 | . 最近は人工進化や人工生命の研究も始めたので、もはや「強化学習の人」と名乗っていいのかよくわかりませんが、今後も国内のコミュニティに何か貢献できればと思います。 . 今年は強化学習に対する楽観論も悲観論も多く目にした一年でした。 David SilverやSuttonはReward is Enoughという強気な論文を出し、知的なシステムはおよそ全て報酬最大化で作れると主張しました。 さすがに強気すぎると思いますが、その後Reward is enough for convex MDPsやOn the Expressivity of Markov Rewardといったテクニカルな論文が出てきたのは面白いです。 また、オフライン強化学習・教師なし強化学習の論文が増えてきたと思います。 ざっくり、オフライン強化学習 = 強化学習 - 探索、教師なし強化学習 = 強化学習 - 報酬と思ってもらって問題ないでしょう。 何を隠そう僕の修士論文も単なる「階層型強化学習」だったのですが、リジェクト諸般の事情により教師なしに魔改造して再投稿しました。 Sergey LevineにいたってはUnderstanding the World Through Actionというタイトルが強い論文の中で、「大量にデータを集めてオフライン教師なし強化学習をすれば世界を理解できる（≒世界を理解しているのと同等のシステムが作れる？）」と言っています。面白い方向性だと思います。 一方で、みんな大好きルービックキューブ論文を出したOpen AIのロボティクスチームは、「とりあえず今データがある領域に注力する」とのことで解散してしまいました。 このブログを書いている最中にWebGPTの論文を目にしましたが、今後は言語モデル＋強化学習で色々やっていくのでしょうか。品川さんは喜びそうですが、僕なんかはこういう到底自分でできないものは「テレビの中の研究」という感じがして一歩引いてしまいます（最近は、テレビとかたとえに使うと古いのかな...）。 Open AIのロボティクスは、Sim2Realにこだわりすぎたのでは？という意見を某所でお聞きしました。実際そうなのかは知りませんが、大規模にシミュレーションしてSim2Realを頑張るのか、実機のデータで頑張るのかというのは、面白い視点ですよね。 . Open AIが今までほど強化学習に注力しなくなったことで、Open AI gymをはじめ強化学習研究で使われてきたソフトウェア群にも、色々と情勢の変化がありそうです。 . OpenAI Gymのメンテナが変わりました。これからはOpen AIではなくメリーランド大学の学生さんがメンテナになるようです。mujoco-pyなど関連するライブラリについては相変わらず放置されています。 | DeepmindがMuJoCoを買い取って無料にしました。今後ソースコードも公開されるようです。 | Googleから新しくbraxというシミュレータが公開されました。 | そんなわけで、僕はこれまでmujoco-py + gymで作成したカスタム環境でたくさん実験をやってきましたが、MuJoCoを使うにしてもdm_controlを使うとか、はたまたbraxにしてしまうとか、別の選択肢を検討したくなってきました。 このブログでは、とりあえずbraxを試してみようと思います。 . 1. &#12399;&#12376;&#12417;&#12395;: &#12471;&#12511;&#12517;&#12524;&#12540;&#12471;&#12519;&#12531;&#12539;&#35598;&#12525;&#12508;&#12483;&#12488;&#12539;GPU . 本題に入りますが、ざっくり、強化学習とは、報酬から行動を学習する枠組みだと言うことができます。 では何の行動を学習させたいのでしょうか。 ゲームのAIだったり、チャットボットだったり、色々な選択肢があると思いますが、どういうわけかシミュレータ上で動く謎ロボットというのがポピュラーな選択肢です。 . このブログをごらんの方の中には、こういったナナフシのような謎ロボットの画像を目にしたことがある方も多いのではないでしょうか。 . . これはOpen AI gymのHalfCheetahというロボットです。足が2本なのでハーフなのだと思いますが、なんとも残酷なネーミングです。愛玩されるため病気のまま品種改良されてきた犬猫のような哀愁が漂います。 . MuJoCoシミュレーターに「こことここがジョイントで、可動域はこうです。床は白黒でお願いします」みたいなXMLファイルを渡すと、こういうロボットを作ってくれます。 もしくは、dm_controlなどのPythonライブラリにXMLを作らせることもできます。 このような謎ロボットが実験で広く使われている要因として、 . みんなが使っているから | Atariなどのゲームより高速 | ジョイントの速さ・位置などの完全な内部状態が手に入る マルコフ性について心配しなくてもいい | . | 色々カスタマイズできて便利だから | 普通のロボットを訓練するためのテストにちょうどいいから | . などの理由があると思いますが、なんだかんだみんなが使っているからというのが大きい気がします。 . ところで、このMuJoCoシミュレータというのは非常に高速に動作するのですが、CPU上でしか動作しません。 今日使われている深層学習のコードは、その計算量のほとんどを占める行列演算がベクトル並列化ととても相性がいいため、ネットワークやバッチサイズが大きくなればなるほどGPU上で高速に動作します。 となると、GPUで学習を回している場合、どうしてもCPUからGPUにデータを転送するボトルネックが発生し、高速化の妨げになります。 そこで、GPU上でシミュレーションを行えるようにしたのが、今回紹介するbraxというシミュレータです。 . 2. Jax&#12391;numpy&#28436;&#31639;&#12434;&#39640;&#36895;&#21270;&#12375;&#12390;&#12415;&#12427; . では、braxはCUDAか何かで書かれているのかな？と思ったかもしれませんが、なんと全てPythonで書かれているのです。 その鍵となるのがjaxというライブラリです。 おもむろに、インストールしてみましょう。 . ! pip install jax . ドキュメントの冒頭に&#39;JAX is Autograd and XLA&#39;とありますが、Jaxは . Numpy演算をXLAに変換するコンパイラ(Tensorflow) jax.jit | XLAはTensorflowのバックエンドとして開発された中間言語で、GPU/TPU用にすごく速いコードを生成できる | . | Numpy演算を追跡して勾配を計算する機能 jax.grad/jax.vjp など | . | . の2つのコア機能を核とするライブラリです。 この節では、ひとまず前者の「XLAに変換するコンパイラ」としての機能に焦点を当ててみます。 . コンパイラはJIT方式で実装されており、 . jax.jitに関数fを渡す (f_compiled = jax.jit(f)） | コンパイルされる関数f_compiledを最初に呼び出したとき、jaxはPythonの関数をXLAにコンパイルする | 2回目以降関数呼び出しが高速になる という処理の流れになります。 | では、さっそく何かシミュレーションしてみましょう。 適当に天井からボールを落としてみましょう。 . import typing as t import numpy as np from IPython.display import HTML, clear_output try: import pandas as pd import seaborn as sns from celluloid import Camera from matplotlib import pyplot as plt from matplotlib.animation import ArtistAnimation except ImportError as _e: ! pip isntall pandas seaborn celluloid clear_output() import pandas as pd import seaborn as sns from celluloid import Camera from matplotlib import pyplot as plt from matplotlib.animation import ArtistAnimation sns.set_theme(style=&quot;darkgrid&quot;) Array = np.ndarray GRAVITY = -9.8 def move_balls( ball_positions: Array, ball_velocities: Array, delta_t: float = 0.1, ) -&gt; Array: accel_x = np.zeros(ball_positions.shape[0]) accel_y = np.ones(ball_positions.shape[0]) * GRAVITY * delta_t # y方向にGΔt加速 new_velocities = np.stack((accel_x, accel_y), axis=1) + ball_velocities new_positions = ball_positions + delta_t * new_velocities return new_positions, new_velocities def simulate_balls( n_balls: int, n_steps: int = 100, forward: t.Callable[[Array], Array] = move_balls, ) -&gt; t.List[Array]: p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0) v = np.random.randn(n_balls, 2) results = [p] for _ in range(n_steps): p, v = forward(p, v) results.append(p) return results . 適当にボールを20個落としてみます。 . def ball_animation(balls: t.Iterable[Array]) -&gt; ArtistAnimation: fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot() ax.set_xlim(-50, 50) ax.set_ylim(-50, 50) camera = Camera(fig) for ball_batch in balls: ax.scatter(ball_batch[:, 0], ball_batch[:, 1], color=&quot;red&quot;, alpha=0.7) camera.snap() return camera.animate() HTML(ball_animation(simulate_balls(20, 40)).to_jshtml()) . . &lt;/input&gt; Once Loop Reflect では、このシミュレーションをするのに、どれくらい時間がかかるでしょうか。ボールの数を変えてベンチマークしてみましょう。 . def bench( f: t.Callable[..., t.Any], inputs: t.Iterable[t.Any], number: int = 10, ) -&gt; t.List[float]: import timeit return [timeit.Timer(lambda: f(x)).timeit(number=number) for x in inputs] def bench_and_plot(f: t.Callable[..., t.Any], title: str) -&gt; pd.DataFrame: inputs = [4000, 8000, 16000, 32000, 64000] result = pd.DataFrame({&quot;x&quot;: inputs, &quot;y&quot;: bench(f, inputs)}) result[&quot;Method&quot;] = [title] * len(inputs) ax = sns.lineplot(data=result, x=&quot;x&quot;, y=&quot;y&quot;) ax.set_title(title) ax.set_xlabel(&quot;Num. of balls&quot;) ax.set_ylabel(&quot;Time (sec.)&quot;) return result numpy_result = bench_and_plot(simulate_balls, &quot;NumPy&quot;) . おおむね線形に実行時間が増えていることがわかります。このコードを、jaxを使って高速化してみましょう。 基本的にはnumpyをjax.numpyに置き換えればいいです。 . import jax import jax.numpy as jnp JaxArray = jnp.DeviceArray def move_balls_jax( ball_positions: JaxArray, ball_velocities: JaxArray, delta_t: float = 0.1, ) -&gt; JaxArray: accel_x = jnp.zeros(ball_positions.shape[0]) accel_y = jnp.ones(ball_positions.shape[0]) * GRAVITY * delta_t new_velocities = jnp.stack((accel_x, accel_y), axis=1) + ball_velocities new_positions = ball_positions + delta_t * new_velocities return new_positions, new_velocities . では同じようにベンチマークをとってみましょう。 . jax_nojit_result = bench_and_plot( lambda n: simulate_balls(n, forward=move_balls_jax), &quot;JAX (without JIT)&quot;, ) . 謎の挙動を見せているし、すごく遅いですね。今度はJITコンパイルしてみましょう。 jax.jit(f, backend=&quot;cpu&quot;)で関数をCPU上で動くXLAコードにコンパイルできます。 . jax_cpu_result = bench_and_plot( lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=&quot;cpu&quot;)), &quot;JAX (with JIT on CPU)&quot;, ) . すごく速くなりました。今度はGPUでやってみます。 . jax_gpu_result = bench_and_plot( lambda n: simulate_balls(n, forward=jax.jit(move_balls_jax, backend=&quot;gpu&quot;)), &quot;JAX (with JIT for GPU)&quot;, ) . 圧倒的に速いですね。一応線形に実行時間が増えてはいますが...。 なお、今回は学内スパコンのNVIDIA P100 GPUを使用しています。 . ax = sns.lineplot( data=pd.concat( [numpy_result, jax_nojit_result, jax_cpu_result, jax_gpu_result], ignore_index=True, ), x=&quot;x&quot;, y=&quot;y&quot;, style=&quot;Method&quot;, hue=&quot;Method&quot;, ) ax.set_title(&quot;Ball benchmark&quot;) ax.set_xlabel(&quot;Num. of balls&quot;) ax.set_ylabel(&quot;Time (sec.)&quot;) None . . このボールの数だとGPUは線形に計算時間が増えているように見えませんね。 まあ何はともあれ、GPU用にJITコンパイルしてあげると速そうだなあ、という感じがします。 . 3. Jax&#12391;&#21246;&#37197;&#12434;&#35336;&#31639;&#12375;&#12390;&#12415;&#12427; . Jaxは単に速いNumPyとしての機能に加え、自動微分によって、関数$f(x, y, z, ...)$の各$x, y, z,...$による偏微分$ frac{ partial f}{ partial x}, frac{ partial f}{ partial y}, frac{ partial f}{ partial z}, ...$を計算する機能を持っています。ここではjax.gradによる勾配の計算だけを紹介します。 . なんか、適当に関数を最適化してみましょう。まずは、適当に関数を決めてみます。 $z = x^2 + y^2 + y$ にしました。 . def f(x, y): return x ** 2 + y ** 2 + y def plot_f(traj: t.Optional[Array] = None) -&gt; None: x, y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100)) fig = plt.figure() ax = fig.add_subplot(111, projection=&quot;3d&quot;) ax.plot_surface( x, y, f(x, y), cmap=sns.color_palette(&quot;flare&quot;, as_cmap=True), alpha=0.8, linewidth=0, ) if traj is not None: ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], color=&quot;blue&quot;) ax.set_xlabel(&quot;x&quot;, fontsize=14) ax.set_ylabel(&quot;y&quot;, fontsize=14) ax.set_zlabel(&quot;z&quot;, fontsize=14, horizontalalignment=&quot;right&quot;) ax.set_title(&quot;f&quot;, fontsize=15) plot_f() . $(x, y) = (5, 5)$でのこの関数の勾配を計算してみます。勾配を計算してほしい引数をjax.grad(argnums=...)で指定します。 . jax.grad(f, argnums=(0, 1))(jnp.array(5.0), jnp.array(5.0)) . (DeviceArray(10., dtype=float32, weak_type=True), DeviceArray(11., dtype=float32, weak_type=True)) . $ frac{ partial z}{ partial x}, frac{ partial z}{ partial y}$を計算してくれました。 せっかくなので、最急降下法してみましょう。 . def steepest_descent(alpha: float = 0.01) -&gt; JaxArray: f_grad = jax.grad(f, argnums=(0, 1)) x, y = jnp.array(5.0), jnp.array(5.0) traj = [] while True: traj.append((x, y, f(x, y))) x_grad, y_grad = f_grad(x, y) if jnp.linalg.norm(jnp.array([x_grad, y_grad])) &lt; 0.05: break x -= alpha * x_grad y -= alpha * y_grad return jnp.array(traj) plot_f(steepest_descent()) . 最急降下方向に進んでくれているように見えます。 ところで、gradはトップダウン型リバースモード自動微分（誤差逆伝播法の難しい言い方です）を採用しているので、リバースモードでVector Jacobian Productを計算するvjpという関数が使われています。 フォーワードモードで計算するjvpという関数もあります。 このあたりの機能は、ただネットワークを学習させたいだけならほとんど使いませんが、一応やってみましょう。 . primals, f_vjp = jax.vjp(f, 5.0, 5.0) print(f&quot;VJP value: {primals.item()} grad: {[x.item() for x in f_vjp(1.0)]}&quot;) value, grad = jax.jvp(f, (5.0, 5.0), (1.0, 1.0)) print(f&quot;JVP value: {value.item()} grad: {grad.item()}&quot;) . VJP value: 55.0 grad: [10.0, 11.0] JVP value: 55.0 grad: 21.0 . フォーワードモードの場合勾配となんかのベクトルvとの内積がでてきます。 このあたり、色々な教科書に書いてあると思いますが、Forward modeとReverse modeの違いなど、Probabilistic Machine Learning: An Introductionの13章が特にわかりやすいと思います。興味がある方は参考にしてみてください。 . 4. Brax&#12434;&#20351;&#12387;&#12390;&#12415;&#12427; . じゃあMuJoCoみたいな物理シミュレーターもJaxで書いてしまえば勝手にGPU上で動いて速いんじゃない？というモチベーションで作られたのがbraxです。 簡単に特徴をまとめてみます。 . Jaxで記述されているため、jitで高速化できる | Protocol Bufferでシステムを定義 (cf. MuJoCoはXML） | dataclassQPを使った簡潔な状態記述 Qは正準座標、Pは運動量らしい | . | OpenAI gym風のEnv APIやAnt・Halfcheetahなどの謎ロボット | . おもむろにインストールしてみます。 . try: import brax except ImportError: !pip install git+https://github.com/google/brax.git@main clear_output() import brax . さっきと同じ、ボールを動かしてみましょう。さっきはxy座標で動かしましたが、brax . def make_ball() -&gt; None: config = brax.Config(dt=0.1, substeps=4) # ボールを追加 ball = config.bodies.add(name=&quot;ball&quot;, mass=1) capsule = ball.colliders.add().capsule capsule.radius = 0.5 # y座標に重力 config.gravity.y = GRAVITY return config def make_qp(p, v) -&gt; brax.QP: return brax.QP( pos=jnp.array([[p[0], p[1], 0.0]]), # position vel=jnp.array([[v[0], v[1], 0.0]]), # velocity rot=jnp.zeros((1,4)), # rotation ang=jnp.zeros((1, 3)), # angular velocity ) def simulate_one_ball_brax(n_steps: int = 100) -&gt; t.List[Array]: sys = brax.System(make_ball()) qp = make_qp([0.0, 48.0], [1.0, 0.0]) results = [] for _ in range(n_steps): qp, _ = sys.step(qp, []) results.append(qp.pos[:2]) return results HTML(ball_animation(simulate_one_ball_brax(40)).to_jshtml()) . &lt;/input&gt; Once Loop Reflect ここで、4つのAPIを使いました。 . brax.Configでシステムを定義 | brax.System(config)でシステムを作成 | brax.QPで初期位置・速度・アングル等を作成 | brax.System.step(qp, ...)で1ステップシミュレーションした結果を取得 | . ボールが一つだとなんとなく物足りないですね。増やしてみましょう。そのためには、jax.vmapでsys.stepをベクトル化します。 デフォルトで、vmapは引数のテンソルに対する演算をaxis=0でバッチ化します。 このあたりはin_axes=(1, 0, ...)とかやれば調節できますが、今回はデフォルトでOKです。 . [make_qp(*pv) for pv in zip(p, v)]で、List[brax.QP]を作っていますが、これをjax.tree_mapでもう一回QPに戻しています。 . List[QP(p=(0, 0), v(0, 0)), QP(..), ...] . が . QP( p=[(0, 0), (0.1, 0.2),. ...], v=[(0, 0), (1, 2), ...], ) . に変換される感じです。 このジャーゴンは便利なので覚えてもいいと思います。 ちなみに、treemapのノードが葉かどうかはオブジェクトがPyTreeか否かによります。 これは「以上のデータ構造をJaxは暗に木構造だとみなします。不足なら自分で登録してください」という話なので、最初は面食らうと思います。 これを陽なAPIでやろうにするとRustやScalaにあるtraitが必要なので、悪い設計ではないと思いますが。 というわけで、コードはこんな感じになります。 . def simulate_balls_brax(n_balls: int, n_steps: int = 100) -&gt; t.List[Array]: sys = brax.System(make_ball()) p = np.random.uniform(size=(n_balls, 2), low=-50, high=50.0) v = np.random.randn(n_balls, 2) qps = [make_qp(*pv) for pv in zip(p, v)] qps = jax.tree_map(lambda *args: jnp.stack(args), *qps) # ここで step_vmap = jax.jit(jax.vmap(lambda qp: sys.step(qp, []))) results = [] for _ in range(n_steps): qps, _ = step_vmap(qps) results.append(qps.pos[:, 0, :2]) return results HTML(ball_animation(simulate_balls_brax(20, 40)).to_jshtml()) . &lt;/input&gt; Once Loop Reflect jitを使わないとbraxがなぜかnumpyの関数を呼ぼうとしてエラーになったので、jitも併用しています。 . 5. Haiku&#12391;&#35598;&#12398;&#12525;&#12508;&#12483;&#12488;&#12434;&#23398;&#32722;&#12373;&#12379;&#12390;&#12415;&#12427; . というわけで、braxの使い方をざっと見てみましたが、毎回自分でロボットを考えるのは大変だし査読者にも文句を言われるなので、今回は謎ロボットを学習させてみましょう。 open AI gym風のbrax.envs.Envがサポートされています。今回はAntを訓練してみましょう。 gym.makeに相当するのがbrax.envs.createです。 stepのAPIはgymと違い内部状態・報酬などが入ったbrax.envs.Stateというクラスを渡して次のStateを受け取るというインターフェイスです。 . import brax.envs def render_html(sys: brax.System, qps: t.List[brax.QP]) -&gt; HTML: import uuid import brax.io.html html = brax.io.html.render(sys, qps) # A weired trick to show multiple brax viewers... html = html.replace(&quot;brax-viewer&quot;, f&quot;brax-viewer-{uuid.uuid4()}&quot;) return HTML(html) def random_ant() -&gt; HTML: env = brax.envs.create(env_name=&quot;ant&quot;) prng_key = jax.random.PRNGKey(0) state = env.reset(prng_key) qps = [state.qp] step_jit = jax.jit(env.step) for i in range(10): prng_key, action_key = jax.random.split(prng_key) action = jax.random.normal(action_key, shape=(env.action_size,)) state = step_jit(state, action) qps.append(state.qp) return render_html(env.sys, qps) random_ant() . brax visualizer . . なんか、跳ねていますね。嬉しそう。 では、さっそく学習させてみましょう。 今回は、深層強化学習の代表的な手法であるPPOを使ってみます。 本当はSACも用意したかったのですが、時間がなかったので諦めました。 とりあえず、三層MLPを用意しましょう。 例えば、こんな感じのものがあればいいです。 . def mlp_v1( observation: JaxArray, w1: JaxArray, b1: JaxArray, w2: JaxArray, b2: JaxArray, w3: JaxArray, b3: JaxArray, ) -&gt; JaxArray: x = jnp.dot(observation, w1) + b1 x = jnp.tanh(x) x = jnp.dot(x, w2) + b2 x = jnp.tanh(x) return jnp.dot(x, w3) + b3 . これをjitしてgradをとってAdamか何かでパラメタを更新して...とやればMLPが動くわけですが、パラメタが多すぎてちょっと面倒ですね。 そこでここでは、jaxでニューラルネットワークを訓練する際、パラメタの管理などをやってくれるライブラリであるHaikuを使ってみます。 なお、brax公式のexamplesではFlaxを使っています。 正直HaikuもFlaxもそこまで変わらないのですが、Flaxの方がややAPIの押しが強い（PyTorchでいうnn.Module相当のものがdataclassでないといけなかったりとか）印象があります。 また、HaikuはDeepmindが、FlaxはGoogleが開発しているライブラリとなります。 とりあえずインストールしてみましょう。 . try: import haiku as hk import optax import chex import distrax except ImportError as e: ! pip install git+https://github.com/deepmind/dm-haiku git+https://github.com/deepmind/optax git+https://github.com/deepmind/chex git+https://github.com/deepmind/distrax import haiku as hk import optax import chex import distrax clear_output() . ニューラルネットワークを定義するためのPythonライブラリはtheano、tensorflowと色々ありましたが、最近はtorch.nn.Moduleやchainer.Linkのように、ネットワークの重み・forwardの出力・隣接しているノードを記録したオブジェクトを使って、動的に計算グラフを構築するものが多いかと思います。 しかし、Haikuによるそれは少し異なります。ポイントは、勾配を計算する部分はJaxが担当するので、Haikuはただ「ネットワークのパラメタを管理するだけ」でいいということです。 そのために、HaikuはtransformというAPIを用意しています。 これは見たほうが早いでしょう。 . def mlp_v2(observation: JaxArray) -&gt; JaxArray: w1 = hk.get_parameter(&quot;w1&quot;, shape=[observation.shape[1], 3], init=jnp.ones) b1 = hk.get_parameter(&quot;b1&quot;, shape=[3], init=jnp.zeros) w2 = hk.get_parameter(&quot;w2&quot;, shape=[3, 3], init=jnp.ones) b2 = hk.get_parameter(&quot;b2&quot;, shape=[3], init=jnp.zeros) w3 = hk.get_parameter(&quot;w3&quot;, shape=[3, 2], init=jnp.ones) b3 = hk.get_parameter(&quot;b3&quot;, shape=[2], init=jnp.zeros) x = jnp.dot(observation, w1) + b1 x = jnp.tanh(x) x = jnp.dot(x, w2) + b2 x = jnp.tanh(x) return jnp.dot(x, w3) + b3 prng_seq = hk.PRNGSequence(0) # これをグローバル変数にするのは良くないです。真似しないで init, apply = hk.transform(mlp_v2) # transformする # initは乱数シード・インプットを受け取って、初期化したパラメタを返す関数 params = init(next(prng_seq), jnp.zeros((10, 2))) print(params) # applyはパラメタ・乱数シード・インプットを受け取って、出力を返す関数 output = apply(params, next(prng_seq), jnp.zeros((10, 2))) . FlatMap({ &#39;~&#39;: FlatMap({ &#39;w1&#39;: DeviceArray([[1., 1., 1.], [1., 1., 1.]], dtype=float32), &#39;b1&#39;: DeviceArray([0., 0., 0.], dtype=float32), &#39;w2&#39;: DeviceArray([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=float32), &#39;b2&#39;: DeviceArray([0., 0., 0.], dtype=float32), &#39;w3&#39;: DeviceArray([[1., 1.], [1., 1.], [1., 1.]], dtype=float32), &#39;b3&#39;: DeviceArray([0., 0.], dtype=float32), }), }) . こんな感じになります。 まとめると、 . transform(f)は二つの関数init、applyをかえす | transformはfを、fの中でhaiku.get_parameterを使って呼び出されたパラメタを入力とする関数に変換する | initはパラメタを初期化して返す。パラメタはFlatMapというオブジェクトだがこれはほとんどdictと同じ | applyは与えられたパラメタを使って所望の計算を行う という感じですね。 | . ついでに、上の例ではhk.PRNGSequenceというPRNGKeyの更新を勝手にやってくれるものを使っています。 . しかし、これでもまだ面倒ですね。 実際のところ、よく使うネットワークはモジュールとしてまとまっているので、これを使えばいいです。 . def mlp_v3(output_size: int, observation: JaxArray) -&gt; JaxArray: x = hk.Linear(64)(observation) x = jnp.tanh(x) x = hk.Linear(64)(observation) x = jnp.tanh(x) return hk.Linear(output_size)(observation) . これを使って、PPOのネットワークを書いてみましょう。 方策は標準偏差が状態に依存しない正規分布にします。 . class NetworkOutput(t.NamedTuple): mean: JaxArray stddev: JaxArray value: JaxArray def policy_and_value(action_size: int, observation: JaxArray) -&gt; NetworkOutput: mean = mlp_v3(output_size=action_size, observation=observation) value = mlp_v3(output_size=1, observation=observation) logstd = hk.get_parameter(&quot;logstd&quot;, (1, action_size), init=jnp.zeros) stddev = jnp.ones_like(mean) * jnp.exp(logstd + 1e-8) return NetworkOutput(mean, stddev, value) . これだけです。デフォルトでは、ネットワークの重みはTruncatedNormalで初期化されます。今回は全部デフォルトのままにしました。 . 次に、これを使って、環境とインタラクトするコードを書いてみます。 いま、braxの利点を活かすために、 . ネットワークから次のアクションをサンプルして | シミュレータで次の状態をシミュレート という過程をすべてjax.jitの中でやるのが理想ですよね。 | ですから、たとえばこんな感じにやればいいです。 . Action = JaxArray def make_step_function( env: brax.envs.Env, ) -&gt; t.Tuple[t.Callable[..., t.Any], t.Callable[..., t.Any]]: def step(state: brax.envs.State) -&gt; t.Tuple[brax.envs.State, NetworkOutput, Action]: out = policy_and_value(env.action_size, state.obs) policy = distrax.MultivariateNormalDiag(out.mean, out.stddev) action = policy.sample(seed=hk.next_rng_key()) # transformするとこれが使えます state = env.step(state, jnp.tanh(action)) return state, out, action init, apply = hk.transform(step) return jax.jit(init), jax.jit(apply) . ここで、行動のサンプルにはdistraxというライブラリを使いました。 平均値にノイズをいれるだけなので、ライブラリを使ってもあまり変わらないのですが...。 いま、各ジョイントに対して加える力が、それぞれ独立な正規分布からサンプリングされると仮定しているので、MutliVariateNormDiag(共分散行列が対角行列になる多変量正規分布）を使ってモデリングしています。 distrax.Independentとdistrax.Normalを使っても同じことができます。 行動は一応tanhで$[-1, 1]$の範囲にならしています。 . ちょっと試してみましょう。 . ant = brax.envs.create(env_name=&quot;ant&quot;, batch_size=1) init, step = make_step_function(ant) initial_state = jax.jit(ant.reset)(next(prng_seq)) params = init(next(prng_seq), initial_state) _next_state, out, action = step(params, next(prng_seq), initial_state) # chexはテスト用のライブラリです chex.assert_shape((out.mean, out.stddev, action), (1, ant.action_size)) . というわけで無事にstepをJITコンパイルして高速化できました。 resetはほとんど呼ばないので別にコンパイルしなくてもいいのですが、jitしないとbraxがjnp.DeviceArrayのかわりにnumpyを使いたがって少し面倒なのでjitしています。 . あとはPPOを実装していきますが、時間の都合で手短かにいきます。 まずはGAEですね。 普通に書くとjax.jitがループアンローリングを行ってコンパイル時間が激遅になるので、jax.lax.fori_loopという黒魔術を使います。 コンパイル時定数はstatic_argnumsで指定します。 vmapで各ワーカー用に並列化します。 . import functools @functools.partial(jax.jit, static_argnums=2) def gae( r_t: JaxArray, discount_t: JaxArray, lambda_: float, values: JaxArray, ) -&gt; chex.Array: chex.assert_rank([r_t, values, discount_t], 1) chex.assert_type([r_t, values, discount_t], float) lambda_ = jnp.ones_like(discount_t) * lambda_ delta_t = r_t + discount_t * values[1:] - values[:-1] n = delta_t.shape[0] def update(i: int, advantage_t: JaxArray) -&gt; JaxArray: t_ = n - i - 1 adv_t = delta_t[t_] + lambda_[t_] * discount_t[t_] * advantage_t[t_ + 1] return jax.ops.index_update(advantage_t, t_, adv_t) advantage_t = jax.lax.fori_loop(0, n, update, jnp.zeros(n + 1)) return advantage_t[:-1] batched_gae = jax.vmap(gae, in_axes=(1, 1, None, 1), out_axes=1) . なんかブログで書くには黒魔術すぎる気もしますが...。 . 次に学習データのバッチを構成する部分ですね。 これは、普通にPyTorchとかと変わらないです。 . import dataclasses @chex.dataclass class RolloutResult: &quot;&quot;&quot; Required experiences for PPO. &quot;&quot;&quot; observations: t.List[JaxArray] actions: t.List[JaxArray] = dataclasses.field(default_factory=list) rewards: t.List[JaxArray] = dataclasses.field(default_factory=list) terminals: t.List[JaxArray] = dataclasses.field(default_factory=list) outputs: t.List[NetworkOutput] = dataclasses.field(default_factory=list) def append( self, *, observation: JaxArray, action: JaxArray, reward: JaxArray, output: NetworkOutput, terminal: JaxArray, ) -&gt; None: self.observations.append(observation) self.actions.append(action) self.rewards.append(reward) self.outputs.append(output) self.terminals.append(terminal) def clear(self) -&gt; None: self.observations = [self.observations[-1]] self.actions.clear() self.rewards.clear() self.outputs.clear() self.terminals.clear() class Batch(t.NamedTuple): &quot;&quot;&quot;Batch for PPO, also used as minibatch by indexing.&quot;&quot;&quot; observation: JaxArray action: JaxArray reward: JaxArray advantage: JaxArray value_target: JaxArray log_prob: JaxArray def __getitem__(self, idx: Array) -&gt; &quot;Batch&quot;: return self.__class__( observation=self.observation[idx], action=self.action[idx], reward=self.reward[idx], advantage=self.advantage[idx], value_target=self.value_target[idx], log_prob=self.log_prob[idx], ) @jax.jit def make_batch(rollout: RolloutResult, next_value: JaxArray) -&gt; Batch: action = jnp.concatenate(rollout.actions) mean, stddev, value = jax.tree_map(lambda *x: jnp.concatenate(x), *rollout.outputs) log_prob = distrax.MultivariateNormalDiag(mean, stddev).log_prob(action) reward = jnp.stack(rollout.rewards) mask = 1.0 - jnp.stack(rollout.terminals) value = jnp.concatenate( (value.reshape(reward.shape), next_value.reshape(1, -1)), axis=0, ) advantage = batched_gae(reward, mask * 0.99, 0.95, value) value_target = advantage + value[:-1] return Batch( observation=jnp.concatenate(rollout.observations[:-1]), action=action, reward=jnp.ravel(reward), advantage=jnp.ravel(advantage), value_target=jnp.ravel(value_target), log_prob=log_prob, ) . 普通のdataclassesはjitできないので、chex.dataclassを使います。 さっき少しだけ触れましたが、chex.dataclassは作成したdataclassをPyTreeとしてjaxに登録してくれます。 実はflax.struct.dataclassというだいたい同じものもあって、braxの内部ではこれを使っているようです。 また$ gamma = 0.99, lambda = 0.95$としました。 . いよいよ学習の部分ですね。 まず、損失関数をjax.gradできるように書きます。 . def ppo_loss(action_size: int, batch: Batch) -&gt; JaxArray: mean, stddev, value = policy_and_value(action_size, batch.observation) # Policy loss policy = distrax.MultivariateNormalDiag(mean, stddev) log_prob = policy.log_prob(batch.action) prob_ratio = jnp.exp(log_prob - batch.log_prob) clipped_ratio = jnp.clip(prob_ratio, 0.8, 1.2) clipped_obj = jnp.fmin(prob_ratio * batch.advantage, clipped_ratio * batch.advantage) policy_loss = -jnp.mean(clipped_obj) # Value loss value_loss = jnp.mean(0.5 * (value - batch.value_target) ** 2) # Entropy regularization entropy_mean = jnp.mean(policy.entropy(), axis=-1) return policy_loss + value_loss - 0.001 * entropy_mean . $ epsilon = 0.2$で固定しているので、$[1 - 0.2, 1 + 0.2]$の範囲でクリップします。 . ではこれを使って、今度はパラメタの更新を全部jitにつっこんでみましょう。 パラメタの更新にはoptaxというライブラリを使います。色々なSGDのバリアントを実装しているライブラリですが、僕はほとんどAdamしか使いません...。 . import optax def make_update_function( action_size: int, opt_update: optax.TransformUpdateFn, ) -&gt; t.Callable[..., t.Any]: # hk.Paramsを使い回すのでinitは捨てていい # 行動をサンプルしないので、without_apply_rngが使える _, loss_fn = hk.without_apply_rng(hk.transform(lambda batch: ppo_loss(action_size, batch))) grad_fn = jax.grad(loss_fn) # ここでjitしていい @jax.jit def update( params: hk.Params, opt_state: optax.OptState, batch: Batch, ) -&gt; t.Tuple[hk.Params, Batch]: grad = grad_fn(params, batch) updates, new_opt_state = opt_update(grad, opt_state) return optax.apply_updates(params, updates), new_opt_state return update . さて、ここまで来たらあと一歩ですね。 次に面倒ですが次の状態のvalueをとってきてバッチを作る部分を書きます。 . def make_next_value_function(action_size: int) -&gt; Batch: def next_value_fn(obs: JaxArray) -&gt; JaxArray: output = policy_and_value(action_size, obs) return output.value _, next_value_fn = hk.without_apply_rng(hk.transform(next_value_fn)) return jax.jit(next_value_fn) . 速度を求めるなら、これはmake_batchと一緒にjitしてしまってもいいですが、まあ面倒なのでこれでもいいでしょう。 . では材料がそろったのでメインループを書いていきましょう。 面倒ですが、評価用のenvironmentも別に作ります。 . try: import tqdm except ImportError as _e: ! pip install tqdm import tqdm clear_output() . import datetime from tqdm.notebook import trange def sample_minibatch_indices( n_instances: int, n_minibatches: int, prng_key: chex.PRNGKey, ) -&gt; t.Iterable[JaxArray]: indices = jax.random.permutation(prng_key, n_instances) minibatch_size = n_instances // n_minibatches for start in range(0, n_instances, minibatch_size): yield indices[start : start + minibatch_size] def train_ppo( env_name: str = &quot;ant&quot;, n_workers: int = 32, n_steps: int = 2048, n_training_steps: int = 10000000, n_optim_epochs: int = 10, n_minibatches: int = 64, eval_freq: int = 20, eval_workers: int = 16, seed: int = 0, ) -&gt; HTML: # 環境と、環境を含んだstep関数を作る env = brax.envs.create(env_name=env_name, episode_length=1000, batch_size=n_workers) eval_env = brax.envs.create( env_name=env_name, episode_length=1000, batch_size=eval_workers, ) network_init, step = make_step_function(env) _, eval_step = make_step_function(eval_env) eval_reset = jax.jit(eval_env.reset) # 乱数 prng_seq = hk.PRNGSequence(seed) # 初期状態 state = jax.jit(env.reset)(rng=next(prng_seq)) rollout = RolloutResult(observations=[state.obs]) # Optimizerとパラメタを初期化する optim = optax.chain(optax.clip_by_global_norm(1.0), optax.adam(3e-4, eps=1e-4)) update = make_update_function(env.action_size, optim.update) params = network_init(next(prng_seq), state) opt_state = optim.init(params) # next_value next_value_fn = make_next_value_function(env.action_size) n_instances = n_workers * n_steps def evaluate(step: int) -&gt; None: eval_state = eval_reset(rng=next(prng_seq)) return_ = jnp.zeros(eval_workers) done = jnp.zeros(eval_workers, dtype=bool) for _ in range(1000): eval_state, _, _ = eval_step(params, next(prng_seq), eval_state) return_ = return_ + eval_state.reward * (1.0 - done) done = jnp.bitwise_or(done, eval_state.done.astype(bool)) print(f&quot;Step: {step} Avg. ret: {jnp.mean(return_).item()}&quot;) for i in trange(n_training_steps // n_instances): for _ in range(n_steps): state, output, action = step(params, next(prng_seq), state) rollout.append( observation=state.obs, action=action, reward=state.reward, output=output, terminal=state.done, ) next_value = next_value_fn(params, state.obs) batch = make_batch(rollout, next_value) rollout.clear() # Batchを作ったので、ミニバッチサンプリングして学習 for _ in range(n_optim_epochs): for idx in sample_minibatch_indices( n_instances, n_minibatches, next(prng_seq), ): minibatch = batch[idx] params, opt_state = update(params, opt_state, minibatch) # 時々評価する if (i + 1) % eval_freq == 0: evaluate(i + 1) evaluate(i + 1) # Visualize eval_state = eval_reset(rng=next(prng_seq)) qps = [] while eval_state.done[0] == 0.0: eval_state, _, _ = eval_step(params, next(prng_seq), eval_state) qps.append(jax.tree_map(lambda qp: qp[0], eval_state.qp)) return render_html(eval_env.sys, qps) start_time = datetime.datetime.now() html = train_ppo() elapsed = datetime.datetime.now() - start_time print(f&quot;Train completed after {elapsed.total_seconds() / 60:.2f} min.&quot;) html . Step: 20 Avg. ret: -286.34039306640625 Step: 40 Avg. ret: -273.5491943359375 Step: 60 Avg. ret: -193.0821990966797 Step: 80 Avg. ret: -84.20954132080078 Step: 100 Avg. ret: -45.654090881347656 Step: 120 Avg. ret: -20.323640823364258 Step: 140 Avg. ret: -42.74524688720703 Step: 152 Avg. ret: -5.514527320861816 Train completed after 41.34 min. . brax visualizer . . 100万ステップの訓練が41分で終わりました。速いですねやっぱり。 なんか前に跳ねすぎている微妙なのがビジュアライズされていますが...。 . 6. &#12414;&#12392;&#12417; . というわけで、このブログではJax、Brax、Haikuを使って、GPUだけでシミュレータ上のロボットを訓練する例を示しました。 かなり駆け足の解説になりましたが、なんとなくプログラムの組み方を理解していただけたのではないかと思います。 . 総括すると、Jaxはかなり広い範囲のNumPy演算をGPU/TPU上で高速に動作するコードに変換できる、非常に強力なライブラリです。 今回紹介したvmapは、例えば一つのGPU上で演算を並列化する機能ですが、他にもpmapによるデバイスをまたいだ並列化もできます。 ですから特に、 . ✔ CPUとGPUの通信オーバーヘッドが気になるとき | ✔ 大規模に並列なシミュレーションを行いたいとき | . は、Jaxが効果を発揮すると思います。また、jax.lax.fori_loopを使って . ✔ CythonやC++/Rustなど他の言語を使わずにPythonのループを高速化したいとき | . にも使えます。 一方で、単に深層学習を高速化したい場合、例えば . 🔺 PyTorchモデルの訓練がボトルネックになっている場合 | . などは、JaxやHaiku/Flaxを使うことによる高速化の恩恵はあまりないと思います。 うまくJitを使えばJaxの方が速いと思いますが、PyTorchのCUDAコードはかなり速いですからね。また、PyTorchと比較した際、 . 🔺 学習コストについてもJaxの方が大きい | . のではないかと思います。 なので、個人的には学習以外の部分でベクトル並列化・Jitコンパイルによる並列化の余地がある場合に、Jaxは便利に使えるのかなあと思います。 ただDeepmindはAlphaFold2を始め、多くのJax+Haiku製深層学習コードをリリースしていますし、一応読める程度に親しんでおくだけでもある程度のメリットはあると思います。 . さて、冒頭の大規模にシミュレーションしてSim2Realを頑張るのか、実機のデータで頑張るのかという話に戻りますが、シミュレーションをスケールさせたいのであれば大規模にシミュレーションしたいならBraxのように「シミュレーターをJaxでコンパイルできるように作る」というアプローチは面白いと思います。分子動力学計算など、物理演算以外のシミュレーションへの活用も期待されます（と聞いたことがあります。僕は分子動力学計算が何なのかよくわかりません...）。 一方で、シミュレータが微分可能であるという利点をどう活かすのかも興味深いテーマです。僕も以前PFNさんのインターンで、報酬が微分可能なシミュレータを使って、$ sum_{t = 0}^T frac{ partial r_t}{ partial theta}$についての山登り法で方策を更新するのを試したことがあるのですが、報酬が遠いとなかなか難しいなあという印象でした。うまい方法があればいいのですが...。意外と勾配降下だけでなく進化計算などのメタヒューリスティクスと組み合わせると面白いかもしれないです。 . さて、僕はもう一つアドベントカレンダーの記事を書く予定があったのですが、時間がないので他の人に代わってもらうかもしれません...。出たらそちらもよろしくお願いします。 .",
            "url": "https://kngwyu.github.io/rlog/ja/2021/12/18/jax-brax-haiku.html",
            "relUrl": "/ja/2021/12/18/jax-brax-haiku.html",
            "date": " • Dec 18, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "より良い問題設計へ向けて： 何が強化学習を難しくするのかを理解しよう",
            "content": "1. &#12399;&#12376;&#12417;&#12395; . . Note: このブログは強化学習苦手の会 Advent Calendar 202020日目の記事として書かれました . 強化学習は、逐次的に意思決定する問題を定義するためのフレームワークです。 従来は大変だった1 ニューラルネットワークの訓練が簡単になったことや、 Alpha GOなど深層強化学習(Deep RL)の成功を背景に、対話システム・経済 など、様々なアプリケーションで強化学習の利用が試みられているように思います。 僕個人としても、強化学習は汎用的で面白いツールだと思うので、将来的には色々な応用分野で広く活用されるといいな、と思います。 . 一方で、強化学習を何か特定の問題に応用してみよう、という場面では、 その汎用性ゆえかえってとっつきにくい・扱いづらい面があるように思います。 実際に、苦手の会などで応用研究をされている方から、 . 問題を定義するのがそもそも大変 | 色々な手法があって、何がなんだかよくわからない | . などの意見を観測できました。 . では応用研究に対する「ツール」として強化学習を扱う上で何が大事なのだろう、と考えたとき、 僕は簡単な問題を設計することこそが大事だという仮説に思いいたりました。 簡単な問題を設計するためには、強化学習の中でもどういう問題が難しいのか、 ということをきちんと理解しておく必要があるように思います。 . そこでこのブログ記事では、強化学習の中でも「難しい問題」がどういうものなのか、 そういう問題はなぜ難しいのかについて、例を通してなるべく直感的に説明することを試みます。 強化学習の難しさがわかった暁には、きっと . そもそも強化学習を使わないという選択ができるし、 | なるべく簡単に解けるような強化学習の問題を設計できるし、 | 問題に合わせて手法を選択できる | . ことでしょう。 . 記事の構成として、強化学習の難しさについて「場合分け」を行い、 . MDPを解くことの難しさ | データを収集することの難しさ | . という2つの観点から整理していきます。 . 前提知識について、初心者の方でも読めるように、 強化学習についての知識についてはなるべく記事の中で補足します。 しかし、すごく雑に書くので、詳細はReinforcement Learning: An Introduction などの教科書を参照されるといいと思います。 また、プログラムを見たほうがイメージしやすい（方もいる）かと思ってPythonのコード例をたまに出しています。 コード例では、$ sum_{s} f(s) g(s, a)$のようにテンソルの適当な軸で掛け算して足し込む演算に numpy.einsum を多用しているので、知っていたほうが読みやすいかもしれません。 . . Suttonも誤差逆伝播を使うのにはトリッキーな工夫が必要だと言っています。&#8617; . | 2. &#26368;&#36969;&#21270;&#21839;&#38988;&#12392;&#12375;&#12390;&#12398;&#20596;&#38754;: MDP&#12434;&#35299;&#12367;&#12371;&#12392;&#12398;&#38627;&#12375;&#12373; . 強化学習は、エージェントが集めてきたデータを使って、マルコフ決定過程上で「いい感じ」に行動する方策を 訓練する問題のことです。 しかしこの章では、いったん「学習」のことは忘れましょう。 学習しないでいいのは、環境の情報を全部知っているからです。 このとき、どのくらい問題を解くのが大変なのでしょうか？ . ではまず、マルコフ決定過程 (Markov Decision Process, 以後MDPと呼ぶ)を導入します。 . . 定義1: MDP . MDPは、 状態集合 $ mathcal{S}$、 行動集合 $ mathcal{A}$、 状態遷移確率 $P: mathcal{S} times mathcal{A} rightarrow mathcal{S}$, 報酬関数 $r: mathcal{S} times mathcal{A} rightarrow mathbb{R}$1, 割引報酬率 $0 leq gamma &lt; 1$ から成る . . なんだか、記号で言われてもよくわからないですね。 せっかくなので、お絵描きしてみましょう。 . . この報酬関数は最も簡単な定義です。他に$r: mathcal{S} times mathcal{A} times mathcal{S} rightarrow mathbb{R}$(遷移先に依存)、$r: mathcal{S} times mathcal{A} rightarrow mathbb{E}[R_{s, a}]$（確率的）があります。&#8617; . | from typing import Dict, List, Optional, Sequence, Tuple, TypeVar import numpy as np from matplotlib import pyplot as plt from matplotlib.axes import Axes from matplotlib.text import Annotation # Some types for annotations T = TypeVar(&quot;T&quot;) class Array(Sequence[T]): pass Array1 = Array[float] Array2 = Array[Array1] Array3 = Array[Array2] Point = Tuple[float, float] def a_to_b( ax: Axes, a: Point, b: Point, text: str = &quot;&quot;, style: str = &quot;normal&quot;, **kwargs, ) -&gt; Annotation: &quot;&quot;&quot;Draw arrow from a to b. Optionally&quot;&quot;&quot; STYLE_ALIASES: Dict[str, str] = { &quot;normal&quot;: &quot;arc3,rad=-0.4&quot;, &quot;self&quot;: &quot;arc3,rad=-1.6&quot;, } arrowkwargs = {} for arrowkey in list(filter(lambda key: key.startswith(&quot;arrow&quot;), kwargs)): arrowkwargs[arrowkey[5:]] = kwargs.pop(arrowkey) if len(text) &gt; 0: bbox = dict( boxstyle=&quot;round&quot;, fc=&quot;w&quot;, ec=arrowkwargs.get(&quot;color&quot;, &quot;k&quot;), alpha=arrowkwargs.get(&quot;alpha&quot;, 1.0), ) else: bbox = None return ax.annotate( text, xy=b, xytext=a, arrowprops=dict( shrinkA=10, shrinkB=10, width=1.0, headwidth=6.0, connectionstyle=STYLE_ALIASES.get(style, style), **arrowkwargs, ), bbox=bbox, **kwargs, ) class ChainMDP: &quot;&quot;&quot;Chain MDP with N states and two actions.&quot;&quot;&quot; ACT_COLORS: List[str] = [&quot;xkcd:vermillion&quot;, &quot;xkcd:light royal blue&quot;] INTERVAL: float = 1.2 OFFSET: float = 0.8 SHIFT: float = 0.5 HEIGHT: float = 4.0 def __init__( self, success_probs: Sequence[Sequence[float]], reward_function: Sequence[Sequence[float]], ) -&gt; None: success_probs = np.array(success_probs) self.n_states = success_probs.shape[0] assert success_probs.shape[1] == 2 np.testing.assert_almost_equal(success_probs &gt;= 0, np.ones_like(success_probs)) np.testing.assert_almost_equal(success_probs &lt;= 1, np.ones_like(success_probs)) self.p = np.zeros((self.n_states, 2, self.n_states)) for si in range(self.n_states): left, right = max(0, si - 1), min(self.n_states - 1, si + 1) # Action 0 is for right self.p[si][0][right] += success_probs[si][0] self.p[si][0][si] += 1.0 - success_probs[si][0] # Action 1 is for left self.p[si][1][left] += success_probs[si][1] self.p[si][1][si] += 1.0 - success_probs[si][1] self.r = np.array(reward_function) # |S| x 2 assert self.r.shape == (self.n_states, 2) # For plotting self.circles = [] self.cached_ax = None def figure_shape(self) -&gt; Tuple[int, int]: width = self.n_states + (self.n_states - 1) * self.INTERVAL + self.OFFSET * 2.5 height = self.HEIGHT return width, height def show(self, title: str = &quot;&quot;, ax: Optional[Axes] = None) -&gt; Axes: if self.cached_ax is not None: return self.cached_ax from matplotlib.patches import Circle width, height = self.figure_shape() circle_position = height / 2 - height / 10 if ax is None: fig = plt.figure(title or &quot;ChainMDP&quot;, (width, height)) ax = fig.add_axes([0, 0, 1, 1], aspect=1.0) ax.set_xlim(0, width) ax.set_ylim(0, height) ax.set_xticks([]) ax.set_yticks([]) def xi(si: int) -&gt; float: return self.OFFSET + (1.0 + self.INTERVAL) * si + 0.5 self.circles = [ Circle((xi(i), circle_position), 0.5, fc=&quot;w&quot;, ec=&quot;k&quot;) for i in range(self.n_states) ] for i in range(self.n_states): x = self.OFFSET + (1.0 + self.INTERVAL) * i + 0.1 ax.text(x, height * 0.85, f&quot;State {i}&quot;, fontsize=16) def annon(act: int, prob: float, *args, **kwargs) -&gt; None: # We don&#39;t hold references to annotations (i.e., we treat them immutable) a_to_b( ax, *args, **kwargs, arrowcolor=self.ACT_COLORS[act], text=f&quot;P: {prob:.02}&quot;, arrowalpha=prob, fontsize=11, ) for si in range(self.n_states): ax.add_patch(self.circles[si]) x = xi(si) # Action 0: y = circle_position + self.SHIFT if si &lt; self.n_states - 1 and 1e-3 &lt; self.p[si][0][si + 1]: p_right = self.p[si][0][si + 1] annon( 0, p_right, (x + self.SHIFT, y), (xi(si + 1) - self.SHIFT * 1.2, y - self.SHIFT * 0.3), verticalalignment=&quot;center_baseline&quot;, ) else: p_right = 0.0 if p_right + 1e-3 &lt; 1.0: annon( 0, 1.0 - p_right, (x - self.SHIFT * 1.2, y), (x + self.SHIFT * 0.5, y - self.SHIFT * 0.1), style=&quot;self&quot;, verticalalignment=&quot;bottom&quot;, ) ax.text( x - self.SHIFT * 1.2, y + self.SHIFT * 1.4, f&quot;r({si}, 0): {self.r[si][0]:+.02}&quot;, color=self.ACT_COLORS[0], fontsize=14, ) # Action 1: y = circle_position - self.SHIFT if 0 &lt; si and 1e-3 &lt; self.p[si][1][si - 1]: p_left = self.p[si][1][si - 1] annon( 1, self.p[si][1][si - 1], (x - self.SHIFT * 1.6, y), (xi(si - 1) + self.SHIFT * 1.4, y + self.SHIFT * 0.2), verticalalignment=&quot;top&quot;, ) else: p_left = 0.0 if p_left + 1e-3 &lt; 1.0: annon( 1, 1.0 - p_left, (x + self.SHIFT * 0.4, y), (x - self.SHIFT * 0.45, y + self.SHIFT * 0.1), style=&quot;self&quot;, verticalalignment=&quot;top&quot;, ) ax.text( x - self.SHIFT * 1.2, y - self.SHIFT * 1.4, f&quot;r({si}, 1): {self.r[si][1]:+.02}&quot;, color=self.ACT_COLORS[1], fontsize=14, ) for i in range(2): ax.plot([0.0], [0.0], color=self.ACT_COLORS[i], label=f&quot;Action {i}&quot;) ax.legend(fontsize=11, loc=&quot;upper right&quot;) if len(title) &gt; 0: ax.text(0.06, height * 0.9, title, fontsize=18) self.cached_ax = ax return ax . . mdp1 = ChainMDP( [[0.6, 0.7], [0.2, 0.4], [0.3, 0.7]], [[0.6, 0.2], [0.0, 0.1], [-0.1, 0.5]] ) ax = mdp1.show(title=&quot;MDP1&quot;) . MDPはこのように、状態を頂点とするグラフとして表現できます。 このMDPには3つの状態(図中のState 0, 1, 2)があり、各状態で、2つの行動のどちらかを選択できます。 よって $ mathcal{S} = {s_0, s_1, s_2 }, mathcal{A} = {a_0, a_1 }$です。 各行動は、以下のように特徴づけられます。 . 赤の行動(Action 0) をとると、一つ右の頂点へ遷移するか、失敗して今の頂点にとどまる | 青の行動(Action 1) をとると、一つ左の頂点へ遷移するか、失敗して今の頂点にとどまる | . このような確率的なグラフの上で、ある行動方針 $ pi: mathcal{S} times mathcal{A} rightarrow mathbb{R}$ に従って行動するエージェントを考えます。 行動方針が決まれば、貰える割引報酬和の期待値が、以下のように定まります。 これを状態価値と呼びます。 . . 定義2: 状態価値関数 $V^ pi$を以下のように定義する。 . $V^ pi(s) := sum_{a in mathcal{A}} pi(a|s) left( r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s, a) V^ pi(s&#39;) right)$ . . この時、強化学習は $ sum_{s in mathcal{S}} V^ pi(s)$ を最大化するような $ pi$を求める問題になります。1 そのような $ pi$ を 最適方策 $ pi^*$ と呼びます。また、$V^{ pi^*}$ を略記して $V^*$ と書きます。 . 手始めに、簡単な問題で $V^*$ を求めてみましょう。 . . 初期状態分布(雑に言うと、スタート地点の分布)を$ mu(s)$とすると、$ sum_{s in mathcal{S}} mu(s)V_ pi(s)$ がエージェントが獲得する割引報酬和の期待値です。$V_ pi(s)$ が最大ならこれも最大になります。&#8617; . | mdp2 = ChainMDP( [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0]], [[0.0, 0.0], [0.0, 0.0], [1.0, 0.0]] ) _ = mdp2.show(&quot;MDP2&quot;) . . この問題の最適方策 $ pi^*$ は、明らかですね。 一番右の状態でしか報酬が出ないので、全ての状態において右に行けばいいです。 . また、この時の $V^*$ も求めてみます。 . まず、状態2は 吸引状態 であり、状態2に辿りついてしまったらもう他の状態には遷移できません。 よって、 . $$ begin{aligned} V^*(2) &amp;= 1.0 + gamma V^*(2) 　 &amp;= 1.0 + gamma (1.0 + gamma V^*(2)) &amp;= 1.0 + gamma + gamma^2 + gamma^3 + ... &amp;= lim_{n to infty} frac{1 - gamma ^ n}{1 - gamma} &amp;= frac{1}{1 - gamma} ~~~~~( gamma &lt; 1より) end{aligned} $$となります。 たとえば $ gamma = 0.9$ なら、 . $$ begin{aligned} V^*(2) &amp;= frac{1}{1 - gamma} = 10 V^*(1) &amp;= 0.0 + gamma * V^*(2) = 9 V^*(0) &amp;= 0.0 + gamma * V^*(1) = 8.1 end{aligned} $$ですね。 . この例から、$ pi$にもとづいて行動した時、吸引状態以外のループにはまりえないなら、 $O(| mathcal{S}|^2| mathcal{A}|)$ (状態数の2乗と行動数に比例するオーダー)で $V^ pi$ が評価できることがわかります。 この場合は$ pi$も$P$も決定的なので実質$O(|S|)$になっています。 . 2.1: $V^ pi cdot V^*$&#12434;&#35413;&#20385;&#12377;&#12427;&#38627;&#12375;&#12373; . では、もう少し難しいMDPを考えてみましょう。 . mdp3 = ChainMDP( [[1.0, 0.0], [0.8, 1.0], [1.0, 0.9]], [[0.0, 0.0], [0.5, 0.0], [0.0, 1.0]] ) _ = mdp3.show(&quot;MDP3&quot;) . . 今度は、State 1で右に、State 2で左に行けば良さそうです。 $$ begin{aligned} V^* (1) = 0.5 + gamma (0.1 * V^*(1) + 0.9 * V^*(2)) V^* (2) = 1.0 + gamma (0.8 * V^*(1) + 0.2 * V^*(2)) end{aligned} $$ . 先ほどの問題と違って1も2も吸引状態ではないので、$V(1)$と$V(2)$がお互いに依存する面倒な 方程式が出てきてしまいました。 このようなループの存在が、強化学習を難しくしている要素の一つです。 . とはいえ、コンピューターで数値的に解くのは簡単です。 状態$s$にいて、あと$n$回行動できる時の価値関数を$V_n^ pi(s)$と書きます。 任意の$s$について、$V_0^ pi(s) = 0$です（1回も行動できないので!）。 $V_i^ pi$ から $V_{i + 1}^ pi$ を求めるには、1ステップだけ先読みすればいいので、 $$ V_{i + 1}^ pi(s) = sum_{a in mathcal{A}} pi(a|s) left( r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s, a) V_{i}^ pi(s&#39;) right) $$ で計算できます。$ gamma &lt; 1$によりこの反復計算は収束し、$V^ pi$ が求まります。 実際にプログラムで書いてみましょう。 . MAX_ITER_V_PI: int = int(1e5) def v_pi( r: Array2, p: Array3, pi: Array2, gamma: float, epsilon: float, ) -&gt; Tuple[Array1, int]: v = np.zeros(r.shape[0]) # Vπ r_pi = np.einsum(&quot;sa,sa-&gt;s&quot;, pi, r) # |S|, πを使ったときに貰う報酬のベクトル p_pi = np.einsum(&quot;saS,sa-&gt;sS&quot;, p, pi) # |S| x |S|, πを使ったときの状態遷移確率 for n_iter in range(MAX_ITER_V_PI): v_next = r_pi + gamma * np.einsum(&quot;s,sS&quot;, v, p_pi.T) if np.all(np.absolute(v_next - v) &lt; epsilon): return v_next, n_iter + 1 v = v_next # 理論的には必ず収束するので、バグ予防 raise RuntimeError(&quot;Policy Evaluation did not converge &gt;_&lt;&quot;) pi_star = np.array([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0]]) v_star_mdp3, n_iter = v_pi(mdp3.r, mdp3.p, pi_star, gamma=0.9, epsilon=1e-4) print(f&quot;反復回数: {n_iter}&quot;) print(&quot; &quot;.join([f&quot;V({i}): {v:.3}&quot; for i, v in enumerate(v_star_mdp3)])) . 反復回数: 86 V(0): 6.49 V(1): 7.21 V(2): 7.51 . 86回この計算を反復した後、なんかそれっぽい数字が出てきました。 この反復回数は、何によって決まるのでしょうか？ . 任意の $s$ について $|V_{i+1}^ pi(s) - V_i^ pi(s)| &lt; epsilon$ なら計算終わり、とします1。 $V_n^ pi(s)$は「あと$n$ステップ行動できる時の状態価値の期待値」なので、$i$ ステップ目にもらった報酬を $R_i$とすると、 $$ V_n^ pi(s) = mathbb{E}_{s, pi} left[ R_1 + gamma R_2 + gamma^2 R_3 + ... right] $$ と書けます。 なので、報酬の範囲が$0 leq R_t &lt; R_ textrm{max}$だと仮定すると、 $ gamma^{k - 1} R_ textrm{max} &lt; epsilon$ならこの数値計算が収束することがわかります。 簡単のため$R_ textrm{max}=1$としてみると、$k$が満たすべき条件は $$ gamma^{k-1} &lt; epsilon Leftrightarrow k &lt; frac{ log epsilon}{ log gamma} + 1 $$ となります。 コードの中で $ gamma = 0.9, epsilon = 0.0001$ としたので、たかだか89回の反復で収束することがわかります。 実験結果では86回だったので、だいたい同じくらいですね。 . よって、$V^ pi$を反復法により評価した時、その反復回数は報酬・$ epsilon$・$ gamma$に依存することがわかりました。 報酬と$ epsilon$には$ log$のオーダーでしか依存しないのに対し、$ gamma$に対しては $O((- log gamma)^{-1})$のオーダーで依存していることに注意してください。 試しに、$ epsilon=0.0001$の時の$ frac{ log epsilon}{ log gamma}$をプロットしてみましょう。 . . この$ epsilon$は$ epsilon$-Optimal Policyの$ epsilon$ではありません。&#8617; . | このように、$ gamma$が大きくなると一気に反復回数が増えることがわかります。 また、この数値計算が収束した時、真の$V^ pi$との差が $$ begin{aligned} V^ pi(s) - V_k^ pi(s) &amp;= mathbb{E}_{s, pi} left[ gamma^k R_{k + 1} + gamma^{k + 1} R_{k + 2} ... right] &amp;&lt; frac{ gamma^k R_ textrm{max}}{1 - gamma} &lt; frac{ gamma epsilon}{1 - gamma} end{aligned} $$ で抑えられることもわかります。 . 次は、いきなり$V^*$を求めてみましょう。 $V^ pi$を求めた時と同じように、状態$s$にいて、 あと$n$回行動できる時の最適価値関数を$V_n^*(s)$と書きます。 先ほどと同様に、$V_i^*$から1ステップ先読みして$V^*_{i + 1}$を求めます。 残り$i + 1$ステップある時、 $r(s, a) + sum_{s&#39; in mathcal{S}} P(s&#39;|s, a) V_i^*(s)$ が最大になるような行動$a$を選ぶのが最適です。 ですから、$V^*_{i + 1}$は $$ V_{i + 1}^*(s) = max_a left( r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s, a) V_{i}^ pi(s&#39;) right) $$ で求められます。 さっきより簡単な式になってしまいました。 プログラムで書いてみます。 . MAX_ITER_VI: int = int(1e6) def value_iteration( r: Array2, p: Array3, gamma: float, epsilon: float, ) -&gt; Tuple[Array1, int]: v = np.zeros(p.shape[0]) for n_iter in range(MAX_ITER_VI): # これ↓はQ Valueとも言います r_plus_gamma_pv = r + gamma * np.einsum(&quot;saS,S-&gt;sa&quot;, p, v) v_next = r_plus_gamma_pv.max(axis=1) if np.all(np.absolute(v_next - v) &lt; epsilon): return v_next, n_iter + 1 v = v_next raise RuntimeError(&quot;Value Iteration did not converge &gt;_&lt;&quot;) v_star_mdp3_vi, n_iter = value_iteration(mdp3.r, mdp3.p, 0.9, 1e-4) print(f&quot;反復回数: {n_iter}&quot;) print(&quot; &quot;.join([f&quot;V({i}): {v:.3}&quot; for i, v in enumerate(v_star_mdp3_vi)])) . 反復回数: 86 V(0): 6.49 V(1): 7.21 V(2): 7.51 . 先程と同じく、86回の反復で$V^*$が求まりました。 この反復回数も、先ほどの$V^ pi$と同じように$ gamma, epsilon$を用いて抑えられます。 . しかし、$ epsilon$は人手で設定するパラメタです。 最適方策が求まれば$V^*$は大して正確でなくとも困らないという場合は、もっと$ epsilon$を大きくして、 計算を早く終わらせたい気がします。 では、「どんな場合に$ epsilon$を大きくできるか」を考えてみましょう。 . 簡単のため、 $Q^ pi(s, a) = r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s, a) V^ pi(s&#39;)$ (QはQualityのQらしい)を導入します。 残り$k$ステップある時の最適行動を$a_k^* = textrm{argmax}_a Q_k^*(s, a)$とします。 すると、$k+1$ステップ目以降の割引報酬和は $ frac{ gamma^{k}R_ textrm{max}}{1 - gamma}$で抑えられるので、 $$ Q_k^*(s, a_k^*) - max_{a neq a_k^*} Q_k^*(s, a) &lt; frac{ gamma^k R_ textrm{max}}{1 - gamma} $$ が成り立つなら、$a_k^*$が今後他の行動に逆転されることはありません。 なので$a_k^*$が最適でいいよね、はいこの話終わり、ということになります。 以下略記して $A_ textrm{min}^*(s, a_k) = Q_k^*(s, a_k^*) - max_{a neq a_k^*} Q_k^*(s, a)$ と書きます（他の行動に対するアドバンテージの最小値という意味）。 いま$ gamma^{k-1} R_ textrm{max}&lt; epsilon$が終了条件なので、 $$ A_ textrm{min}^*(s, a_k) &lt; frac{ epsilon gamma}{1 - gamma} Leftrightarrow frac{A_ textrm{min}^*(s, a_k)(1 - gamma)}{ gamma}&lt; epsilon $$ が成り立ちます。 これが意味するのは、$V*$と二番目にいい$Q^*(s, a)$との差が大きいほど$ epsilon$を大きくできるということです。 . ここまでの議論から、計算量の観点では、 . $ gamma$が大きいほどMDPを解くのが難しい | 最適解と二番目にいい解との差が小さいほどMDPを解くのが難しい | . という2点が言えそうですね。 . 2.2: &#26041;&#31574;&#26368;&#36969;&#21270;&#12398;&#38627;&#12375;&#12373; . 前節で用いた再帰的な数値計算は動的計画法(DP)と呼ばれるものです。 Q学習など、多くの強化学習アルゴリズムがDPをもとにしています。 一方で、単に強化学習をブラックボックス最適化だと考えることもできます。 特に、方策パラメタ$ theta$を最適化して解く方法を方策最適化と呼びます。 . いま、$ pi(0|s) = theta(s), pi(1|s) = 1.0 - theta(s)$によって$ pi$をパラメタ$ theta$により表すことにします （これをdirect parameterizationと呼びます）。 ためしに、先ほどのMDP3で$ pi(0|0)=1.0$を固定して、$ theta(1), theta(2)$を動かした時の$ sum_{s in mathcal{S}} V^ pi(s)$の変動をプロットしてみましょう。 . def v_pi_sum_2dim( r: Array2, p: Array3, gamma: float, epsilon: float, initial_pi: Array2, states: Tuple[int, int], n_discretization: int, ) -&gt; Array2: res = [] for i2 in range(n_discretization + 1): p2 = (1.0 / n_discretization) * i2 for i1 in range(n_discretization + 1): p1 = (1.0 / n_discretization) * i1 pi = initial_pi.copy() pi[states[0]] = p1, 1 - p1 pi[states[1]] = p2, 1 - p2 res.append(v_pi(r, p, pi, gamma, epsilon)[0].sum()) return np.array(res).reshape(n_discretization + 1, -1) def plot_piv_heatmap( data: Array2, xlabel: str = &quot;&quot;, ylabel: str = &quot;&quot;, title: str = &quot;&quot;, ax: Optional[Axes] = None, ) -&gt; Axes: from matplotlib.ticker import LinearLocator if ax is None: fig = plt.figure() ax = fig.add_subplot(111, projection=&quot;3d&quot;) n_discr = data.shape[0] x, y = np.meshgrid(np.linspace(0, 1, n_discr), np.linspace(0, 1, n_discr)) ax.plot_surface(x, y, data, cmap=&quot;inferno&quot;, linewidth=0, antialiased=False) ax.zaxis.set_major_locator(LinearLocator(10)) ax.zaxis.set_major_formatter(&#39;{x:.01f}&#39;) ax.set_xlabel(xlabel, fontsize=14) ax.set_ylabel(ylabel, fontsize=14) ax.set_zlabel(&quot;∑Vπ&quot;, fontsize=14, horizontalalignment=&quot;right&quot;) ax.set_title(title, fontsize=15) return ax initial_pi = np.array([[1.0, 0.0], [0.5, 0.5], [0.5, 0.5]]) v_pi_sums = v_pi_sum_2dim(mdp3.r, mdp3.p, 0.9, 1e-4, initial_pi, (1, 2), 20) ax = plot_piv_heatmap(v_pi_sums, &quot;θ(1)&quot;, &quot;θ(2)&quot;, &quot;MDP3&quot;) _ = ax.set_xlim(tuple(reversed(ax.get_xlim()))) _ = ax.set_ylim(tuple(reversed(ax.get_ylim()))) . . なんかいい感じに山になっていますね。 この問題の場合は、山登り法（勾配上昇法）で$ theta$を更新していけば大域解 $ theta(1) = 0.0, theta(2) = 1.0$にたどり着きそうです1。 . しかし、$f( theta) = sum_{s in mathcal{S}} V^{ pi_ theta}(s)$は、いつもこのような性質 のいい関数になっているのでしょうが？ 結論から言うとそうではないです。 例えば、以下のようなMDPではどうでしょうか？($ gamma=0.95$にしています) . . この記事では勾配の導出については一切触れないので、別途資料などを参照してください。&#8617; . | mdp4 = ChainMDP( [[1.0, 0.0], [0.6, 0.9], [0.9, 0.6], [1.0, 1.0]], [[0.0, 1.0], [0.0, 0.0], [0.0, 0.0], [0.9, 0.0]], ) width, height = mdp4.figure_shape() fig = plt.figure(&quot;MDP4-pi-vis&quot;, (width * 1.25, height)) mdp_ax = fig.add_axes([0.42, 0.0, 1.0, 1.0]) _ = mdp4.show(&quot;MDP4&quot;, ax=mdp_ax) pi_ax = fig.add_axes([0.0, 0.0, 0.4, 1.0], projection=&quot;3d&quot;) initial_pi = np.array([[0.0, 1.0], [0.5, 0.5], [0.5, 0.5], [1.0, 0.0]]) v_pi_sums = v_pi_sum_2dim(mdp4.r, mdp4.p, 0.95, 1e-4, initial_pi, (1, 2), 24) _ = plot_piv_heatmap(v_pi_sums, &quot;θ(1)&quot;, &quot;θ(2)&quot;, ax=pi_ax) print( f&quot;f(θ(1) = 0.0, θ(2) = 0.0): {v_pi_sums[0][0]} n&quot; f&quot;f(θ(1) = 0.5, θ(2) = 0.5): {v_pi_sums[12][12]} n&quot; f&quot;f(θ(1) = 1.0, θ(2) = 1.0): {v_pi_sums[24][24]}&quot; ) . . f(θ(1) = 0.0, θ(2) = 0.0): 74.25901721830479 f(θ(1) = 0.5, θ(2) = 0.5): 72.01388270994806 f(θ(1) = 1.0, θ(2) = 1.0): 70.6327625115528 . 一番右だと永遠に0.9がもらえて、一番左だと1.0がもらえるので、より最適方策を見分けるのが難しそうな感じがします。 . プロットしてみると、$f( theta)$は先程とは逆に谷のような形になっていて、山登り法で解いても 必ずしも大域解に収束しなそうに見えます。 これをもっと専門的な言葉で言うと、$f(0.0) + f(1.0) &gt; 2 * f(0.5)$よりこれは凹関数ではありません。 あまり詳しく説明しませんが、凹関数だと山登り法が大域解に収束するなど嬉しい点があるので、 これは最適化する上で厄介な特徴だと言えます。 . 以上より、方策最適化で問題を解く時は$ sum_{s in mathcal{S}} V(s)$が凹関数かどうかが、 問題の難しさに影響を与えそうだということがわかりました。 . 2.A &#26041;&#31574;&#21453;&#24489;&#27861;&#12398;&#38627;&#12375;&#12373; . . Note: この節は特に内容がないのでアペンディクス扱いになっています。飛ばしても問題ありません。 . ところで2.1で$V^*$を求めたときに使った手法を価値反復法と言います。 もう一つ、方策反復法という手法で$V^*$を求めることができます。 . $ pi^*$が満たすべき性質について考えてみます。 $ pi$が最適であるとき、 $$ V^ pi(s) geq max_{a in mathcal{A}} r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s,a) V^ pi(s&#39;) $$ が成り立ちます。 これが成り立たないとすると、 $$ pi&#39;(s, a) = begin{cases} 1.0 &amp;( textrm{if}~a = textrm{argmax}_a r(s, a) + gamma sum_{s&#39; in mathcal{S}} P(s&#39;|s,a) V^ pi(s&#39;)) 0.0 &amp;( textrm{otherwise}) end{cases} $$ の方が性能が良くなり、$ pi$が最適であることと矛盾します。 . では、この性質が成り立つまで方策を改善し続けるというアルゴリズムを試してみましょう。 さっき書いたv_pi関数を使って実装できます。 . MAX_ITER_PI: int = 10000 def policy_iteration( r: Array2, p: Array3, gamma: float, epsilon: float, ) -&gt; Tuple[Array1, Array2, int]: pi = np.zeros(p.shape[:2]) # |S| x |A| pi[:, 1] = 1.0 # 最初の方策は決定的ならなんでもいいが、行動1を選ぶ方策にしてみる state_indices = np.arange(0, p.shape[0], dtype=np.uint) for n_iter in range(MAX_ITER_PI): v_pi_, _ = v_pi(r, p, pi, gamma, epsilon) q_pi = r + gamma * np.einsum(&quot;saS,S-&gt;sa&quot;, p, v_pi_) greedy_actions = np.argmax(q_pi, axis=1) pi_next = np.zeros_like(pi) pi_next[state_indices, greedy_actions] = 1.0 # pi == pi_next なら収束 if np.linalg.norm(pi - pi_next) &lt; 1.0: return v_pi_, pi_next, n_iter + 1 pi = pi_next raise RuntimeError(&quot;Policy Iteration did not converge &gt;_&lt;&quot;) v_star_mdp3_vi, _, n_iter = policy_iteration(mdp3.r, mdp3.p, 0.9, 1e-4) print(f&quot;反復回数: {n_iter}&quot;) print(&quot; &quot;.join([f&quot;V({i}): {v:.3}&quot; for i, v in enumerate(v_star_mdp3_vi)])) . 反復回数: 2 V(0): 6.49 V(1): 7.21 V(2): 7.51 . なんか2回反復しただけで求まってしまいましたが...。 このアルゴリズムは方策反復法と呼ばれ、なんやかんやで最適方策に収束することが知られています。 では、この反復回数は、何によって決まるのでしょうか？ 方策の組み合わせは$|A|^{|S|}$通りありますが、上の実験だとずっと速く収束しているので、もっといいバウンドがありそうに思えます。 しかし、実際のところ最悪ケースでは指数時間かかることが知られています。 この記事では、この方策反復法が難しくなる場合についても解説したかったのですが、 理解できなかったので、諦めました。ヾ(｡&gt;﹏&lt;｡)ﾉ . 2.B &#21442;&#32771;&#25991;&#29486;&#12394;&#12393; . On the Complexity of Solving Markov Decision Problems | Cournell CS 6789: Foundations of Reinforcement Learning | . 参考文献では$ frac{1}{1 - gamma}$で反復回数を抑えているじゃないか、話が違うじゃないか、という気が一見してしまいます。 これは有名不等式$ log x leq x - 1$ からなんやかんやで$ frac{1}{1 - gamma} geq - frac{1}{ log gamma}$ だから〜という感じで考えればなんとかなると思います。 この不等式は$x=1$で等号なので、よく使う$ gamma=0.99$とかの設定ならかなり差は近くなります。 . 3. &#12458;&#12531;&#12521;&#12452;&#12531;&#23398;&#32722;&#12392;&#12375;&#12390;&#12398;&#20596;&#38754;: &#12487;&#12540;&#12479;&#12434;&#21454;&#38598;&#12377;&#12427;&#12371;&#12392;&#12398;&#38627;&#12375;&#12373; . 前節では、環境の情報から$V^*$や$ pi^*$を計算してみました。 この節では、満を持して学習の問題に取り組んでいこうと思います。 . 学習する理由として、 . $P$や$r$がわからない わからないが、なぜかシミュレーションすることはできる | $P$も$r$もわかっているが、問題が大きすぎて $O(|S|^2 |A| frac{...}{1 - gamma})$ とか解けなさそう | ...などがあると思いますが、ここでは最もゆるい設定を考えます。 MDPのパラメタはわからないが、あるMDPを使って行動できるシミュレータがあります。 これを環境と呼ぶことにしましょう。 . 環境の中で行動$a_t$をとると、 次の状態$s_{t+1}$が$P(s_{t+1}|s_t,a_t)$からサンプリングされて与えられます。 同時に報酬$r(s_t, a_t)$も与えられます。 この環境の中で、適当に行動しているとなんやかんやで情報が集まってきて問題が解ける というのが強化学習の仕組みです。 しかし、本当に適当に行動してしまっていいのでしょうか。 . 3.1 &#22577;&#37228;&#12394;&#12375;&#25506;&#26619;&#12398;&#21839;&#38988; . というわけで、とりあえず別に学習しなくていいので、環境から情報を集めてこよう、という問題を考えてみましょう。 . from matplotlib.figure import Figure from matplotlib.image import FigureImage class GridMDP: from matplotlib.colors import ListedColormap #: Up, Down, Left, Right ACTIONS = np.array([[-1, 0], [+1, 0], [0, -1], [0, +1]]) #: Symbols EMPTY, BLOCK, START, GOAL = range(4) DESCRIPTIONS = [&quot;Empty&quot;, &quot;Block&quot;, &quot;Start&quot;, &quot;Goal&quot;] #: Colormap for visualizing the map CM = ListedColormap([&quot;w&quot;, &quot;.75&quot;, &quot;xkcd:leaf green&quot;, &quot;xkcd:vermillion&quot;]) REWARD_COLORS = [&quot;xkcd:light royal blue&quot;, &quot;xkcd:vermillion&quot;] FIG_AREA = 28 # Returns PIL.Image def __download_agent_image(): from io import BytesIO from urllib import request from PIL import Image fd = BytesIO( request.urlopen( &quot;https://2.bp.blogspot.com/-ZwYKR5Zu28s/U6Qo2qAjsqI&quot; + &quot;/AAAAAAAAhkM/HkbDZEJwvPs/s400/omocha_robot.png&quot; ).read() ) return Image.open(fd) AGENT_IMAGE = __download_agent_image() def __init__( self, map_array: Sequence[Sequence[int]], reward_array: Optional[Sequence[Sequence[float]]] = None, action_noise: float = 0.1, horizon: Optional[int] = None, seed: int = 123456789, legend_loc: str = &quot;upper right&quot;, ) -&gt; None: def add_padding(seq: Sequence[Sequence[T]], value: T) -&gt; list: width = len(seq[0]) + 2 ret_list = [[value for _ in range(width)]] for col in seq: ret_list.append([value] + list(col) + [value]) ret_list.append([value for _ in range(width)]) return ret_list self.map_array = np.array(add_padding(map_array, 1), dtype=np.uint8) assert self.map_array.max() &lt;= 3 assert 0 &lt;= self.map_array.min() self.rows, self.cols = self.map_array.shape if reward_array is None: self.reward_array = np.zeros((self.rows, self.cols), np.float64) else: self.reward_array = np.array( add_padding(reward_array, 0.0), np.float64 ) self.action_noise = action_noise self.horizon = horizon self.start_positions = np.argwhere(self.map_array == self.START) if len(self.start_positions) == 0: raise ValueError(&quot;map_array needs at least one start posiiton&quot;) self.random_state = np.random.RandomState(seed) _ = self.reset() # Visualization stuffs self.legend_loc = legend_loc self.map_fig, self.map_ax, self.map_img = None, None, None self.agent_img, self.agent_fig_img = None, None def n_states(self) -&gt; int: return np.prod(self.map_array.shape) @staticmethod def n_actions() -&gt; int: return 4 def reset(self) -&gt; Array1: idx = self.random_state.randint(self.start_positions.shape[0]) self.state = self.start_positions[idx] self.n_steps = 0 return self.state.copy() def state_index(self, state: Array1) -&gt; int: y, x = state return y * self.map_array.shape[1] + x def _load_agent_img(self, fig_height: float) -&gt; None: from io import BytesIO from urllib import request fd = BytesIO(request.urlopen(self.ROBOT).read()) img = Image.open(fd) scale = fig_height / img.height self.agent_img = img.resize((int(img.width * scale), int(img.height * scale))) def _fig_inches(self) -&gt; Tuple[int, int]: prod = self.rows * self.cols scale = np.sqrt(self.FIG_AREA / prod) return self.cols * scale, self.rows * scale def _is_valid_state(self, *args) -&gt; bool: if len(args) == 2: y, x = args else: y, x = args[0] return 0 &lt;= y &lt; self.rows and 0 &lt;= x &lt; self.cols def _possible_actions(self) -&gt; Array1: possible_actions = [] for i, act in enumerate(self.ACTIONS): y, x = self.state + act if self._is_valid_state(y, x) and self.map_array[y, x] != self.BLOCK: possible_actions.append(i) return np.array(possible_actions) def _reward(self, next_state: Array1) -&gt; float: y, x = next_state return self.reward_array[y, x] def _is_terminal(self) -&gt; bool: if self.horizon is not None and self.n_steps &gt; self.horizon: return True y, x = self.state return self.map_array[y, x] == self.GOAL def step(self, action: int) -&gt; Tuple[Tuple[int, int], float, bool]: self.n_steps += 1 possible_actions = self._possible_actions() if self.random_state.random_sample() &lt; self.action_noise: action = self.random_state.choice(possible_actions) if action in possible_actions: next_state = self.state + self.ACTIONS[action] else: next_state = self.state.copy() reward = self._reward(next_state) self.state = next_state is_terminal = self._is_terminal() return next_state.copy(), reward, is_terminal def _draw_agent(self, fig: Figure) -&gt; FigureImage: unit = self.map_img.get_window_extent().y1 / self.rows y, x = self.state return fig.figimage( self.agent_img, unit * (x + 0.3), unit * (self.rows - 0.8 - y), ) def _draw_rewards(self) -&gt; None: for y in range(self.rows): for x in range(self.cols): rew = self.reward_array[y, x] if abs(rew) &lt; 1e-3: continue if self.map_array[y, x] == self.GOAL: color = &quot;w&quot; else: color = self.REWARD_COLORS[int(rew &gt;= 0)] self.map_ax.text( x + 0.1, y + 0.5, f&quot;{rew:+.2}&quot;, color=color, fontsize=12, ) def show(self, title: str = &quot;&quot;, explicit: bool = False) -&gt; Axes: if self.map_fig is None: self.map_fig = plt.figure(title or &quot;GridMDP&quot;, self._fig_inches()) ax = self.map_fig.add_axes([0, 0, 1, 1]) ax.set_aspect(&quot;equal&quot;) ax.set_xticks([]) ax.set_yticks([]) self.map_img = ax.imshow( self.map_array, cmap=self.CM, extent=(0, self.cols, self.rows, 0), vmin=0, vmax=4, alpha=0.6, ) for i in range(1, 4): if np.any(self.map_array == i): ax.plot([0.0], [0.0], color=self.CM(i), label=self.DESCRIPTIONS[i]) ax.legend(fontsize=12, loc=self.legend_loc) ax.text(0.1, 0.8, title or &quot;GridMDP&quot;, fontsize=16) self.map_ax = ax imw, imh = self.AGENT_IMAGE.width, self.AGENT_IMAGE.height scale = (self.map_img.get_window_extent().y1 / self.rows) / imh self.agent_img = self.AGENT_IMAGE.resize( (int(imw * scale), int(imh * scale)) ) if np.linalg.norm(self.reward_array) &gt; 1e-3: self._draw_rewards() if self.agent_fig_img is not None: self.agent_fig_img.remove() self.agent_fig_img = self._draw_agent(self.map_fig) if explicit: from IPython.display import display self.map_fig.canvas.draw() display(self.map_fig) return self.map_ax . . grid_mdp1 = GridMDP( [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 2, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], horizon=50, ) _ = grid_mdp1.show(&quot;GridMDP1&quot;) . GridMDPと題されたこちらが、今回使用する「環境」になります。 環境の中で行動する主体をエージェントと呼びます。 今回は、いらすとや様のロボットの画像を使用させていただきました。 各マス目の中で、エージェントは行動は上下左右に移動の4種類の行動を選択できます。 行動は時々失敗して、一様ランダムな状態遷移が発生します。 ここで、前章で用いなかったいくつかの新しい概念を導入します。 . 「スタート地点」の存在 ロボットくんは、決められた場所から行動を始めなくてはなりません。この問題では、初期状態はあらかじめ決まられたいくつかのスタート地点から均等に選びます。理論的なフレームワークでは、初期状態分布$ mu: mathcal{S} rightarrow mathbb{R}$として表現すればいいです。 | . | 「終了地点」の存在 ロボットくんは、いくつかの決められた場所に到達したら強制的にスタートまで戻されます。 | . | エピソード スタートから終了するまでの一連の流れをエピソードと呼びます。 | . | 「エピソード長さ（ホライゾン）」の存在 一定のターンが経過した時、ロボットくんはスタート地点まで戻されます。強化学習ではしばしば、シミュレーターを何度もリセットして学習します。理論を実際に近づけるため、MDPのフレームワークにもこれが導入される場合があります。 | . | . ではさっそく、適当に行動してもらいましょう。 エージェントをランダムに行動させて、訪問した場所に色をつけていきます。 なお、エピソード長さは50とします。 . from abc import ABC, abstractmethod from typing import Callable class VisitationHeatmap: def __init__( self, map_shape: Tuple[int, int], figsize: Tuple[float, float], ax: Optional[Axes] = None, max_visit: int = 1000, title: str = &quot;&quot;, ) -&gt; None: from matplotlib import colors as mc from mpl_toolkits.axes_grid1 import make_axes_locatable self.counter = np.zeros(map_shape, np.int64) self.title = title self.fig = plt.figure(self.title, figsize, facecolor=&quot;w&quot;) self.ax = self.fig.add_axes([0, 0, 1, 1]) self.ax.set_aspect(&quot;equal&quot;) self.ax.set_xticks([]) self.ax.set_yticks([]) r, g, b = mc.to_rgb(&quot;xkcd:fuchsia&quot;) cdict = { &quot;red&quot;: [(0.0, r, r), (1.0, r, r)], &quot;green&quot;: [(0.0, g, g), (1.0, g, g)], &quot;blue&quot;: [(0.0, b, b), (1.0, b, b)], &quot;alpha&quot;: [(0.0, 0.0, 0.0), (1.0, 1.0, 1.0)], } self.img = self.ax.imshow( np.zeros(map_shape), cmap=mc.LinearSegmentedColormap(&quot;visitation&quot;, cdict), extent=(0, map_shape[1], map_shape[0], 0), vmin=0, vmax=max_visit, ) divider = make_axes_locatable(self.ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;4%&quot;, pad=0.1) self.fig.colorbar(self.img, cax=cax, orientation=&quot;vertical&quot;) cax.set_ylabel(&quot;Num. Visitation&quot;, rotation=0, position=(1.0, 1.1), fontsize=14) self._update_text() self.agent = None def _update_text(self) -&gt; None: self.text = self.ax.text( 0.1, -0.5, f&quot;{self.title} After {self.counter.sum()} steps&quot;, fontsize=16, ) def _draw_agent(self, draw: Callable[[Figure], FigureImage]) -&gt; None: if self.agent is not None: self.agent.remove() self.agent = draw(self.fig) def visit(self, state: Array1) -&gt; int: y, x = state res = self.counter[y, x] self.counter[y, x] += 1 self.img.set_data(self.counter) self.text.remove() self._update_text() return res def show(self) -&gt; None: from IPython.display import display display(self.fig) def do_nothing( _state: int, _action: int, _next_state: int, _reward: float, _is_terminal: bool, ) -&gt; None: return def simulation( mdp: GridMDP, n: int, act: Callable[[int], int], learn: Callable[[int, int, int, float, bool], None] = do_nothing, max_visit: Optional[int] = None, vis_freq: Optional[int] = None, vis_last: bool = False, title: str = &quot;&quot;, ) -&gt; None: visitation = VisitationHeatmap( mdp.map_array.shape, mdp._fig_inches(), max_visit=max_visit or n // 10, title=title, ) state = mdp.reset() visitation.visit(state) vis_interval = n + 1 if vis_freq is None else n // vis_freq for i in range(n): if (i + 1) % vis_interval == 0 and (vis_last or i &lt; n - 1): visitation._draw_agent(mdp._draw_agent) visitation.show() action = act(mdp.state_index(state)) next_state, reward, terminal = mdp.step(action) visitation.visit(next_state) learn( mdp.state_index(state), action, mdp.state_index(next_state), reward, terminal, ) if terminal: state = mdp.reset() else: state = next_state visitation._draw_agent(mdp._draw_agent) simulation(grid_mdp1, 1000, lambda _: np.random.randint(4), vis_freq=2) . . ランダムに行動させただけですが、それなりにまんべんなく色が塗られていて、まあまあいいのではないか、という気がします。 しかし、もっと広い環境ではどうでしょうか。 . grid_mdp2_map = [[0] * 15 for _ in range(15)] grid_mdp2_map[7][7] = 2 grid_mdp2 = GridMDP(grid_mdp2_map, horizon=50) _ = grid_mdp2.show() random_state = np.random.RandomState(1) simulation( grid_mdp2, 5000, lambda _: random_state.randint(4), max_visit=100, title=&quot;Random Exploration&quot;, vis_freq=None, ) . . なんか駄目っぽい感じですね。 場所によっては全く色がついていません。 環境が広いと、ランダムに歩き回るのでは、効率よく情報を集めてこれないようです。 具体的にどのくらい難しいのかと言うと、平均一回訪問するのにかかる時間が、だいたい . 一方通行の直線: $O(|S|)$ | 二次元ランダムウォーク: $O(|S|^2)$? (参考: plane上のランダムウォーク） | 最悪ケース: $O(2^{|S|})$ | . くらいになります。 一方通行なのはなんとなくわかりますね。 ランダムウォークの場合、同じ場所を行ったりきたりできるので、そのぶん時間がかかってしまいます。 最悪ケースは、以下のように構成すればいいです。 . この環境で状態$0$からランダムに行動すると、右端にたどり着くまでに平均$2^5$くらいのステップ数がかかります。 そんなんありかよ...って感じですが。 . この結果から、最悪の場合だと指数時間かかるから賢くデータ収集しないといけないよね、 思うこともできます。 その一方で、ランダムウォークのように遷移の対称性がある環境なら、 ランダムに行動してもそんなに悪くないんじゃないかな、とも思えます。 . さてその話は一旦おいておいて、もっと効率よくデータを集める方法を考えてみましょう。 . 訪問した場所を覚えておいて、訪問していない場所を優先して探査する | 状態と状態の間に距離が定義できると仮定して、遠くに行くようにする | 環境がユークリッド空間だと仮定してSLAMで自己位置推定する | など、色々な方法が考えられると思いますが、ここでは1の方法を使ってみましょう。 . 以下のようなアルゴリズムを考えます。 . 適当な方策$ pi_0$から開始する | 状態行動訪問回数$n(s, a)$、状態行動次状態訪問回数$n(s, a, s&#39;)$を記録しておく ただし、初期値は$n_0(s, a) = 1.0, n_0(s, a, s) = frac{1}{|S|}$とする(0除算防止のため) | . | エピソードが終わったとき、以下のように方策を更新する 状態遷移関数の推定値$ hat{P}(s&#39;|s, a) = frac{n(s, a, s&#39;)}{n(s, a}$、疑似報酬$r_k(s, a)= frac{1}{n(s, a)}$、適当な$ gamma$から成るMDP$ mathcal{M}_k$を解く | $ mathcal{M}_k$の最適価値関数$V^*_k,Q^*_k$から以下のように方策$pi_{k+1}$を構成する $V^*_k(s) &lt; frac{1}{|S|} sum_{s&#39; in mathcal{S}}V^*_k(s&#39;)$ なら $ pi_{k+1}(s)$は$Q^*_k$に基づく貪欲行動 | それ以外の場合、$ pi_{k+1}(s)$は一様ランダムな行動をとる (=方策を緩和する) | . | | 疑似報酬$r_k= frac{1}{n(s, a)}$を使用してプランニングするのが、最も重要なポイントです。 この値は、一度も状態行動ペア$(s,a)$を経験していないなら$1$、一度経験したら$1/2$、2回経験したら$1/3$のように減衰します。 これを報酬とするMDPを解くことで、あまり経験していない状態行動ペアをとろうとする方策が得られます。 完全な貪欲方策ではなく緩和をいれているのは、高い報酬の状態をループしないようにするためです。 では、やってみましょう。 . class RewardFreeExplore: def __init__( self, n_states: int, n_actions: int, seed: int = 1, gamma: float = 0.95, ) -&gt; None: self.sa_count = np.ones((n_states, n_actions)) self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states self.pi = np.ones((n_states, n_actions)) / n_actions self.random_state = np.random.RandomState(seed) self.n_states, self.n_actions = n_states, n_actions self.n_updates = 0 self.gamma = gamma self.pi_indices = np.arange(n_states, dtype=np.uint32) def learn( self, state: int, action: int, next_state: int, reward: float, is_terminal: bool, ) -&gt; None: # 訪問記録を更新する self.sa_count[state, action] += 1 if is_terminal: # エピソードが終わったら、Value Iterationを解いて方策を更新する r = 1.0 / self.sa_count p = self.sas_count / np.expand_dims(self.sa_count, axis=-1) v, _n_iter = value_iteration(r, p, self.gamma, 1e-2) v_is_larger_than_mean = v &gt; v.mean() q = r + self.gamma * np.einsum(&quot;saS,S-&gt;sa&quot;, p, v) self.pi.fill(0.0) for state in range(self.n_states): # Vが大きい場所では方策を緩和する if v_is_larger_than_mean[state]: self.pi[state] = 1.0 / self.n_actions # そうでない場合は貪欲 else: self.pi[state][q[state].argmax()] = 1.0 self.n_updates += 1 else: self.sas_count[state, action, next_state] += 1 def act(self, state: int) -&gt; int: return self.random_state.choice(self.n_actions, p=self.pi[state]) agent = RewardFreeExplore(grid_mdp2.n_states(), grid_mdp2.n_actions()) simulation( grid_mdp2, 5000, agent.act, learn=agent.learn, max_visit=100, title=&quot;Strategic Exploration&quot;, vis_freq=None, ) . . さっきよりも満遍なく、色々な状態を訪問してくれるようになりましたね。 . 3.2 &#22577;&#37228;&#12354;&#12426;&#25506;&#26619; . 次は、環境から報酬が与えられるので、なるべく早く学習を終わらせたい、という問題を考えます。 訪問していない場所に積極的にいけばいい、という方針はさっきと変わりません。 一方で、あまりに報酬がもらえなさそうな状態はとっとと諦めることをしなくてはいけない点が異なっています。 . 例えば、報酬の推定値を$ hat{r}(s, a)$とするとき、先ほどのアルゴリズムの疑似報酬を $r_k(s, a)= hat{r}(s, a)+ frac{ beta}{ sqrt{n(s, a)}}$とすればいいです。 これを、単に$r_k(s, a)= hat{r}(s, a)$とするアルゴリズム(Approximate Value Iteration)と比較してみましょう。 こちらは、確率$ epsilon$で一様分布から行動をサンプリングし、$1- epsilon$で$Q^*_k$が一番大きい行動を選択するという行動方策を使ってみましょう（$ epsilon$-Greedyと言います）。 今回は$ epsilon=0.9 rightarrow 0.4$とします。 . grid_mdp3 = GridMDP( [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0, 0]], horizon=100, legend_loc=&quot;lower right&quot; ) _ = grid_mdp3.show(&quot;GridMDP3&quot;) . . まず、こちらの環境で実験してみます。素直に$0.1$の報酬→$9.0$の報酬を目指せばいい感じです。 また、$ gamma=0.99$とします。 . class EpsgApproxVI: def __init__( self, n_states: int, n_actions: int, seed: int = 1, gamma: float = 0.99, epsilon: float = 0.9, epsilon_delta: float = 0.0001, ) -&gt; None: self.sa_count = np.ones((n_states, n_actions)) self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states self.r_sum = np.zeros((n_states, n_actions)) self.pi = np.ones((n_states, n_actions)) / n_actions self.random_state = np.random.RandomState(seed) self.n_states, self.n_actions = n_states, n_actions self.n_updates = 0 self.gamma = gamma self.epsilon = epsilon self.epsilon_delta = epsilon_delta self.pi_indices = np.arange(n_states, dtype=np.uint32) def learn( self, state: int, action: int, next_state: int, reward: float, is_terminal: bool, ) -&gt; None: self.sa_count[state, action] += 1 self.r_sum[state, action] += reward if is_terminal: r = self.r_sum / self.sa_count p = self.sas_count / np.expand_dims(self.sa_count, axis=-1) v, _n_iter = value_iteration(r, p, self.gamma, 1e-2) q = r + self.gamma * np.einsum(&quot;saS,S-&gt;sa&quot;, p, v) self.pi.fill(0) for state in range(self.n_states): self.pi[state][q[state].argmax()] = 1.0 self.n_updates += 1 else: self.sas_count[state, action, next_state] += 1 def act(self, state: int) -&gt; int: if self.random_state.rand() &lt; self.epsilon: self.epsilon -= self.epsilon_delta return self.random_state.choice(self.n_actions) else: return self.random_state.choice(self.n_actions, p=self.pi[state]) class MBIB_EB: def __init__( self, n_states: int, n_actions: int, seed: int = 1, gamma: float = 0.99, beta: float = 0.1, ) -&gt; None: self.sa_count = np.ones((n_states, n_actions)) self.sas_count = np.ones((n_states, n_actions, n_states)) / n_states self.r_sum = np.zeros((n_states, n_actions)) self.pi = np.ones((n_states, n_actions)) / n_actions self.random_state = np.random.RandomState(seed) self.n_states, self.n_actions = n_states, n_actions self.n_updates = 0 self.gamma = gamma self.beta = beta self.pi_indices = np.arange(n_states, dtype=np.uint32) def learn( self, state: int, action: int, next_state: int, reward: float, is_terminal: bool, ) -&gt; None: self.sa_count[state, action] += 1 self.r_sum[state, action] += reward if is_terminal: r = self.r_sum / self.sa_count + self.beta / np.sqrt(self.sa_count) p = self.sas_count / np.expand_dims(self.sa_count, axis=-1) v, _n_iter = value_iteration(r, p, self.gamma, 1e-2) v_is_larger_than_mean = v &gt; v.mean() q = r + self.gamma * np.einsum(&quot;saS,S-&gt;sa&quot;, p, v) self.pi.fill(0.0) for state in range(self.n_states): if v_is_larger_than_mean[state]: self.pi[state] = 1.0 / self.n_actions else: self.pi[state][q[state].argmax()] = 1.0 self.n_updates += 1 else: self.sas_count[state, action, next_state] += 1 def act(self, state: int) -&gt; int: return self.random_state.choice(self.n_actions, p=self.pi[state]) epsg_vi = EpsgApproxVI(grid_mdp3.n_states(), grid_mdp3.n_actions()) simulation( grid_mdp3, 10000, epsg_vi.act, learn=epsg_vi.learn, max_visit=100, title=&quot;ε-Greedy&quot;, vis_freq=None, ) mbib_eb = MBIB_EB(grid_mdp3.n_states(), grid_mdp3.n_actions()) simulation( grid_mdp3, 10000, mbib_eb.act, learn=mbib_eb.learn, max_visit=100, title=&quot;Startegic Exploration&quot;, vis_freq=None, ) . . 両方とも、いい感じに探査してくれているように見えます。 $ epsilon$-Greedyの方が、$R_t=9.0$がもらえるゴールの周辺を多く探査していて、 良さそうに見えます。 一方で、もう少し意地悪な環境の場合はどうでしょうか？ . grid_mdp4 = GridMDP( [[0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 3], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 9.0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], horizon=100, legend_loc=&quot;lower right&quot; ) _ = grid_mdp4.show(&quot;GridMDP3&quot;) . . この環境では、$+5.0$とかいう邪魔くさい報酬があります。 しかもここはゴールなので、ここに行くとまたリセットしてやり直しです。 ここを目指すように学習してしまうと、なかなか$+9.0$の方に行くのは厳しそうに見えます。 実験してみましょう。 . epsg_vi = EpsgApproxVI(grid_mdp4.n_states(), grid_mdp4.n_actions()) simulation( grid_mdp4, 10000, epsg_vi.act, learn=epsg_vi.learn, max_visit=100, title=&quot;ε-Greedy&quot;, vis_freq=None, ) mbib_eb = MBIB_EB(grid_mdp4.n_states(), grid_mdp4.n_actions()) simulation( grid_mdp4, 10000, mbib_eb.act, learn=mbib_eb.learn, max_visit=100, title=&quot;Startegic Exploration&quot;, vis_freq=None, ) . . 予想通り、$ epsilon$-Greedyの方は右上ばかり行ってしまってイマイチな感じになりました。 . 以上の結果から、 . 邪魔がなく遷移関数が対称な状態空間（ランダムウォークのように考えられるもの）では、わりあい簡単にデータ収集ができる | 邪魔な報酬がない環境では、わりあい簡単にデータ収集ができる | . という2点が言えるかと思います。 ワーストケースを考えると探査が難しいのも事実ですが、実用上は難しいケースを考えるより邪魔な報酬を排除する ことを考えるのが重要です。 . 3.A &#21442;&#32771;&#25991;&#29486;&#12394;&#12393; . On the Sample Complexity of Reinforcement Learning | Reward-Free Exploration for Reinforcement Learning | Sample Complexity Bounds of Exploration | An analysis of model-based Interval Estimation for Markov Decision Processes | 3.1で紹介したアルゴリズムは一応2.の文献を参考にしていますが、僕がさっき適当に考えた(は？)ものです。 理論保証があるかはあやしいと思います。 3.2のやつはMBIB-EB(4.)に似ていますが、方策の緩和が入っている点が違います。 緩和も僕が適当に考えたものなのですが、入れた方が性能が良かったので入れてみました。 良い子の皆さんは真似しないでください。 . 4. &#32080;&#35542; . 強化学習で重要なのは1に報酬、2に報酬、3、4がなくて5に$ gamma$(もしくはエピソード長さ$H$)です。 . 最適解と2番目にいい解の差が大きくなるように問題を設計しましょう | $ gamma$はできる限り小さくしましょう | . これらの点について納得いただければ、この記事は成功と言えるのではないかな、と思います。 本当はもう少し書きたいこともあるのですが...。 . 何か質問・誤りの指摘などあればコメント欄からお願いします。 . 5. &#35613;&#36766; . 強化学習苦手の会もくもく会主催の品川さんおよび、参加者の方々との有意義な議論に感謝します。 .",
            "url": "https://kngwyu.github.io/rlog/ja/2020/12/22/understanding-what-makes-rl-difficult.html",
            "relUrl": "/ja/2020/12/22/understanding-what-makes-rl-difficult.html",
            "date": " • Dec 22, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Series of blog posts mainly about reinforcement learning (RL). See this for my profile. .",
          "url": "https://kngwyu.github.io/rlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kngwyu.github.io/rlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}