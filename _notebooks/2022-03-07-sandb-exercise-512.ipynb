{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc261403-a701-441d-9664-c925f22fe645",
   "metadata": {},
   "source": [
    "# Exercise 5.12: Racetrack from the Reinforcement Learning textbook\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [ja]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59e4e8-5171-49f1-a7b9-286bfe32fbbb",
   "metadata": {},
   "source": [
    "Here I demonstrate the execise 5.12 of the textbook [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book.html) by Richard Sutton and Andrew G. Barto, using both of planning method and Monte Carlo method.\n",
    "Basic knowledge of Python (>= 3.7) and [NumPy](https://numpy.org/) are assumed.\n",
    "Some konwledge of [matplotlib](https://matplotlib.org/) and [Python typing library](https://docs.python.org/3/library/typing.html) also helps.\n",
    "\n",
    "Contact: `yuji.kanagawa@oist.jp`\n",
    "\n",
    "## Modeling the problem in code\n",
    "\n",
    "Let's start from writing the problem in code.\n",
    "What are important in this phase?\n",
    "Here, I'd like to emphasize the importantness of looking back at the definition of the environment.\n",
    "I.e., in reinforcement learning (RL), environments are modelled by Markov decision process (MDP), consisting of **states, actions, transition function, and reward function**.\n",
    "So first let's check the definition of **states** and **actions** in the problem statement.\n",
    "It's (somehow) not very straightforward, but we can find\n",
    "> In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step.\n",
    "\n",
    "So, a state consists of **position** and **velocity** of the car (a.k.a. agent).\n",
    "What about actions?\n",
    "\n",
    "> The actions are increments to the velocity components. Each may be changed by +1, −1, or 0 in each step, for a total of nine ($3 \\times 3$) actions.\n",
    "\n",
    "So there are 9 actions for each direction (↓↙←↖↑↗→↘ or no acceleration).\n",
    "Here, we can also notice that the total number of states is given by $\\textrm{Num. positions} \\times \\textrm{Num. choices of velocity}$.\n",
    "And the texbook also says\n",
    "> Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line.\n",
    "\n",
    "So there are 24 possible velocity at the non-starting positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e536d05-5cbf-4bca-8c91-d5a6631861b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "list(itertools.product(range(5), range(5)))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572b5d5-9f6f-4726-b54e-650de0e01c05",
   "metadata": {},
   "source": [
    "Again, number of states is given by (roughly) $24 \\times \\textrm{Num. positions}$ and number of actions is $9$.\n",
    "Sounds not very easy problem with many positions.\n",
    "\n",
    "So, let's start the coding from representing the state and actions.\n",
    "There are multiple ways, but I prefer to NumPy array for representing everything.\n",
    "\n",
    "Let's consider a ASCII representation of the map (or track) like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cfb3612e-1918-4fa9-a066-fba8e92515c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_TRACK = \"\"\"\n",
    "######   F\n",
    "###      F\n",
    "##      ##\n",
    "#      ###\n",
    "#SSSSS####\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2575ca6-d9fd-487a-9ffb-ab55c4cb82ec",
   "metadata": {},
   "source": [
    "Here, `S` denotes a starting positiona, `F` denotes a finishing position, `#` denotes a wall, and ` ` denotes a road.\n",
    "We have this track as a 2D NumPy Array, and encode agent's position as an index of this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "49c92d83-3948-475d-a8ca-156a210f05ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35 35 35 35 35 35 32 32 32 70]\n",
      " [35 35 35 32 32 32 32 32 32 70]\n",
      " [35 35 32 32 32 32 32 32 35 35]\n",
      " [35 32 32 32 32 32 32 35 35 35]\n",
      " [35 83 83 83 83 83 35 35 35 35]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ascii_to_array(ascii_track: str) -> np.ndarray:\n",
    "    lines = [line for line in ascii_track.split(\"\\n\") if len(line) > 0]\n",
    "    byte_lines = [list(bytes(line, encoding=\"utf-8\")) for line in lines]\n",
    "    return np.array(byte_lines, dtype=np.uint8)\n",
    "\n",
    "track = ascii_to_array(SMALL_TRACK)\n",
    "print(track)\n",
    "position = np.array([0, 0])\n",
    "track[tuple(position)] == int.from_bytes(b'#', \"big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20792c21-6c6b-4066-9132-80ba61c3e04b",
   "metadata": {},
   "source": [
    "Then, agent's velocity and acceleration are also naturally represented by an array.\n",
    "And, we represent an action as an index of an array consisting of all possible acceleration vetors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8de3683e-95cb-48b7-b08a-3cfc20e4745d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1],\n",
       "       [-1,  0],\n",
       "       [-1,  1],\n",
       "       [ 0, -1],\n",
       "       [ 0,  0],\n",
       "       [ 0,  1],\n",
       "       [ 1, -1],\n",
       "       [ 1,  0],\n",
       "       [ 1,  1]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(itertools.product([-1, 0, 1], [-1, 0, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ed2837-b1ef-46c6-add3-93961bb30c4d",
   "metadata": {},
   "source": [
    "The next step is to represent a transition function as a **black box simulator**.\n",
    "Note that we will visit another representation by **transition matrix**, but implementing the simulator is easier.\n",
    "Basically, the simulator should take an agent's action and current state, and then return the next state.\n",
    "Let's call this function `step`.\n",
    "However, let's make it return some other things to make the implementation easier.\n",
    "Reward function sounds fairly easy to implement given the agent's position.\n",
    "> The rewards are −1 for each step until the car crosses the finish line.\n",
    "\n",
    "Also, we have to handle the *termination* of the episode.\n",
    "> Each episode begins in one of the randomly selected start states with both velocity components zero and\n",
    "ends when the car crosses the finish line.\n",
    "\n",
    "So the resulting `step` function should return a tuple `(state, reward, termination)`.\n",
    "The below cell contains my implementation of the simulator with matplotlib visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6f7e4677-64f6-43dd-9ef1-f5ea7ca95918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(position=array([4, 5]), velocity=array([0, 0]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(position=array([3, 5]), velocity=array([1, 0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import NamedTuple, Optional, Tuple\n",
    "\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "class State(NamedTuple):\n",
    "    position: np.ndarray\n",
    "    velocity: np.ndarray\n",
    "\n",
    "\n",
    "class RacetrackEnv:\n",
    "    EMPTY = int.from_bytes(b\" \", \"big\")\n",
    "    WALL = int.from_bytes(b\"#\", \"big\")\n",
    "    START = int.from_bytes(b\"S\", \"big\")\n",
    "    FINISH = int.from_bytes(b\"F\", \"big\")\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ascii_track: str,\n",
    "        noise_prob: float = 0.1,\n",
    "        seed: int = 0,\n",
    "    ) -> None:\n",
    "        self._track = ascii_to_array(ascii_track)\n",
    "        self._max_height, self._max_width = self._track.shape\n",
    "        self._noise_prob = noise_prob\n",
    "        self._actions = np.array(list(itertools.product([-1, 0, 1], [-1, 0, 1])))\n",
    "        self._no_accel = 4\n",
    "        self._random_state = np.random.RandomState(seed=seed)\n",
    "        self._start_positions = np.argwhere(self._track == self.START)\n",
    "        self._ax = None\n",
    "        self._agent_fig = None\n",
    "\n",
    "    def step(self, state: State, action: int) -> Tuple[State, float, bool]:\n",
    "        position, velocity = state\n",
    "        if self._random_state.rand() < self._noise_prob:\n",
    "            accel = self._actions[no_accel]\n",
    "        else:\n",
    "            accel = self._actions[action]\n",
    "        # velocity is clipped so that only ↑→ directions are possible\n",
    "        next_velocity = np.clip(velocity + accel, a_min=0, a_max=4)\n",
    "        # If both of velocity is 0, cancel the acceleration\n",
    "        if np.sum(next_velocity) == 0:\n",
    "            next_velocity = velocity\n",
    "        # y_velocity is flipped to adjust the coordinate system\n",
    "        next_position = position + next_velocity * np.array([-1, 1])\n",
    "        clipped_position = np.clip(\n",
    "            next_position,\n",
    "            a_min=0,\n",
    "            a_max=(self._max_height - 1, self._max_width - 1),\n",
    "        )\n",
    "        next_track = self._track[tuple(clipped_position)]\n",
    "        if next_track == self.FINISH:\n",
    "            # Goal!\n",
    "            return State(next_position, next_velocity), 0, True\n",
    "        elif (\n",
    "            next_position[0] >= self._max_height\n",
    "            or next_position[1] >= self._max_width\n",
    "            or next_track == self.WALL\n",
    "        ):\n",
    "            # Out of the track\n",
    "            return self.reset(), -1.0, False\n",
    "        else:\n",
    "            return State(next_position, next_velocity), -1, False\n",
    "\n",
    "    def reset(self) -> State:\n",
    "        n_starts = len(self._start_positions)\n",
    "        initial_pos_idx = self._random_state.choice(n_starts)\n",
    "        initial_pos = self._start_positions[initial_pos_idx]\n",
    "        initial_velocity = np.array([0, 0])\n",
    "        return State(initial_pos, initial_velocity)\n",
    "\n",
    "    def render(\n",
    "        self,\n",
    "        state: Optional[State] = None,\n",
    "        figsize: Tuple[float, float] = (8.0, 8.0),\n",
    "    ) -> Axes:\n",
    "        if self._ax is None:\n",
    "            figure = plt.figure(figsize=figsize)\n",
    "            ax = figure.add_subplot(111)\n",
    "            ax.set_aspect(\"equal\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # Map the track to one of [0, 1, 2, 3] to that simple colormap works\n",
    "            map_array = np.zeros_like(track)\n",
    "            symbols = [self.EMPTY, self.WALL, self.START, self.FINISH]\n",
    "            for i in range(track.shape[0]):\n",
    "                for j in range(track.shape[1]):\n",
    "                    map_array[i, j] = symbols.index(self._track[i, j])\n",
    "            cm = ListedColormap(\n",
    "                [\"w\", \".75\", \"xkcd:reddish orange\", \"xkcd:kermit green\"]\n",
    "            )\n",
    "            map_img = ax.imshow(\n",
    "                map_array,\n",
    "                cmap=cm,\n",
    "                vmin=0,\n",
    "                vmax=4,\n",
    "                alpha=0.8,\n",
    "            )\n",
    "            descriptions = [\"Empty\", \"Wall\", \"Start\", \"Finish\"]\n",
    "            for i in range(1, 4):\n",
    "                if np.any(map_array == i):\n",
    "                    ax.plot([0.0], [0.0], color=cm(i), label=descriptions[i])\n",
    "            ax.legend(fontsize=12, loc=\"lower right\")\n",
    "            self._ax = ax\n",
    "        if state is not None:\n",
    "            if self._agent_fig is not None:\n",
    "                self._agent_fig.remove()\n",
    "            pos, vel = state\n",
    "            self._agent_fig = self._ax.plot(pos[1], pos[0], \"k^\", markersize=20)[0]\n",
    "            # Show velocity\n",
    "            self._ax.annotate(\n",
    "                \"\",\n",
    "                xy=(pos[1] + vel[1], pos[0] - vel[0]),\n",
    "                xycoords=\"data\",\n",
    "                xytext=(pos[1], pos[0]),\n",
    "                textcoords=\"data\",\n",
    "                arrowprops={\"color\": \"red\", \"alpha\": 0.6, \"width\": 2},\n",
    "            )\n",
    "        return self._ax\n",
    "\n",
    "\n",
    "env = RacetrackEnv(SMALL_TRACK)\n",
    "state = env.reset()\n",
    "print(state)\n",
    "display(env.render(state=state).get_figure())\n",
    "next_state, reward, termination = env.step(state, 7)\n",
    "print(next_state)\n",
    "env.render(state=next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a70eba-2b57-4798-a541-1116a09a8651",
   "metadata": {},
   "source": [
    "Note that the vertical velocity is negative, so that we can simply represent the coordinate by an array index.\n",
    "\n",
    "## Solve a small problem by dynamic programming\n",
    "\n",
    "OK, now we have a simulator, so let's solve the problem!\n",
    "However, before stepping into reinforcement learning, it's better to compute an optimal policy in a small problem for sanity check.\n",
    "To do so, we need a transition matrix `p`, which is a $|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|$ NumPy array where `p[i][j][k]` is the probability of transiting from `i` to `k` when action `j` is taken.\n",
    "Also, we need a $|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|$ reward matrix `r`.\n",
    "Note that this representation is not general as $R_t$ can be stochastic, but since the only stochasticity of rewards is the noise to actions in this problem, this notion suffices.\n",
    "It is often not very straightforward to get `p` and `r` from the problem definition, but basically we need to give 0-indexd indices to each state (`0, 1, 2, ...`) and fill the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "45668f8f-2939-4af8-83a9-af0132735d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_and_r(env: RacetrackEnv) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    velocities = list(itertools.product(range(5), range(5)))\n",
    "    state_indices = np.zeros((env._max_height, env._max_width, 5, 5), dtype=int)\n",
    "    states = []\n",
    "    current_state_idx = 0\n",
    "    starting_states = []\n",
    "    # Number all states\n",
    "    for y in range(env._max_height):\n",
    "        for x in range(env._max_width):\n",
    "            track = env._track[y][x]\n",
    "            if track == env.WALL:\n",
    "                continue\n",
    "            for y_velocity in range(5):\n",
    "                for x_velocity in range(5):\n",
    "                    # |v| == 0 is only allowed at starts\n",
    "                    if y_velocity == 0 and x_velocity == 0 and track != env.START:\n",
    "                        continue\n",
    "                    state_indices[y][x][y_velocity][x_velocity] = current_state_idx\n",
    "                    if track == env.START:\n",
    "                        starting_states.append(current_state_idx)\n",
    "                    current_state_idx += 1\n",
    "                    states.append(\n",
    "                        State(np.array([y, x]), np.array([y_velocity, x_velocity]))\n",
    "                    )\n",
    "    # Terminating state\n",
    "    n_states = len(states)\n",
    "    n_actions = len(env._actions)\n",
    "    p = np.zeros((n_states, n_actions, n_states))\n",
    "    r = np.ones((n_states, n_actions, n_states)) * -1\n",
    "    noise = env._noise_prob\n",
    "\n",
    "    def state_prob(*indices):\n",
    "        \"\"\"Returns a |S| length zero-initialized array where specified elements are filled\"\"\"\n",
    "        prob = 1.0 / len(indices)\n",
    "        res = np.zeros(n_states)\n",
    "        for idx in indices:\n",
    "            res[idx] = prob\n",
    "        return res\n",
    "\n",
    "    for i, state in enumerate(states[:-1]):\n",
    "        position, velocity = state\n",
    "        # At a terminating state, the agent cannot move\n",
    "        if env._track[tuple(position)] == env.FINISH:\n",
    "            for j in range(n_actions):\n",
    "                p[i][j] = state_prob(i)\n",
    "            continue\n",
    "        # First, compute next state probs without noise\n",
    "        next_state_prob = []\n",
    "        for j, action in enumerate(env._actions):\n",
    "            next_velocity = np.clip(velocity + action, a_min=0, a_max=4)\n",
    "            if np.sum(next_velocity) == 0:\n",
    "                next_velocity = velocity\n",
    "            next_position = position + next_velocity * np.array([-1, 1])\n",
    "            clipped_position = np.clip(\n",
    "                next_position,\n",
    "                a_min=0,\n",
    "                a_max=(env._max_height - 1, env._max_width - 1),\n",
    "            )\n",
    "            next_track = env._track[tuple(clipped_position)]\n",
    "            if (\n",
    "                next_position[0] >= env._max_height\n",
    "                or next_position[1] >= env._max_width\n",
    "                or next_track == env.WALL\n",
    "            ):\n",
    "                next_state_prob.append(state_prob(*starting_states))\n",
    "            else:\n",
    "                ny, nx = next_position\n",
    "                nvy, nvx = next_velocity\n",
    "                next_state_idx = state_indices[ny, nx, nvy, nvx]\n",
    "                next_state_prob.append(state_prob(next_state_idx))\n",
    "                # Reward is 0.0 when the episode finishes\n",
    "                if next_track == env.FINISH:\n",
    "                    r[i][j][next_state_idx] = 0.0\n",
    "        # Then linearly mix the transition probs with noise\n",
    "        for j in range(n_actions):\n",
    "            p[i][j] = noise * next_state_prob[env._no_accel] + (1.0 - noise) * next_state_prob[j]\n",
    "    \n",
    "    return p, r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac08e4e-debf-4a51-90eb-f74b07691b98",
   "metadata": {},
   "source": [
    "Then, let's compute the optimal Q value by value iteration.\n",
    "So far, we only learned dynamic programming by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739f4e7-3cb6-4255-9839-fb402d94b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(p: np.ndaray, r: np.ndarray, max_steps, discount, convergence_threshold=0.05):\n",
    "    n_states, n_actions = r.shape\n",
    "    q = np.zeros_like(r)\n",
    "    v = np.zeros(r.shape[0])\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        v_old = v.copy()\n",
    "        for s in range(n_states):\n",
    "            q[s] = r[s] + discount * p[s].dot(v)\n",
    "            v[s] = q[s].max()\n",
    "        diff = np.abs(v_old - v).mean()\n",
    "        if diff < convergence_threshold:\n",
    "            break\n",
    "    return q, v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
